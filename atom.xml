<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shiyu&#39;s Blog</title>
  
  <subtitle>Learn to live.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shiyuuuu.github.io/"/>
  <updated>2021-04-12T14:59:12.007Z</updated>
  <id>https://shiyuuuu.github.io/</id>
  
  <author>
    <name>Shiyu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CVPR 2021 待读论文列表</title>
    <link href="https://shiyuuuu.github.io/2021/04/13/CVPR21_toRead/"/>
    <id>https://shiyuuuu.github.io/2021/04/13/CVPR21_toRead/</id>
    <published>2021-04-12T16:00:00.000Z</published>
    <updated>2021-04-12T14:59:12.007Z</updated>
    
    <content type="html"><![CDATA[<ol><li><strong>Contrastive Learning for Compact Single Image Dehazing</strong></li></ol><p><img src="/2021/04/13/CVPR21_toRead/image-20210412225732809.png" alt></p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Contrastive Learning for Compact Single Image Dehazing&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;/2021/04/13/CVPR21_toRead/image-20210412225732809.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="CVPR21" scheme="https://shiyuuuu.github.io/categories/Paper-reading/CVPR21/"/>
    
    
      <category term="To Do" scheme="https://shiyuuuu.github.io/tags/To-Do/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Unsupervised Domain-Specific Deblurring via Disentangled Representations</title>
    <link href="https://shiyuuuu.github.io/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/"/>
    <id>https://shiyuuuu.github.io/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/</id>
    <published>2021-04-11T16:00:00.000Z</published>
    <updated>2021-04-12T07:29:46.474Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412113117919.png" alt></p><p>出处：CVPR2019</p><a id="more"></a><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Lu_Unsupervised_Domain-Specific_Deblurring_via_Disentangled_Representations_CVPR_2019_paper.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/ustclby/Unsupervised-Domain-Specific-Deblurring" target="_blank" rel="noopener">code</a></p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ul><li>之前的方法大多需要成对的训练数据，在实际中采集成对数据较困难。</li><li>现有的无监督方法比如cycleGAN这种，往往编码了色彩、纹理等模糊之外的信息。</li></ul><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul><li>提出了一直特定域的无监督特征解耦的去模糊方法。通过将模糊图像中的内容和模糊特征解开，以将模糊信息准确地编码到去模糊框架中。</li><li>他们在人脸图像和文本图像去模糊上达到了与最好的监督学习方法comparable的效果。</li></ul><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152435021.png" alt></p><p>目的是实现图像的去模糊，为什么要添加模糊编码器？<br>作者的思路：既然清晰的图像是不含模糊信息的，可以认为，<strong>清晰图像的内容编码器提取到了清晰图像的内容信息</strong>，如果清晰的图像通过结合模糊编码器模糊特征去生成出模糊图像，是不是可以说，模糊编码器是在对清晰图像做模糊化处理，这个的前提就是  模糊编码器确实提取到了图像的模糊特征。所以说由清晰图像生成模糊图像也保证了模糊编码器是对图像的模糊信息进行编码的作用。清晰图像到模糊图像是为了优化模糊编码和模糊图像的内容编码的作用。</p><p>如何去保证这个模糊编码器是真的提取到模糊图像的模糊特征了呢？又怎么保证模糊图像的内容编码器真的提取到图像的内容信息？</p><ol><li>让清晰图像内容编码器从清晰图像S 提取到的特征和模糊编码器从模糊图像b提取的模糊特征一同经过Gb生成模糊图像bs，并让bs是模糊化后的s，而不包含b的任何内容，也就是模糊编码器Eb不编码模糊图像的内容信息。</li><li>通过添加一个 KL 散度损失来规范模糊特征的分布，使其接近正态分布 p(z)∼N(0,1)。这个思路和 VAE 中的限制数据编码的潜在空间的分布思路是相近的，这里将模糊编码器的编码向量限制住，旨在控制模糊编码器仅对图像的模糊信息进行编码。</li></ol><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152701796.png" alt></p><h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152753767.png" alt></p><h2 id="experiments"><a href="#experiments" class="headerlink" title="experiments"></a>experiments</h2><p>内容编码器由三个卷积层和四个残差块组成。模糊编码器包含四个卷积层和一个全连接层。<br>对于生成器，该架构与内容编码器对称，具有四个残差块，后面是三个反卷积层。<br>对于判别器，    采用多尺度结构，其中每个尺度的特征图经过五个卷积层，然后被送到 sigmoid 输出。</p><h3 id="ablation-study"><a href="#ablation-study" class="headerlink" title="ablation study"></a>ablation study</h3><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152846257.png" alt></p><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152852355.png" alt></p><h3 id="results"><a href="#results" class="headerlink" title="results"></a>results</h3><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152922347.png" alt></p><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152934111.png" alt></p><p><img src="/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412152940368.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/image-20210412113117919.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2019&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="deblurring" scheme="https://shiyuuuu.github.io/categories/Paper-reading/deblurring/"/>
    
    
      <category term="unpaired" scheme="https://shiyuuuu.github.io/tags/unpaired/"/>
    
      <category term="deblurring" scheme="https://shiyuuuu.github.io/tags/deblurring/"/>
    
      <category term="unsupervised" scheme="https://shiyuuuu.github.io/tags/unsupervised/"/>
    
  </entry>
  
  <entry>
    <title>画出好看的图</title>
    <link href="https://shiyuuuu.github.io/2021/04/12/%E7%94%BB%E5%9B%BE/"/>
    <id>https://shiyuuuu.github.io/2021/04/12/%E7%94%BB%E5%9B%BE/</id>
    <published>2021-04-11T16:00:00.000Z</published>
    <updated>2021-04-12T08:17:19.694Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/12/%E7%94%BB%E5%9B%BE/image-20210412160217935.png" alt></p><a id="more"></a><p><a href="https://mp.weixin.qq.com/s?__biz=MzA4MjEyNTA5Mw==&amp;mid=2652577438&amp;idx=2&amp;sn=5903becae76d016b8afe6868d00bdc4e&amp;chksm=846536d4b312bfc2c4c9ee6e5ad7a9a8223d9da7f348d66cf1101c0dd38b76da47259794a3dd&amp;mpshare=1&amp;scene=1&amp;srcid=0412bcaR3nz4OPXsn5zJlZWU&amp;sharer_sharetime=1618188473619&amp;sharer_shareid=830756841a1facbb06db3dcc0906fe0f&amp;key=6b62ea248264eadbdb7ffb6417146bd38e137a69e73ac25314b1ff429d2882caa5613e08868063d16320bac933c4e08c8cb1aada1c97faad16e64bb994cee7a7353148b2423a83b9a2f911ae425c14af8ffe84dc74ac3fd14c03277d2d17757104346afa78f7962f19abb07cfcef40903a8241c8e8de4401411322087a2a0e34&amp;ascene=1&amp;uin=NzI2MDMzNzA3&amp;devicetype=Windows+XP&amp;version=62060841&amp;lang=zh_CN&amp;exportkey=AUKiXJ7uaIJ%2FKCDoV7PXouo%3D&amp;pass_ticket=TkojhHDgehTysLkk8Hn7AidJL40Gxu%2FR73zne%2BPOyZWQ%2B7YOrM8F8qHZLz7Mze0%2B&amp;wx_header=0" target="_blank" rel="noopener">吊打 Pyecharts，这个新 Python 绘图库竟然这么漂亮！</a></p><p>链接: <a href="https://pan.baidu.com/s/1ccJBCiB_O_Xo-ci0UwjcDg" target="_blank" rel="noopener">https://pan.baidu.com/s/1ccJBCiB_O_Xo-ci0UwjcDg</a>  密码: ah9a</p><p><img src="/2021/04/12/%E7%94%BB%E5%9B%BE/image-20210412161709485.png" alt></p><p><a href="https://zhuanlan.zhihu.com/p/54831105" target="_blank" rel="noopener">听说一分钟就可以制作时间轴？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/12/%E7%94%BB%E5%9B%BE/image-20210412160217935.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="工具" scheme="https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="画图" scheme="https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/%E7%94%BB%E5%9B%BE/"/>
    
    
      <category term="工具" scheme="https://shiyuuuu.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="画图" scheme="https://shiyuuuu.github.io/tags/%E7%94%BB%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Toward Convolutional Blind Denoising of Real Photographs</title>
    <link href="https://shiyuuuu.github.io/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/"/>
    <id>https://shiyuuuu.github.io/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/</id>
    <published>2021-04-09T16:00:00.000Z</published>
    <updated>2021-04-12T03:21:05.828Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210410205830196.png" alt></p><p>出处：CVPR2019</p><a id="more"></a><p><a href="https://arxiv.org/pdf/1807.04686.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/GuoShi28/CBDNet" target="_blank" rel="noopener">code</a>  </p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>尽管之前一些基于CNN的算法在图像的加性高斯白噪声去除上取得了很好的效果，但这些方法往往是在过拟合AWGN，而在真实噪声去除上效果并不好。这篇文章提出了一种更加真实的噪声模型，既考虑了信号依赖噪声也考虑了相机内的ISP。另一方面为了进一步提供交互方法，方便纠正去噪结果，他们设计了一个噪声估计的子网络，它利用了非对称学习的策略避免噪声水平的低估。他们的方法在3个real-world noisy image的数据集上取得了state-of-the-art的结果。</p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul><li>同时考虑heteroscedastic Gaussian noise和in-camera processing pipeline</li><li>非对称学习的噪声估计子网络</li><li>三个real-world noisy image dataset上性能SOTA</li></ul><h2 id="噪声模型"><a href="#噪声模型" class="headerlink" title="噪声模型"></a>噪声模型</h2><p>在真实成像系统中，图像噪声有多个来源，并且受到相机内ISP 的影响，包括去马赛克，gamma校正和压缩。泊松高斯分布被认为是比AWGN更适合模拟真实噪声分布的模型，包含了平稳噪声部分和信号依赖部分。实际上，由传感产生的噪声可以被建模成泊松，其余的平稳噪声可以被建模为高斯，所以泊松高斯模型是一个合理的噪声模型。泊松高斯模型可以近似为异方差高斯。L 为raw图像的数值（辐照度），它的噪声可以建模为两部分：信号依赖噪声部分和平稳噪声部分。前一项通常和图像亮度有关，一般亮度越暗，噪声水平越大（比如夜景）<br>文章还进一步考虑了相机内ISP 流程，这里的y表示合成的噪声图像，DM 表示去马赛克，f（）表示相机响应函数CRF<br>进一步考虑压缩效应，JPEG压缩后的合成噪声图像为3式所示<br>对于RAW图像，可以使用第一个公式合成噪声；对于未压缩图像，可以使用第二个公式合成图像；对于压缩图像，使用第三个公式合成图像。<br>有了这个噪声模型就可以生成合成的noisy images</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412110823484.png" alt></p><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><h3 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h3><p>网络包含两个子网络，一个是噪声估计子网络，一个是非盲去噪子网络。<br>噪声估计子网络将噪声观测图像y转换为估计的噪声水平图σ^(y)。然后，非盲去噪子网络将y和σ^(y)作为输入得到学习到的残差项，再加上y 最终的去噪结果x^。<br>噪声估计子网络使用五层全卷积网络，卷积核为3×3×323×3×32，并且不进行pooling和batch normalization。<br>非盲去噪子网络使用16层的U-Net结构<br>另外，虽然在DnCNN中提到，batch normalization成功应用于高斯去噪中，但是对于真实图像的噪声去除并没有多大帮助，这可能是由于真实世界的噪声分别与高斯分布相差较大。<br>除此之外，噪声估计子网络允许用户在估计的噪声水平图σ^(y) 输入到非盲去噪子网络之前对应进行调整。(红框）</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412110955127.png" alt></p><h3 id="Asymmetric-Loss"><a href="#Asymmetric-Loss" class="headerlink" title="Asymmetric Loss"></a>Asymmetric Loss</h3><p>作者观察到非盲去噪方法（如BM3D、FFDNet等）对噪声估计的误差具有非对称敏感性。在之前的非盲去噪方法中，如果输入的噪声和真实的噪声匹配，可以较好的去噪，如果输入噪声水平（估计））高于真实噪声水平（高估）去噪结果仍能保持较好的效果，虽然也平滑了部分低对比度的纹理。但当输入噪声标准差低于真实值时（低估），去噪结果包含可察觉的噪声。正是因为这个特性，BM3D可以通过设置相对较高的输入噪声标准差得到满意的真实图像去噪效果。<br>因此，非盲去噪方法对低估误差比较敏感，而对高估的误差比较鲁棒。本文就提出了这种非对称loss 用来避免噪声水平图的低估。给定像素i的估计噪声水平σ^(yi)和真实噪声水平σ(yi)，当估计值小于真实值时，应该对其MSE引入更多的惩罚。0&lt;alpha&lt;0.5，低估时，前一项大于0.5，高估时，前一项小于0.5。<br>另外引入TV loss约束噪声水平图的平滑性，最后是一个重建误差，L2 loss，总的loss是这三项加权相加。</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412111041194.png" alt></p><h2 id="experiments"><a href="#experiments" class="headerlink" title="experiments"></a>experiments</h2><p>如果只用合成数据，真实图像的噪声并不能由本文的噪声模型完美的刻画，而如果只用真实噪声图像，在获取clean image的时候，需要上百张noisy images 做平均，这个成本很高，而且由这样做平均得到的图像往往过于平滑。<br>为了提高网络的泛化能力，交替使用一批合成图像和一批真实图像进行训练。</p><h3 id="results-on-real-datasets"><a href="#results-on-real-datasets" class="headerlink" title="results on real datasets"></a>results on real datasets</h3><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412111439635.png" alt></p><p>NC12没有GT，DND包括50对真实噪声图像和对应的干净图像（由low-ISO image后处理得到），Nam包括11个静止场景，每个场景的无噪图像由500张JPEG带噪图像平均得到。<br>在这三个数据集上都取得了最佳的效果。</p><h3 id="ablation-study"><a href="#ablation-study" class="headerlink" title="ablation study"></a>ablation study</h3><p>验证不同噪声模型的效果：第一种是最普通的高斯噪声，第二种是本文里用的异方差高斯，第三种是普通高斯加ISP ，第四种是异方差高斯加ISP ，也就是本文所用的模型。</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412111726402.png" alt></p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412112004636.png" alt></p><p>验证训练时是否既用合成数据集又用真实数据集的效果是否最好，有两个对照组：只用合成噪声图像训练，或者只用真实噪声图像训练。</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412111910871.png" alt></p><p>验证非对称loss，alpha=0.5时，在低估和高估情况下，加的惩罚一样多，alpha&lt;0.5时，给低估加更多惩罚。Alpha=0.3时，效果最好</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412111936419.png" alt></p><p>对于给定的噪声估计图，用户可以交互式的修改σ(y)，来修正去噪效果，具体是在σ(y)前乘一个系数，第一幅图当这个系数等于0.7时，效果最好，第二幅图是系数为1.3时效果最好。</p><p><img src="/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210412112056745.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/image-20210410205830196.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2019&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="denoising" scheme="https://shiyuuuu.github.io/categories/Paper-reading/denoising/"/>
    
    
      <category term="denoising" scheme="https://shiyuuuu.github.io/tags/denoising/"/>
    
      <category term="blind" scheme="https://shiyuuuu.github.io/tags/blind/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Unpaired Image Super-Resolution using Pseudo-Supervision</title>
    <link href="https://shiyuuuu.github.io/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/"/>
    <id>https://shiyuuuu.github.io/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/</id>
    <published>2021-04-08T16:00:00.000Z</published>
    <updated>2021-04-09T08:26:08.506Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409155557355.png" alt></p><p>出处：CVPR2020</p><a id="more"></a><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Maeda_Unpaired_Image_Super-Resolution_Using_Pseudo-Supervision_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Maeda_Unpaired_Image_Super-Resolution_CVPR_2020_supplemental.pdf" target="_blank" rel="noopener">supplemental</a>   </p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ul><li><p>unpaired super-resolution，当aligned 的HR-LR training set is unavailable.</p></li><li><p>deviation between the generated LR distribution and the true LR distribution causes train-test discrepancy</p></li></ul><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><p>（author: propose a new training method that overcomes the shortcomings of the existing GAN based unpaired super-resolution methods: generated LR distribution and the true LR distribution causes train-test discrepancy）</p><ol><li>bridge the gap between the well-studied existing SR methods and the real-world SR problem without paired datasets.</li><li>Because our correction network is trained on not only the generated LR images but also the true LR images through the bi-directional structure (因为我们的校正网络不仅通过生成的LR图像进行训练，而且还通过双向结构对真实的LR图像进行训练) : minimize the train-test discrepancy</li><li>any existing SR networks and pixel-wise loss function can be integrated because the SR network is separated to be able to learn in a paired manner. (SR网络可以pair对的学习)</li></ol><p>包含unpaired kernel/noise correction network和pseudo-paired SR network</p><p><strong>unpaired kernel/noise correction network</strong>: 去除噪声、调整输入图像的kernel，从输入的HR图像生成pseudo-clean的LR图像</p><p><strong>pseudo paired SR network</strong>: 学习pseudo-clean的LR image到输入的HR image的映射</p><p>SR网络独立于效验网络(correction network)</p><h2 id="related-works"><a href="#related-works" class="headerlink" title="related works"></a>related works</h2><ol><li><p>paired SR：</p><p>VDSR, EDSR, RCAN，LapSRN，DBPN</p></li><li><p>blind SR [39,12,57]</p><p>由任意的kernel降质得到的LR，学习由这样的LR到HR的映射，但当真实图像不是以假设的degradation降质的，（degradation估计不准），就会让真实图像SR的任务很差。</p><p>ZSSR， IKC， </p><p>关于blind SR的研究很少涉及blur kernels以外的综合降质问题（比如noise，compression artifact)。</p></li><li><p>GAN based methods [51,4,56, 32]</p><p>可以直接学习LR 到HR的映射，不需要degradation的假设。</p><p>可以大致分为两类：一类是直接从LR image出发，在生成的HR image和真实的HR image之间加discriminator. 这种方式的确定是无法用pixel wise的loss，因为real HR是未知的。</p></li></ol><p>另一类是，在HR到LR的过程加GAN，生成的LR image和真实的LR image之间加discriminator，使生成的LR尽可能逼近真实的LR image。然后生成的LR与原来的HR之间用pixel-wise的loss训练一个LR2HR的网络，也就是U。与cycleGAN的区别：HR端没有discriminator。缺点：生成的LR分布与真实的LR分布存在偏差，导致在training set和test set性能差别大。（当test set的图像分布在training set中完全没有）</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409160510814.png" alt></p><p>ICCV 2017: DualGAN: Unsupervised Dual Learning for Image-to-Image Translation </p><p>ICCV 2017: CycleGAN</p><p>ECCV 2018：To learn image super-resolution, use a gan to learn how to do image degradation first  </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409160610485.png" alt></p><p>ICCVW 2019：Unsupervised learning for real-world super-resolution  </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409160931259.png" alt></p><p>以上两个第一次训练HR2LR的网络，并用degraded的输出训练LR2HR的网络。</p><p>CVPRW 2018：Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks  </p><p>提出cycle-in-cycle network，但他们的degradation网络是确定的，并且SR网络与bi-cycle网络合在一起。选择loss function的时候有局限性</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161306839.png" alt></p><p>input LR $x$， GT HR $z$， $z$ bicubic下采样得到$y$ (clean LR)</p><p>先看里面的LR2clean LR，$x\sim x’ $  (半个cycleGAN)</p><p>生成得到的clean LR 进SR网络，与GT送入判别器$D_2$，通过$G_3$再回到real LR space，$x \sim x’’$</p><p>【pixel-wise loss不能用在HR space】</p><p>arxiv 2018：Unsupervised degradation learning for single image super-resolution </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161600361.png" alt></p><p>利用了双向的结构，他们也在选择loss function的时候有局限性</p><p>以上4种，ECCV 2018人脸的和arxiv 2018的这篇基本基于cycleGAN的结构。</p><p>这篇文章和以上这些文章最主要的区别是，解决了在训练数据集和测试数据集分布不一致的问题。也就是pseudo-clean LR 和 real LR</p><ol><li><p>通过硬件和数据对齐的操作建立 real SR的数据集：</p><p>ICCV 2019：Toward real-world single image super-resolution: A new benchmark and a new model  </p><p>CVPR 2019：Camera lens super-resolution  </p><p>CVPR 2019：Zoom to learn, learn to zoom  </p></li></ol><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><p>解决其他GAN-based unpaired SR的缺点：separating the entire network into an unpaired kernel/noise correction network and a pseudo-paired<br>SR network  </p><p>correction network：是一个cycleGAN， 完成的是unpaired的real LR和 clean LR之间的translation。clean LR由HR经过predetermined operation得到。</p><p>SR network：成对的学习pseudo-clean LR到HR mapping</p><p>在训练阶段，correction network也先由clean LR到true LR再回到clean LR生成pseudo-clean的LR 图像。SR network成对的学习pseudo-clean LR image到HR的mapping。</p><p>学习印射$F_{XY}:(X)LR-&gt;(Y)HR$, 定义 clean LR：$Y_\downarrow$是由$Y$经过一个指定的下采样操作得到的：$Y\rightarrow Y_\downarrow$是bicubic 下采样和gaussian blur的组合得到的。本文将$F_{XY}$拆分为两个mapping$G_{XY_\downarrow}$和$U_{Y_\downarrow Y}$的组合。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161658477.png" alt></p><p><strong>domain transfer in LR</strong>，其中学习mapping$G_{XY_\downarrow}$是通过上图蓝框中cycleGAN的结构</p><p><strong>mapping from LR to HR</strong>，只看绿线部分，由HR domain出发，先经过bicubic+Gaussian的下采样得到clean LR $Y_\downarrow$, 再把它依次过cycleGAN的两个generator得到pseudo-clean LR $\mathring{y_{\downarrow}}$ , pseudo-clean LR到HR的mapping为$U_{Y_\downarrow Y}$. 而$\mathring{y_{\downarrow}}$和y是成pair的，所以经过$U_{Y_\downarrow Y}$上采样得到的$U_{Y_\downarrow Y}(\mathring{y_{\downarrow}})$与y 之间可以用任意的pixel-wise的loss。</p><p><strong>HR discriminator</strong>, 希望减小训练和测试的偏差，尽管$\mathring{y_{\downarrow}}$用来训练SR网络，但是实际应用的时候，输入的LR image是$G_{XY_\downarrow}(x)$。所以pseudo clean LR和由real LR生成的clean LR的超分辨率后的差异尽可能小，所以最后还在这二者过$U_{Y_\downarrow Y}$的输出上加判别器$D_{X_\uparrow}$ </p><p><strong>test phase</strong>, 黑色实线部分，由real LR image先印射到 clean LR image $G_{XY_\downarrow}(x)$，（cycleGAN训练到比较理想情况的时候, clean LR 与由real LR生成的clean LR 以及pseudo clean LR都很接近）</p><h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p>对于两个生成器和3个判别器，总共的loss：</p><script type="math/tex; mode=display">\mathcal{L}_{trans}=\mathcal{L}_{adv}(G_{XY_\downarrow}, D_{Y_\downarrow}, X, Y_\downarrow)+\mathcal{L}_{adv}(G_{Y_\downarrow X}, D_{X}, Y_\downarrow, X)\\+\gamma\mathcal{L}_{adv}((G_{XY_\downarrow},G_{Y_\downarrow X}),D_{X_\uparrow}, Y_\downarrow,X_\uparrow)\\+\lambda_{cyc}\mathcal{L}_{cyc}(G_{Y_\downarrow X},G_{XY_\downarrow})+\lambda_{idt}\mathcal{L}_{idt}(G_{XY_\downarrow})+\lambda_{geo}\mathcal{L}_{geo}(G_{XY_\downarrow})</script><p>其中，$\mathcal{L}_{adv}((G_{XY_\downarrow},G_{Y_\downarrow X}),D_{X_\uparrow}, Y_\downarrow,X_\uparrow)$是HR discriminator的loss:</p><script type="math/tex; mode=display">\mathcal{L}_{adv}((G_{XY_\downarrow},G_{Y_\downarrow X}),D_{X_\uparrow}, Y_\downarrow,X_\uparrow)\\=\mathbb{E}_{x\sim P_x}[logD_{X_\uparrow}(U_{Y_\downarrow Y}\circ G_{XY_\downarrow}(x))]\\+\mathbb{E}_{y_\downarrow \sim P_{Y_\downarrow}}[log(1-D_{X_\uparrow}(U_{Y_\downarrow Y}(\mathring{y_\downarrow})))]</script><p>cycle consistency loss 被放松到只有单向的：</p><script type="math/tex; mode=display">\mathcal{L}_{cyc}=||G_{XY_\downarrow}\circ G_{Y_\downarrow X}(y_\downarrow)-y_\downarrow||_1</script><p>这使$G_{Y_\downarrow X}$可以一对多，满足不同的噪声和LR图像的分布。</p><p>identity loss在cycleGAN里用来保持图像的色彩，本文中也用了identity loss来避免色彩偏差:</p><script type="math/tex; mode=display">\mathcal{L}_{idt}(G_{XY_\downarrow})=||G_{XY_\downarrow}(y_\downarrow)-y_\downarrow||_1</script><p><strong>geometric ensemble loss</strong> [CVPR 2019: Geometry consistent generative adversarial networks for one-sided unsupervised domain mapping] 减少可能的translation来保持场景的几何形状。本文中的geometric ensemble loss用来保证输入图像翻转、旋转不改变结果。</p><script type="math/tex; mode=display">\mathcal{L}_{geo}(G_{XY_\downarrow})=||G_{XY_\downarrow}(x)-\sum_{i=1}^{8}T_i^{-1}(G_{XY_\downarrow}(T_i(x)))/8||_1</script><p>共带有8中翻转旋转模式。</p><p>而SR网络$U_{Y_\downarrow Y}$是与生成器、判别器无关的网络，只是用来放大图像的局部特征来作为HR端判别器的输入。用下式来更新SR网络:</p><script type="math/tex; mode=display">\mathcal{L}_{rec}=||U_{Y_\downarrow Y(\mathring y_{\downarrow})}-y||_1</script><p>这里的$\mathcal{L}_{rec}$可以由任意的pixel wise的loss代替。（perceptual loss, texture loss, adversarial loss)</p><h3 id="network-architecture"><a href="#network-architecture" class="headerlink" title="network architecture"></a>network architecture</h3><p>最上面那路的$G_{XY_\downarrow}$和 $U_{Y_\downarrow Y}$用RCAN的网络</p><p>而$G_{Y_\downarrow X}$的网络结构：resBlock+fusion layers+BN+Leaky ReLU</p><p>判别器：patchGAN，LR的判别器的stride=1,5层卷积, HR的判别器前面几层stride=2.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="synthetic-distortions"><a href="#synthetic-distortions" class="headerlink" title="synthetic distortions"></a>synthetic distortions</h3><p>DIV2K realistic-wild dataset (800 训练图像)</p><p>simulate ： 4倍下采样、运动模糊、pixel shifting、加性噪声</p><p>每张图只有一种degradation，但是图像与图像之间的degradation不同，对于每张训练图像，合成4张降质图像。</p><p>训练：800 HR+3200 LR（unpair）</p><p>测试：100张validation set</p><p>超参：$\lambda_{cyc}=1, \lambda_{idt}=1,\gamma=0.1$, 4倍SR</p><p><strong>intermediate images</strong> </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161744947.png" alt></p><p><strong>compare with blind methods</strong> </p><p>blind denoising: <strong>NC</strong> [The noise clinic: a blind image denoising algorithm], <strong>RL-restore</strong> [ CVPR 2018: Crafting a toolchain for image restoration by deep reinforcement learning]</p><p>【blind denoising还有CVPR 2019: Toward Convolutional Blind Denoising of Real Photographs (Kai Zhang)】</p><p>blind deblurring: <strong>SRN-Deblur</strong>[CVPR 2018: Scale-recurrent network for deep image deblurring], <strong>DeblurGAN-v2</strong> [Arxiv2019：Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better]</p><p>SR: DBPN(non-blind), <strong>ZSSR</strong>, <strong>IKC</strong> (blind)（zssr: CVPR2018, IKC: cvpr 2019)</p><p>ZSSR+KernelGAN </p><p>这些方法是用的各自论文里提及的数据集训练，没有在这里的数据集上训练。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161830641.png" alt></p><p><strong>compare with NTIRE 2018 baselines</strong> 这些baseline是pair-trained （upper bounds) 这篇论文的方法PSNR比不过 baseline方法，但是SSIM与baseline方法相当. 因为PSNR高估了整体的亮度和色彩的细微差别，这些差别不会显著影响perceptual quality。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161857570.png" alt></p><p><strong>ablation study</strong> </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409161919268.png" alt></p><p>第2行：去除HR的判别器</p><p>第3行：SR network是在$y_\downarrow$上训练的，而不是$\mathring{y_\downarrow}$上，这就相当于一个real LR和Gaussian+bicubic LR之间translation的网络(cycle GAN)加一个SR网络。</p><p>第4行：SR network是在$G_{Y_\downarrow X}(y_\downarrow)$ 上训练的，而不是$\mathring{y_\downarrow}$上. 相当于图1 的b</p><p>第5行：在第4行基础上，用RCAN官方的模型做validation。</p><p><strong>perception-oriented training</strong> </p><p>在HR的$\mathcal{L}_{rec}$里加上perceptual loss, content loss (ESRGAN里面的)，relativistic adversarial loss.</p><p>加上这些loss 后视觉效果比L1 loss好</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162036375.png" alt></p><h3 id="realistic-distortion-1"><a href="#realistic-distortion-1" class="headerlink" title="realistic distortion 1"></a>realistic distortion 1</h3><p>follow unsupervised 人脸SR [ECCV 2018: To learn image super-resolution, use a gan to learn how to do image degradation ﬁrst]</p><p>HR face images: Celeb-A, AFLW, LS3D-W, VGGFace2 (64*64)</p><p>LR face images: 50000张Widerface 包含多种degradation（留出3000作为测试）(16*16)</p><script type="math/tex; mode=display">\mathcal{L}_{\bar{idt}}(G_{XY_\downarrow})=||G_{XY_\downarrow}(x)-x||_1</script><p>他们实验发现identity loss加在x上比加在$y_\downarrow$ 上好。</p><p>超参：$\lambda_{cyc}=1,\lambda_{\bar{idt}}=2,\lambda_{geo}=1,\gamma=0.1$</p><p>训练2倍SR：32*32=&gt;64*64</p><p>视觉指标FID比较：高亮的为基于GAN的unpaired的方法。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162101131.png" alt></p><p><strong>one-to-many degradation</strong> </p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162120879.png" alt></p><h3 id="realistic-distortion-2"><a href="#realistic-distortion-2" class="headerlink" title="realistic distortion 2"></a>realistic distortion 2</h3><p>aerial  image dataset DOTA</p><p>GSD (ground sample distances), </p><p>62 LR images GSD在[55cm, 65cm]之间， HR image的GSD为30cm</p><p>超参：$\lambda_{cyc}=1,\lambda_{\bar{idt}}=10,\lambda_{geo}=100,\gamma=0.1$ , 2倍SR</p><p>在这种数据集里，物体的像素点很少，所以对identity loss和geometric loss用了更大的权重。在训练初期，逐步提高geometric loss的权重。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162312918.png" alt></p><p>只提供视觉上的比较，因为没有GT。</p><p>先用RL-restore（强化学习blind去噪修复）在input的LR image上去噪。但是他的输出over-smoothed. 即使再用SOTA的blind SR方法ZSSR超分辨，artifacts也不能被完全移除。</p><p>geometric loss的作用：</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162239831.png" alt></p><h3 id="AIM-2019-real-world-SR-challenge"><a href="#AIM-2019-real-world-SR-challenge" class="headerlink" title="AIM 2019 real world SR challenge"></a>AIM 2019 real world SR challenge</h3><p>没有HR-LR pair,测试时有官方的脚步来计算PSNR/SSIM。 $\lambda_{cyc}=1,\lambda_{\bar{idt}}=5,\lambda_{geo}=1,\gamma=0.1$</p><p>​        LPIPS: 视觉指标，越低越好。</p><p><img src="/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409162216094.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/image-20210409155557355.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2020&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/tags/super-resolution/"/>
    
      <category term="pseudo-supervision" scheme="https://shiyuuuu.github.io/tags/pseudo-supervision/"/>
    
      <category term="unpaired" scheme="https://shiyuuuu.github.io/tags/unpaired/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Dual super-resolution learning for semantic segmentation</title>
    <link href="https://shiyuuuu.github.io/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/"/>
    <id>https://shiyuuuu.github.io/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/</id>
    <published>2021-04-08T16:00:00.000Z</published>
    <updated>2021-04-09T08:49:26.193Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164117461.png" alt></p><p>出处：CVPR2020 (oral)</p><a id="more"></a><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/wanglixilinx/DSRL">code</a>  <a href="https://github.com/Dootmaan/DSRL" target="_blank" rel="noopener">unofficial-code</a></p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>在不增加计算开销的前提下提高语义分割的性能，而语义分割依赖于HR feature representations，如果直接输入HR images会产生很大的计算开销，所以做SSSR。而只依赖于SSSR的decoder难以恢复original details，所以他们用了SISR来guide SSSR，如何guide呢？用feature affinity module，核心是feature相似矩阵的距离。</p><h2 id="related-works"><a href="#related-works" class="headerlink" title="related works"></a>related works</h2><p>现有的语义分割方法能取得好的性能依赖于HR的深度特征表示：large computation budgets</p><p>现有的语义分割方法保持HR representations:</p><ol><li>用空洞卷积代替 strided 卷积，比如DeepLabs</li><li>结合HR的pathway，比如Unet</li></ol><p>这些方法的输入通常是original HR image，当限制输入图像的大小的时候，他们的性能有明显的下降。（现有语义分割方法：FCN, DeepLabs, PSPNet, 空洞卷积，pyramid pooling module, attention， context encoding）</p><p>现有的轻量级的语音分割：通过factorization加速卷积，ESPNets（split-merge，reduce-expand加速卷积计算），采用一些有效的分类网络（mobileNet、shuffleNet），知识蒸馏帮助训练对抗网络。但他们的性能比SOTA差很多。</p><p>本文提出一种two-stream的framework (dual super-resolution learning, DSRL) 在不产生额外计算开销的情况下提高semantic 分割的准确率: 对于LR的输入保持HR representations。具体的，SISR得到的HR features用来guide spatial维度的相关性学习。DSRL可以在相同的resolution下，显著提高准确率。</p><p>现有的SISR方法：</p><ol><li>pre-upsampling SR：先通过bicubic上采样得到HR图像，再用网络refine HR图像（计算开销大：网络在HR上做的）</li><li>post-upsampling SR：在网络的最后面用可学习的上采样层</li><li>progressive SR：逐渐提高分辨率，可以handle multi-scale的SR（deep laplacian pyramid networks）</li><li>iterative up-and-down SR：通过iterative上\下采样的层得到中间图像，结合中间图像重建最终图像(deep back-projection networks) </li></ol><p>multi tasks：</p><p>mask R-cnn（检测+实例分割）</p><p>RCNN（姿态估计+动作识别）</p><p>cross tasks：</p><p>（希望把语义分割作为主要任务，SISR作为附加任务）</p><h2 id="proposed-method"><a href="#proposed-method" class="headerlink" title="proposed method"></a>proposed method</h2><h4 id="Review-of-encoder-decoder-framework"><a href="#Review-of-encoder-decoder-framework" class="headerlink" title="Review of encoder-decoder framework"></a>Review of encoder-decoder framework</h4><p>用来提取特征的encoder的scaling step是2，OS通常是8或者16（the ratio of input image spatial resolution to the Encoder output resolution），把最后两层strided conv换成空洞卷积。在decoder端，用一个bilinear 上采样层恢复分辨率。</p><p>现有的方法只能将feature上采样至与input image同样的大小，可能比original image小。（分割网络的输入往往是对原图做了下采样）。这样可能损失了一部分有用的label信息，另一方面，也难以只依赖decoder恢复original details。</p><h4 id="DSRL"><a href="#DSRL" class="headerlink" title="DSRL"></a>DSRL</h4><p><strong>contribution</strong>：</p><ol><li>在不额外增加计算开销的前提下，通过保持HR representations，提高性能</li><li>泛化性：可以扩展到需要HR representation的任务中，比如人体姿态估计</li><li>实验：在语音分割和人体姿态估计任务上都获得了良好的性能，相同计算开销，提高2%</li></ol><p>包含三个部分：</p><ol><li>semantic segmentation super-resolution (SSSR)</li><li>single image super-resolution (SISR)</li><li>feature affinity (FA)</li></ol><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164518013.png" alt></p><p>其中SISR与SSSR共享特征提取器，</p><p><strong>SSSR：</strong>加额外的上采样层（一些deconv）来得到最终的prediction mask，比如输入的是512<em>1024，输出1024\</em>2048. 他们的方法可以利用全部的label</p><p><strong>SISR：</strong>只依赖decoder不足够恢复HR semantic特征表示，因为decoder的上采样结构不是简单的bilinear上采样，就是简单的sub-net，这并不会引入额外的信息，因为输入是LR的。 而SISR可以有效恢复图像细节，SSSR和SISR的feature如下图所示，SISR的feature包含更多的物体更多的复杂结构，尽管这些结构不能直接揭示物体属于哪一类，但他们可以根据像素与像素、区域与区域直接的相关性group起来，而这些像素区域的关联揭示了语义信息。</p><p>所以，SISR得到的HR feature用来guide SSSR的HR feature的学习。SISR根据original image的GT来优化。</p><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164654590.png" alt></p><p><strong>FA: feature affinity</strong> SISR得到的structure information如何guide SSSR？用 feature affinity learning，FA来学习SSSR和SISR feature上相似矩阵的距离，相似矩阵刻画的是像素直接pairwise的关系。理论上，应该计算所有像素对的affinity，为了节省计算开销，他们subsamples得到1/8的像素对。为了减少由SISR和SSSR不一致引起的训练的不稳定性，他们还附加了一个feature transform模块。</p><script type="math/tex; mode=display">S_{ij}=(\frac{F_i}{||F_i||_p})^{T}(\frac{F_j}{||F_j||_p})</script><p>最后的FA loss为：</p><script type="math/tex; mode=display">    L_{fa}=\frac{1}{W^2H^2}\sum_{i=1}^{W'H'}\sum_{j=1}^{W'H'}||S_{ij}^{seg}-S_{ij}^{sr}||_q</script><p>$p=2, q=1$，总的loss：</p><script type="math/tex; mode=display">L=L_{ce}+w_1L_{mse}+w_2L_{fa},\\</script><p>其中，</p><script type="math/tex; mode=display">L_{ce}=\frac{1}{N}\sum_{i=1}^{N}-y_ilog(p_i),\\L_{mse}=\frac{1}{N}\sum_{i=1}^{N}||SISR(X_i)-Y_i||^2</script><p>$L_{ce}$ 为cross entropy loss, $w_1=0.1, w_2=1.0$</p><h2 id="experiments"><a href="#experiments" class="headerlink" title="experiments"></a>experiments</h2><p>semantic segmentation：CitySpace数据集，将一张图分为19类。1024<em>2048；CamVid数据集，11类，960\</em>720。</p><p>metric：mIoU（mean Intersection over Union）</p><p>segmentation architecture: ESPNetv2, DeepLabv3+(ablation study), PSPNet, BiseNet, DABNet (lightweight)</p><h4 id="effect-of-components"><a href="#effect-of-components" class="headerlink" title="effect of components"></a>effect of components</h4><p>输入：256<em>512(resize, 1024\</em>2048=&gt;256*512)</p><p>输出：+SR的输出是512<em>1024（2倍），不加SR的输出：256\</em>512</p><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164919074.png" alt></p><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164720479.png" alt></p><p>+SSSR+SISR+FA &gt; +SSSR+SISR &gt; SSSR &gt; 不加</p><p>原图下采样作为baseline和自己方法的输入，输出分辨率不同</p><p>【输出分辨率不同怎么比的？】</p><ol><li><p>把原图的label分别下采样到256*512和512*1024比</p></li><li><p>output统统上采样到原图的分辨率（1024*2048）和label比。（保持同分辨率下比较，并且不对原始的label降质）</p></li></ol><p>第2 种比较相当于比的是bicubic+seg和SR+seg</p><h4 id="effect-of-various-input-resolutions"><a href="#effect-of-various-input-resolutions" class="headerlink" title="effect of various input resolutions"></a>effect of various input resolutions</h4><p>input resolution：256*512, 320*640, 384*768, 448*896, 512*1024 (CitySpace)</p><p>对于每一种input resolution，用他们的framework都比不用性能好，且随着input resolution逐渐增大，性能增值越来越小。</p><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164742539.png" alt></p><h4 id="human-pose-estimation"><a href="#human-pose-estimation" class="headerlink" title="human pose estimation"></a>human pose estimation</h4><p>metric：object keypoint similarity (OKS)</p><p>architecture: HRNet-w32, 用offline person detection的结果预测关键点</p><p>输入human detection box (缩放到固定大小：256<em>192， 162\</em>128, 128*96)</p><p>输出heatmap（64*48）</p><h4 id="results"><a href="#results" class="headerlink" title="results"></a>results</h4><p>visualization of segmentation features</p><p><img src="/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164851703.png" alt></p><p>对于structured objection提升尤其明显，比如人，车</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/image-20210409164117461.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2020 (oral)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/tags/super-resolution/"/>
    
      <category term="segmentation" scheme="https://shiyuuuu.github.io/tags/segmentation/"/>
    
      <category term="LV+HV" scheme="https://shiyuuuu.github.io/tags/LV-HV/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Learning to Have an Ear for Face Super-Resolution</title>
    <link href="https://shiyuuuu.github.io/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/"/>
    <id>https://shiyuuuu.github.io/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/</id>
    <published>2021-04-08T16:00:00.000Z</published>
    <updated>2021-04-09T09:29:10.045Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170554550.png" alt></p><p>出处：CVPR2020 (oral)</p><a id="more"></a><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Meishvili_Learning_to_Have_an_Ear_for_Face_Super-Resolution_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://github.com/gmeishvili/ear_for_face_super_resolution" target="_blank" rel="noopener">code</a>  <a href="https://gmeishvili.github.io/ear_for_face_super_resolution/" target="_blank" rel="noopener">project</a></p><p>task: 用audio和LR图像做16倍的人脸超分辨率，输入的LR图像非常小：8*8 pixels, 这些图像的很多重要细节都被丢失了。如果LR的人脸图像是从视频中提取的，那么我们也可以得到这个人的音频信息，而audio中带有一些脸部的特征：性别和年龄。结合听觉和视觉，他们提出了一个：先从单独的音轨构建脸部特征的latent 表示，再从LR图像简历脸部特征潜在的表示。然后再fusion这两个表示。</p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ol><li><p>limited information</p><p> <img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170710329.png" alt></p></li><li><p>ambiguous mapping</p></li></ol><p>=&gt; incorporate alternative source of information: audio</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170737702.png" alt></p><ol><li>audio carries information about age and gender, audio tracks are available in videos, audio and visual signals both capture some shared attributes of a person.</li></ol><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ol><li>the first attempt to use audio for image restoration</li><li>use both audio and a LR image to perform extreme face super-resolution ($16\times$) </li><li>do not use human annotation and thus can be easily trained with video datasets</li><li>建立了图像和音频的分解表示，因此它可以混合来自不同视频的LR图像和音频，并生成语义上有意义的真实面孔。（our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations.  ）</li></ol><h2 id="related-works"><a href="#related-works" class="headerlink" title="related works"></a>related works</h2><h3 id="speech2Face"><a href="#speech2Face" class="headerlink" title="speech2Face"></a>speech2Face</h3><p>audio to image: speech2Face [CVPR 2019:  Speech2face: Learning the face behind a voice]</p><h3 id="styleGAN"><a href="#styleGAN" class="headerlink" title="styleGAN"></a>styleGAN</h3><p>给定512维随机向量， styleGAN可以从这个向量重构维一张从未见过的人脸照片。</p><p>如果训练一个model，是从图片到512维latent encoding, 这个latent encoding可以通过styleGAN还原为原图。（图=&gt;512维latent code=&gt;原图 ）</p><p>也就是说，可以用这512维向量生成原图。</p><p>如果对这512维的空间稍微做点改变？这个超高维的空间对应人脸的不同属性（肤色、年龄、性别）</p><p>如何知道年龄对应的维度是哪些？</p><blockquote><p>1、作者一开始会训练一个分类器，分类器的训练样本、label来自于CelebA，40维label中就有一个维度是年龄（作者贴出来的代码里的wp.npy文件我猜就是CELEBA的label文件）</p><p>2、分类器收敛后，用个随机噪声z作为stylegan输入，生成图片x，再把这个x送入分类器，得到分类结果y，用个线性变换（或非线性也行）建立起z与y的关系。。。。然后你就可以通过控制y的变换方向来得到z，再生成想要的x了</p></blockquote><p>总之，styleGAN可以从latent code生成逼真的人脸，更重要的是latent code对应人脸不同属性，可以通过改变latent code得到不同的人脸。（比如得到XX小时候的照片）。</p><h3 id="Naive-end-to-end-training"><a href="#Naive-end-to-end-training" class="headerlink" title="Naive end-to-end training"></a>Naive end-to-end training</h3><p>带有两个编码网络的多模态网络，再把encoder的输出concate再一起送入decoding网络得到HR图像。但这种多模态网络以传统的训练方法很难训练好，因为：不同模态的收敛速度不同。</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170854501.png" alt></p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170915033.png" alt></p><p>实验发现，这样直接一起训练会忽略audio信号。audio信号需要更长的处理更强的网络来拟合其latent space。</p><h2 id="method"><a href="#method" class="headerlink" title="method"></a>method</h2><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171414333.png" alt></p><p>包含这样几个部分：LR encoder $E_l$, audio encoder $E_a$，fusion network $F$, face generator$G$  </p><p>分开训练LR image encoder和audio encoder，这样他们的解耦精度就相等。fusion: audio作用在LR image固定的中间representation中，这样audio中的人脸属性可以解耦出来。</p><p>为什么styleGAN？styleGAN可以通过操控latent code的某些维度，改变生成人脸的某些属性。</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171047634.png" alt></p><h3 id="Inverting-the-Generator"><a href="#Inverting-the-Generator" class="headerlink" title="Inverting the Generator"></a>Inverting the Generator</h3><ol><li><p>首先训练一个从高斯隐空间$z \sim \mathcal{N}(0,I_d)$（d维）开始的输出高分辨率图像的generator $G(z)$ 【styleGAN: 人脸生成，可以控制所生成图像的高层级属性：头发等，以高斯分布作为输入，输出高质量的samples和隐空间】，再，用自编码器的限制，通过固定$G(z)$得到generator(fixed) 反转后的encoder ($E_h$). 再将这两部分$G(z)$和$E_h$fine-tuning。这个encoder的映射是HR image到generator输入的隐空间，这个$E_h$可以得到图像$x_i$对应的representations ($z_i$)<strong>可以当做LR,audio，fusion network的encoder的目标</strong>，generator的输出是input HR的近似。【这个模型可以作为生成HR人脸图像的先验，并且中间的representations应该可以由audio编辑】</p><p>给定数据集$\mathcal{D}=\{(x_i^h,x_i^l,a_i)|i=1,…,n \}$, </p></li></ol><script type="math/tex; mode=display">\mathcal{L}_{pre-train}=\sum_{i=1}^{n}|G(z_i)-x_i^h|_1+\lambda_f\mathcal{l}_{feat}(G(z_i),x_i^h)</script><p>其中$z_i=E_h(x_i^h)$ ，$l_{feat}$ 是perceptual loss (VGG feature)</p><p>他们实验发现只回归一个$z_i$不足以很好的恢复$x_i^h$，所以像styleGAN一样，把$z_i$非线性变换得到$w_i$，所以他们生成$k$个不同的$z_{ij},j=1,…,k$. 将非线性变换得到的$w_{ij}$分别插入到generator的不同层。</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171104847.png" alt></p><p>再fine-tuning：</p><script type="math/tex; mode=display">min_{E_h,G}\mathcal{L}_{pre-train}+\lambda_t|G_{init}-G|_2^2</script><p>其中$G_{init}$是styleGAN训练后$G$的权重。训练过程中，总的loss最小后，将$\lambda_t$减小为原来1/2，pre-training和减小正则化：<strong>让encoder和decoder逐渐收敛，不至于过早失去G的latent representation的结构</strong></p><p>总结，(1). 从标准高斯分布学习G(z)（styleGAN），获得HR 图像的分布</p><p>(2). 固定$G(z)$, 用公式1用autoencoder的方式训练reference encoder$E_h$, </p><p>(3). fine-tuning $G(z)$和$E_h$，$E_h$可以得到HR图像的latent representations.</p><h3 id="Pre-training-Low-Res-and-Audio-encoders"><a href="#Pre-training-Low-Res-and-Audio-encoders" class="headerlink" title="Pre-training Low-Res and Audio encoders"></a>Pre-training Low-Res and Audio encoders</h3><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171237776.png" alt></p><p>给定HR-LR pair，pre-train一个LR encoder，将输入的LR映射到与HR相同的的reference encoder（前面训练的）输出的latent representations</p><p>如果直接训练fusion network，使fusion model$F(x_i^l,a_i)$映射到$z_i=E_h(x_i^h)$，会使网络完全忽略音频信号$a_i$。所以他们先分开训练encoder$E_l$和$E_a$， 让他们尽可能从这两种模态多提取信息，再fusion他们。</p><p>为了防止过拟合，pre-train $E_l$和$E_a$时，只用一半数量的训练数据（记为$\mathcal{D}_{pre}$），在fusion的训练阶段用整个训练数据。</p><p>训练$E_l$的目标函数：</p><script type="math/tex; mode=display">min_{E_l}\sum_{x_i^l,x_i^h \in \mathcal{D_{pre}}}|E_l(x_i^l)-z_i|_1+\lambda|D \circ G(E_l(x_i^l))-x_i^l|_1</script><p>其中$D \circ x$是$x$的16倍下采样。$\lambda=40$.</p><p>前面一项很好理解：希望$E_l$在LR图像$x_i^l$上得到的latent representation与HR图像上的latent representation($z_i=E_h(x_i^h)$)相似。第二项：LR图像$x_i^l$经过encoder(得到HR图像的latent representation)再经过decoder$G$（得到HR图像$x_i^h$）再16倍下采样得到LR 图像$x_i^l$  （因为$x_i^l$与$x_i^h$成pair）</p><p>而对于音频encoder：如果将$E_a(a_i)$回归到$z_i$必然有overfitting，因为一些$z_i$中有的属性，$a_i$中没有，比如脸部的姿态(朝左朝右？)。为了消除$z_i$中与$a_i$无关的属性，将$E_a(a_i)$的目标定义为：</p><script type="math/tex; mode=display">\bar{z_i}=\frac{1}{2}(E_h(x_i^h)+E_h(\hat{x}_i^h))</script><p>这里的 $\hat{x}_i^h$是$x_i^h$的水平翻转版本（将原HR图像水平翻转）。</p><p>训练$E_a$的目标函数： </p><script type="math/tex; mode=display">min_{E_a}\sum_{a_i,x_i^h \in \mathcal{D}_{pre}}|E_a(a_i)-\bar{z_i}|_1</script><p>【这里没懂】由于styleGAN的分层的结构，水平翻转的图片的latent code求平均，就消除了音频无法传递的信息。比如图里，输入朝左的面部图像和它水平翻转后的图像（这个水平翻转图像的面部朝向是朝右的），把encoder提取到的latent code求平均，再经过decoder，就得到了朝向正面的图像（消除了音频无法传递的面部朝向信息）。</p><p>小结：</p><p>audio encoder, fusion network: 固定LR image encoder， 提升他的latent representation 。为了加快audio encoder的训练速度，将HR reference encoder的输出和其水平镜像的平均值作为latent representation 对audio encoder预训练。而这个平均消除了音频无法传递的信息，比如视点。</p><h3 id="fusing-audio-and-low-resolution-encodings"><a href="#fusing-audio-and-low-resolution-encodings" class="headerlink" title="fusing audio and low-resolution encodings"></a>fusing audio and low-resolution encodings</h3><p>现在希望聚合pre-train得到的encoder$E_l$和$E_a$提取的信息。由于$E_l$已经是$E_h$的近似，那么希望引入的音频能补出residual：$\Delta z_i=z_i-z_i^l$, 所以fusion network $F$应满足：</p><script type="math/tex; mode=display">z_i^f=E_l(x_i^l)+F(E_l(x_i^l),E_a(a_i))</script><p>因为$E_a$更难训练，所以继续把$E_a$和$F$一起优化。所以，fusion network的优化目标是:</p><script type="math/tex; mode=display">min_{E_a,F}=\sum_{a_i,x_i^h,x_i^l \in \mathcal{D}}|z_i^f-z_i|_1+\lambda|D \circ G(z_i^f)-x_i^l|_1</script><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171311165.png" alt></p><p>(a). matching inputs; (b) LR image与audio来自不同video</p><p>把LR image(8*8)输入到$E_l$得到latent representation, fusion network 融合$E_a$编码后的音轨和encoded LR image, 这一部分与前面的latent representation相加得到的新的latent representation 与预先训练得到的HR image的latent representation很相似，再通过decoder G输出HR图像</p><hr><p>共有3个mappings： </p><ol><li>audio to HR (speech2face是用预训练的人脸识别网络作为额外监督，而我们的方法是完全无监督的)</li><li>LR to HR</li><li>LR+audio to HR</li></ol><hr><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p><strong>dataset</strong>： VoxCeleb2 Dataset, 包含145K 人说话的video</p><p>2M frames at 128*128 pixels. 将每个speaker的一半的数据放入$\mathcal{D_{pre}}$</p><p>test set: 同人的不同video.</p><h3 id="audio-to-image"><a href="#audio-to-image" class="headerlink" title="audio to image"></a>audio to image</h3><p>简单的比较了一下他们的audio-only model ($E_a+G$) 和speech2Face</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172555985.png" alt></p><p>第一行是speech2face的结果，第二行是他们的audio2image的结果 ($E_a+G$)</p><p>性别分类：96%~97%准确率（没有在性别分类上与speech2face比，因为speech2face训练时用了分类器的监督）</p><h3 id="classification-as-a-performance-measure"><a href="#classification-as-a-performance-measure" class="headerlink" title="classification as a performance measure"></a>classification as a performance measure</h3><p>预训练好的  身份分类器、性别分类器、年龄分类器</p><p>closed set：training set和test set用同一个人的不同video</p><p>open set：training set和 test set不同人</p><p><strong>ablations</strong> </p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172636752.png" alt></p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409171454635.png" alt></p><p>(a), (b): 训练$E_h$后时候和$G$一起fine-tune 【(b): HR 图像的表现，upper bound】</p><p>(c), (d)：without fusion F。只要LR 或者只要音频</p><p>​            c与d相比，Audio更能提供性别信息。$\mathcal{C}_g$</p><p>(e)~(h): (f)(g)(h)相比：不加audio&lt;固定audio encoder&lt;fine-tuning </p><p>​            (e)与(h)相比：一个全连接层&lt;三个全连接层</p><p>​            (g)与(h)相比：在训练fusion网络时也fine-tuning $E_a$ 对结果有些许提升。</p><p>gender和age在open set上也能预测比较准确。</p><p><strong>comparisons to other SR methods</strong> </p><p>LapSR(CVPR 2017) , W-SRNet (ICCV 2017 人脸SR) 在这个数据集上重新训练。</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172740704.png" alt></p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172747369.png" alt></p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172758166.png" alt></p><h3 id="mixing"><a href="#mixing" class="headerlink" title="mixing"></a>mixing</h3><p>给定LR，与不同的audio混合</p><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172835492.png" alt></p><h3 id="failure-cases"><a href="#failure-cases" class="headerlink" title="failure cases"></a>failure cases</h3><p><img src="/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409172905749.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/image-20210409170554550.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2020 (oral)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/tags/super-resolution/"/>
    
      <category term="audio+image" scheme="https://shiyuuuu.github.io/tags/audio-image/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Unpaired Image Super-Resolution using Pseudo-Supervision</title>
    <link href="https://shiyuuuu.github.io/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/"/>
    <id>https://shiyuuuu.github.io/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/</id>
    <published>2021-04-08T16:00:00.000Z</published>
    <updated>2021-04-10T12:43:46.869Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210409174629273.png" alt></p><p>出处：CVPR2019 (oral)</p><a id="more"></a><p><a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.pdf" target="_blank" rel="noopener">paper</a>  <a href="https://openaccess.thecvf.com/content_CVPR_2019/supplemental/Dai_Second-Order_Attention_Network_CVPR_2019_supplemental.pdf" target="_blank" rel="noopener">supp</a>   <a href="https://www.youtube.com/watch?v=IyPQqqGPHws&amp;t=2822" target="_blank" rel="noopener">video</a>  <a href="https://github.com/daitao/SAN" target="_blank" rel="noopener">code</a></p><p><strong>bicubic SR 的 SOTA</strong></p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>现有的SISR方法专注于更宽更深的结构设计，而忽略了挖掘中间层特征之间的相关性。本文受启发于ICCV 2017一篇做识别分类的文章（利用了二阶统计信息），提出了一个深度二阶注意力网络SAN 以获得更好的特征表达和特征相关性学习。特别地，提出了一个二阶通道注意力机制<strong>SOCA</strong>来进行相关性学习。同时，提出了一个non-locally增强残差组NLRG来捕获长距离空间内容信息。性能方面取得了SISR最好的结果。</p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ul><li>设计了一个利用了二阶统计信息的网络：second-order attention network (SAN)</li><li>特别的，SAN网络包含second-order channel attention (SOCA)模块</li><li>然后还结合了non-locally enhanced residual group (NLRG)来捕获长距离空间内容信息。</li></ul><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210409175308354.png" alt="image-20210409175308354"></p><h3 id="RL-NL模块"><a href="#RL-NL模块" class="headerlink" title="RL-NL模块"></a>RL-NL模块</h3><p>non-local模块是用来在high-level任务中捕获整幅图像的长范围依赖的。但是，全局non-local操作可能会受限于：<br>1)全局non-local操作需要大量的计算量; 2)对于low-level的任务来说，在一定的区域范围中进行non-local操作是更有效的<br>所以在SAN里，将图像划为k*k个region, 在每个region中进行non-local 操作，RL-NL充分利用LR feature的结构关联</p><h3 id="LSRAG模块"><a href="#LSRAG模块" class="headerlink" title="LSRAG模块"></a>LSRAG模块</h3><p>每一个同源残差组结构（SSRG）包括G 个局部模块LSRAG加上一个同源残差连接结构SSC，简单的LSRAG堆叠不能取得更好的性能，于是加入SSC（share-source skip connections），使LR的低频信息通过，且利用训练。<br>所谓同源残差连接，就是把LR的特征加到每个group的输入x中，这种连接不仅可以帮助深度CNN的训练，同时还可以传递LR图像中丰富的低频信息给high-level的层。<br>Wssc是一个可学习参数，一开始被设置为0。对于每个group来说，都会收到SSC传递过来的F0</p><p>每个LSRAG包括M个简单的residual blocks 和local source skip connection，最后还包含一个二阶channel attention 模块（second-order channel attention module）<br>在LSRAG模块最后有一个二阶channel attention 模块，这是本文的重点</p><h3 id="关于channel-attention"><a href="#关于channel-attention" class="headerlink" title="关于channel attention"></a>关于channel attention</h3><p><strong>RCAN</strong>中：</p><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410200452480.png" alt></p><p>如上图所示，输入是一个 H×W×C 的特征，我们先进行一个空间的全局平均池化得到一个 1×1×C 的通道描述。接着，再经过一个下采样层和一个上采样层得到每一个通道的权重系数，将权重系数和原来的特征相乘即可得到缩放后的新特征，整个过程实际上就是对不同通道的特征重新进行了加权分配。<br>其中，下采样和上采样层都利用 1×1 的卷积来实现，下采样层的通道数减少 r 倍，f 是激活函数 sigmoid</p><p>文本的二阶channel attention和RCAN的channel attention的区别在于将 全局平均池化 替代为全局协方差池化。</p><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410200550769.png" alt></p><p>文本的二阶channel attention和RCAN的channel attention的区别在于将 全局平均池化 替代为全局协方差池化。</p><h3 id="协方差归一化（就是所谓的二阶）"><a href="#协方差归一化（就是所谓的二阶）" class="headerlink" title="协方差归一化（就是所谓的二阶）"></a>协方差归一化（就是所谓的二阶）</h3><p>协方差可以用来描述变量之间的相关性，所以对于HxWxC的特征，reshape为WH（C个维度），可以用协方差矩阵描述C个通道之间的相关性。<br>$I$和1分别是sxs的单位矩阵和全1矩阵。因为这个协方差矩阵是个实对称矩阵，实对称矩阵必可由正交矩阵对角化，可对协方差矩阵作特征值分解，U为正交矩阵，Λ为对角阵，对角元素为特征值，按非增顺序排列。再对协方差做正则化，因为协方差的正则化对表示特征非常重要。Alpha=0.5</p><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410200836365.png" alt></p><p>在具体实现上，在SSRG模块里包含20个LSRAG，和一前一后共两个region-level non-local模块，RL-NL中将图像分为2*2的region，在每个LSRAG里用了10个residual blocks.<br>上采样模块用的是sub-pixel上采样。</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="ablation-study"><a href="#ablation-study" class="headerlink" title="ablation study"></a>ablation study</h3><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410204057798.png" alt></p><h3 id="与其他SR方法对比"><a href="#与其他SR方法对比" class="headerlink" title="与其他SR方法对比"></a>与其他SR方法对比</h3><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410204311454.png" alt></p><p>这个是视觉上的比较，对于丰富复杂的纹理，SAN能更好的恢复</p><p><img src="/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210410204337413.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/image-20210409174629273.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2019 (oral)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/tags/super-resolution/"/>
    
      <category term="pseudo-supervision" scheme="https://shiyuuuu.github.io/tags/pseudo-supervision/"/>
    
      <category term="unpaired" scheme="https://shiyuuuu.github.io/tags/unpaired/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-GLEAN Generative Latent Bank for Large-Factor Image Super-Resolution</title>
    <link href="https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/"/>
    <id>https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-04-09T07:54:01.237Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329170458177.png" alt></p><p>出处：CVPR2021 (oral)</p><a id="more"></a><p><a href="https://ckkelvinchan.github.io/projects/GLEAN/" target="_blank" rel="noopener">project</a>  code <a href="https://arxiv.org/pdf/2012.00739.pdf" target="_blank" rel="noopener">paper</a></p><p>任务是： 大尺度超分辨率（8$\times$ 到 64$\times$），most details and textures are lost during downsampling.</p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ul><li>已有的SR方法：<ul><li>solely rely on $L_2$ loss: 视觉质量不好 (over-smoothing artifacts)。</li><li>adverssrial loss [ESRGAN]: Generator既要捕获图像characteristics又要保真（maintaining the fidelity to the GT）, 限制了其近似自然图像的能力，产生artifacts</li><li>GAN inversion methods [PULSE]：反转pre-trained GAN的生成过程：把image mapping回latent space；再由latent space中optimal vector重建图像。<strong>只靠低维隐向量不足以指导重建的过程</strong>，使得产生的结果low fidelity. 需要image-specific, iterative的优化.</li></ul></li><li>利用pre-trained GAN作为latent bank， 充分利用pre-trained GAN中封装的丰富且多样的先验。换用不同的bank可以不同类的图像：cat，building，human face，car. 利用字典学习的方式，测试阶段，只需要一次前传即可得到恢复后的图像。</li><li>GLEAN 的整体结构：encoder-bank-decoder</li></ul><h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><h3 id="large-factor-SR"><a href="#large-factor-SR" class="headerlink" title="large-factor SR"></a>large-factor SR</h3><ol><li>fully probabilistic pixel recursive network for upsampling extremely coarse images with resolution 8×8. (Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.)</li><li>RFB-ESRGAN：adopts multi-scale receptive fields blocks for 16× SR. (Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, and Yandong Guo. Perceptual extreme super resolution network with receptive field block. In CVPRW, 2020.)</li><li>VarSR: 8× SR by matching the latent distributions of LR and HR images to recover the missing details. (Sangeek Hyun and Jae-Pil Heo. VarSR: Variational super-resolution network for very low resolution images. In ECCV, 2020.)</li><li>perform 16× reference-based SR on paintings with a non-local matching module and a wavelet texture loss. (Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, and Yun Fu. Texture hallucination for large-scale painting super-resolution. In ECCV, 2020.)</li></ol><h3 id="GAN-inversion"><a href="#GAN-inversion" class="headerlink" title="GAN inversion"></a>GAN inversion</h3><ol><li><p>David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al. Semantic photo manipulation with a generative image prior. TOG, 2020.</p></li><li><p>Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code GAN prior. In CVPR, 2020.</p></li><li><p>Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. <strong>PULSE</strong>: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020.</p><p> 通过pixel-wise约束，迭代优化styleGAN的隐变量。</p></li><li><p>Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In ECCV, 2020.</p><p> finetune generator和latent code来缩小训练集和测试集分布的gap.</p></li></ol><p>降质图像$x$，latent space：$\mathcal{Z}$：</p><script type="math/tex; mode=display">z^*=argmin_{z \in \mathcal{Z}}\mathcal{L}(G(z), x)</script><p><strong><em>缺点：</em></strong></p><p>低维的隐向量不能保持图像的spatial information.</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>passing both the latent vectors and multi-resolution convolutional features from the encoder.</p><p>multi-resolution cues need to be passed from the bank to the decoder.</p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329194816287.png" alt></p><ul><li>整个结构为encoder-bank-decoder</li><li>encoder：$E_0$为RRDB-Net，$E_i, i \in\{1,…,N\}$代表堆叠一个stride=2的conv和一个stride=1的conv. 最后由FC层得到$C$, $C$表示隐向量，提供high-level信息。为了更好的指导结构重建，将多分辨率的特征和隐向量都送入bank.</li><li>Generative latent bank: 用pre-trained的Generator, styleGAN，提供纹理和细节生成的先验。<ul><li>对generator的每个block输入不同的隐向量$C_i$, $i \in \{0,…,k-1\}$</li><li>$\{g_i\}$代表每个block输出的feature, 它是由$C_i, g_{i-1}, f_{N-i}$由augmented style block得到。</li><li>不直接输出结果，而是将特征$\{g_i\}$输入到decoder</li><li>优势：像reference-based SR，HR reference image作为显式图像字典。性能很受 输入和reference相似度的影响。GLEAN用GAN-based的字典，不依赖于任何具体的图像，它获取的是图像的分布。而且没有global matching和reference images selection， 计算简便。</li></ul></li><li>decoder：progressive地聚合来自encoder和latent bank的特征。每个conv后跟着pixel-shuffle层。由于有encoder和decoder之间的skip-connection，encoder捕获的信息可以被强化，bank专注于纹理和细节的生成。</li><li>训练：$\mathcal{l_2}$ loss, perceptual loss, adversarial loss. 训练时fix住latent bank，实验发现，finetune latent bank没有性能提升，而且可能使latent bank偏向训练集的分布。</li></ul><h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329171413782.png" alt></p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203432053.png" alt></p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203534511.png" alt></p><h3 id="image-retouching"><a href="#image-retouching" class="headerlink" title="image retouching"></a>image retouching</h3><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203631010.png" alt></p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[ESRGAN]: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. ESRGAN: Enhanced super-resolution generative adversarial networks. In ECCVW, 2018. </p><p>[PULSE]: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020.</p><h3 id="reference-based-SR"><a href="#reference-based-SR" class="headerlink" title="reference-based SR"></a>reference-based SR</h3><ol><li>Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restoration via deep multi-scale component dictionaries. In ECCV, 2020.</li><li>Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020.</li><li>Xu Yan, Weibing Zhao, Kun Yuan, Ruimao Zhang, Zhen Li, and Shuguang Cui. Towards content-independent multi-reference super-resolution: Adaptive pattern matching and feature aggregation. In ECCV, 2020.</li><li>Yang Zhang, Ivor W Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, and Xin Yu. Copy and Paste GAN: Face hallucination from shaded thumbnails. In CVPR, 2020.</li><li>Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. In CVPR, 2019.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329170458177.png&quot; alt&gt;&lt;/p&gt;
&lt;p&gt;出处：CVPR2021 (oral)&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="Super-resolution" scheme="https://shiyuuuu.github.io/tags/Super-resolution/"/>
    
      <category term="GAN" scheme="https://shiyuuuu.github.io/tags/GAN/"/>
    
      <category term="highly ill-posed" scheme="https://shiyuuuu.github.io/tags/highly-ill-posed/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Zero-Reference deep curve estimation for low-light image enhancement</title>
    <link href="https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/"/>
    <id>https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-03-29T13:08:40.347Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210553083.png" alt><br>出处：CVPR2020<br><a id="more"></a></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper PDF</a></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank" rel="noopener">supplemental materials</a></p><p>project: <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank" rel="noopener">https://li-chongyi.github.io/Proj_Zero-DCE.html</a></p><p>code: <a href="https://github.com/Li-Chongyi/Zero-DCE" target="_blank" rel="noopener">https://github.com/Li-Chongyi/Zero-DCE</a></p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>图像编辑软件通过调节曲线来增强图像=&gt;image-specific 曲线估计：</p><p>根据给定图像估计pixel-wise的调整曲线。不需要任何成对或不成对的训练数据（不需要reference/GT) </p><p>图像增强=&gt;非线性曲线映射  而不是通过image-to-image mapping</p><p><strong>方法：</strong> non-reference loss functions</p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ol><li>no reference: 避免了需要paired/unpaired数据的方法中的overfitting的问题</li><li>设计image-specific曲线：高次、pixel-wise</li><li>提升人脸识别的性能</li></ol><h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>调整图像的直方图分布调整，增大图像的动态范围。</p><p>global level: </p><p>[1]. Dinu Coltuc, Philippe Bolon, and Jean-Marc Chassery. Exact histogram specification. IEEE Transactions on Image Processing, 15(5):1143–1152, 2006.   </p><p>[2]. Haidi Ibrahim and Nicholas Sia Pik Kong. Brightness preserving dynamic histogram equalization for image contrast enhancement. IEEE Transactions on Consumer Electronics, 53(4):1752–1758, 2007.</p><p>local level:</p><p>[3].  Chulwoo Lee, Chul Lee, and Chang-Su Kim. Contrast enhancement based on layered difference representation of 2d histograms. IEEE Transactions on Image Processing, 22(12):5372–5384, 2013.</p><p>[4]. J Alex Stark. Adaptive image contrast enhancement using generalizations of histogram equalization. IEEE Transactions on Image Processing, 9(5):889–896, 2000.</p><p>Retinex theory (将图像分解为reflectance和illumination，其中reflectance分量在任何光照条件下保持一致，图像质量增强任务变为illumination estimation问题):</p><p>[5].  Edwin H Land. The retinex theory of color vision. Scientific American, 237(6):108–128, 1977.</p><p>自然的信息保存方法：</p><p>[6].  Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Processing, 22(9):3538–3548, 2013.</p><p><strong>weighted vatiation:</strong></p><p>[7]. <em>Xueyang Fu,</em> Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016.</p><p>coarse illumination map: 搜索RGB中最大intensity的pixel</p><p>[8]. Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, 2017.</p><p>考虑噪声：</p><p>[9]. Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, 2018</p><p>自动exposure校正方法：</p><p>通过全局优化方法估计图像的S形曲线。</p><p>[10]. Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV, 2012. </p><h3 id="Data-driven方法"><a href="#Data-driven方法" class="headerlink" title="Data-driven方法"></a>Data-driven方法</h3><h4 id="CNN-based"><a href="#CNN-based" class="headerlink" title="CNN-based"></a>CNN-based</h4><p>一般需要pair对的数据：资源密集型(resource-intensive)。一般这种成对的数据通过  自动光照退化、改变相机的设置  采集  </p><p>或者用image retouching合成</p><ol><li><p><strong>LOL数据集</strong>[Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018.] 通过改变曝光时间和ISO获取成对的low/normal光照的图像。</p></li><li><p><strong>MIT-adobe FiveK数据集</strong>[Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo ´ Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR, 2011.]包括5000raw图，每一张raw图由专家生成5个retouched图像。</p></li></ol><p>[11]提出了一种估计illumination的方法：poor generalization capability</p><p>[11].  Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019.</p><h4 id="GAN-based"><a href="#GAN-based" class="headerlink" title="GAN-based"></a>GAN-based</h4><p>EnlightenGAN[Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. In CVPR, 2019.]</p><p>用unpair的low/normal光照的数据。需要仔细挑选unpaired的训练数据。</p><h2 id="methods"><a href="#methods" class="headerlink" title="methods"></a>methods</h2><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/b0821ec0907d0582c30b9aecd76e5b2.png" alt></p><p>通过non-reference loss function实现不需要paired/unpaired的数据。通过迭代训练得到结果。</p><h3 id="Light-enhancement-curve"><a href="#Light-enhancement-curve" class="headerlink" title="Light-enhancement curve"></a>Light-enhancement curve</h3><p>自适应的曲线参数只由输入图像决定。曲线要是单调的来保持周围像素的区别。曲线要处处可微保证可以梯度反传。</p><script type="math/tex; mode=display">LE(I(x);\alpha)=I(x)+\alpha I(x)(1-I(x))</script><p>其中$\alpha \in [-1,1]$, 将LE曲线分别作用于R\G\B通道，而不仅仅作用于illumination通道。调节3个通道可以更好的保持固有的色彩，减少过饱和。</p><p>上式可以多次迭代，写为：</p><script type="math/tex; mode=display">LE_n (x)=LE_{n-1}(x)+\alpha_nLE_{n-1}(x)(1-LE_{n-1}(x))</script><p>其中n为迭代次数，他们设为8. 多次迭代有更强的调节能力。</p><p>但是如果$\alpha$在每一个点是固定值，只能是全局调节。全局调节容易带来over-/under- enhance local区域。为了解决这个问题，他们将$\alpha$设置为pixel-wise的参数。上式变为：</p><script type="math/tex; mode=display">LE_n (x)=LE_{n-1}(x)+A_n(x)LE_{n-1}(x)(1-LE_{n-1}(x))</script><p>其中A是一个与原图等大小的parameter map。</p><p>假设，<strong>一个局部区域的像素点有相同的intensity.</strong> 所以输出图像里的相邻像素也有单调的关系。</p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/502493d9ba2415b1b60e47d715c939c.png" alt></p><p>DCE-Net（具体结构见补充材料）</p><ul><li>输入：低光照图像</li><li>输出：pixel-wise的高阶curve参数图。8次迭代共24个parameter maps</li><li>网络细节：不包括down-sampling和BN，它们会丢失相邻像素的关系。最后一层conv接Tanh</li></ul><h3 id="1-spatial-consistency-loss"><a href="#1-spatial-consistency-loss" class="headerlink" title="1. spatial consistency loss"></a>1. spatial consistency loss</h3><p>使增强后的图像保持时域的连贯性(coherence)</p><script type="math/tex; mode=display">L_{spa}=\frac{1}{K}\sum_{i=1}^K\sum_{j \in \Omega(i)}(|(Y_i-Y_j|-|I_i-I_j|)^2</script><p>$K$为局部区域数，$\Omega(i)$为4个相邻区域（上下左右）。Y和I是局部区域的平均强度值(intensity)。局部区域的大小设为4*4. 相当于对K个4*4的局部区域，要求enhance前后图的局部区域与其邻域的强度差，差不多。</p><h3 id="2-exposure-control-loss"><a href="#2-exposure-control-loss" class="headerlink" title="2. exposure control loss"></a>2. exposure control loss</h3><p>控制曝光的程度。测量局部区域的平均强度与well-exposedness的级别E之间的距离。E=0.6、在[0.4,0.7]之间差不多。</p><script type="math/tex; mode=display">L_{exp}=\frac{1}{M}\sum_{k=1}^M|Y_k-E|</script><p>M代表M个16*16的没有overlap的局部区域，Y是enhanced图像中局部的平均强度值。</p><h3 id="3-color-constancy-loss"><a href="#3-color-constancy-loss" class="headerlink" title="3. color constancy loss"></a>3. color constancy loss</h3><p>校正色彩偏差。</p><script type="math/tex; mode=display">L_{loc}=\sum_{\forall(p,q)\in \varepsilon}(J^p-J^q)^2,\varepsilon=\{(R,G),(R,B),(G,B)\}</script><p>其中，$J^p$代表enhanced图像p channel的平均强度。</p><h3 id="4-illumination-smoothness-loss"><a href="#4-illumination-smoothness-loss" class="headerlink" title="4. illumination smoothness loss"></a>4. illumination smoothness loss</h3><p>保持相邻pixel的单调性。对每个curve参数图A有：</p><script type="math/tex; mode=display">L_{tv_\mathcal{A}}=\frac{1}{N}\sum_{n=1}^N\sum_{c \in \xi}(|\nabla_x\mathcal{A}_n^c|+|\nabla_y\mathcal{A}_n^c|)^2,\xi=\{R,G,B\}</script><p>N为迭代次数。求梯度的操作有点像TV loss。</p><h3 id="total-loss"><a href="#total-loss" class="headerlink" title="total loss"></a>total loss</h3><script type="math/tex; mode=display">L_{total}=L_{spa}+L_{exp}+W_{col}L_{col}+W_{tv_A}L_{tv_A}</script><p>$W_{col}=0.5,W_{tv\mathcal{A}}=20$.</p><h3 id="ablation"><a href="#ablation" class="headerlink" title="ablation"></a>ablation</h3><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/d2a9106b97a950ec909bedb23c9f87f.png" alt></p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/a0389d8f096a01b816a148cc38e3030.png" alt></p><p>训练数据：用multi-exposure的训练数据更好。</p><h2 id="benchmark-results"><a href="#benchmark-results" class="headerlink" title="benchmark results"></a>benchmark results</h2><h3 id="no-reference-结果（US-PI）"><a href="#no-reference-结果（US-PI）" class="headerlink" title="no reference 结果（US/PI）"></a>no reference 结果（US/PI）</h3><p>视觉评判标准：</p><p>1) whether the results contain over-/under-exposed artifacts or over-/under enhanced regions; </p><p>2) whether the results introduce color deviation; </p><p>3) whether the results have unnatural texture and obvious noise.  </p><p>指标：</p><ul><li><p>user study (US) score</p></li><li><p>non-reference perceptual index (PI)</p><p>[Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.]</p><p>[Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:1–16, 2017.]</p></li></ul><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/1608020620(1" alt>.png)</p><h3 id="full-reference-结果（PSNR-SSIM"><a href="#full-reference-结果（PSNR-SSIM" class="headerlink" title="full reference 结果（PSNR/SSIM)"></a>full reference 结果（PSNR/SSIM)</h3><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210658313.png" alt></p><h3 id="face-detection"><a href="#face-detection" class="headerlink" title="face detection"></a>face detection</h3><p>用SOTA的face detector(DSFD: <a href="https://github.com/Ir1d/DARKFACE_eval_tools" target="_blank" rel="noopener">https://github.com/Ir1d/DARKFACE_eval_tools</a> )</p><p>将不同enhance方法得到的图送入DSFD，得到P-R curve。</p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210832265.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210553083.png&quot; alt&gt;&lt;br&gt;出处：CVPR2020&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="enhancement" scheme="https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"/>
    
    
      <category term="enhancement" scheme="https://shiyuuuu.github.io/tags/enhancement/"/>
    
      <category term="loss functions" scheme="https://shiyuuuu.github.io/tags/loss-functions/"/>
    
      <category term="zero-reference" scheme="https://shiyuuuu.github.io/tags/zero-reference/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 踩坑记录</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T10:00:08.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ubuntu-踩坑记录"><a href="#ubuntu-踩坑记录" class="headerlink" title="ubuntu 踩坑记录"></a>ubuntu 踩坑记录</h1><h2 id="显卡驱动重装"><a href="#显卡驱动重装" class="headerlink" title="显卡驱动重装"></a>显卡驱动重装</h2><p>某次装好后，遇到bug：</p><blockquote><p>Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown</p></blockquote><a id="more"></a><p>docker 里nvidia-smi不能用了，直接在docker外nvidia-smi也报错：</p><blockquote><p>NVIDIA-SMI couldn’t find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH.</p></blockquote><p>估计是什么时候update弄成的。</p><p>解决方法：重装显卡驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BTW this is all in console mode (for me, alt+ctrl+F2)</span></span><br><span class="line"><span class="comment"># login + password as usual</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># removing ALL nvidia software</span></span><br><span class="line">$ sudo apt-get purge nvidia* </span><br><span class="line"></span><br><span class="line"><span class="comment"># Checking what's left:</span></span><br><span class="line">$ dpkg -l | grep nvidia</span><br><span class="line"><span class="comment"># Then I deleted the ones that showed up (mostly libnvidia-* but also xserver-xorg-video-nvidia-xxx`)</span></span><br><span class="line">$ sudo apt-get purge libnvidia* xserver-xorg-video-nvidia-440 </span><br><span class="line">$ sudo apt autoremove <span class="comment"># clean it up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># now reinstall everything including nvidia-common</span></span><br><span class="line">$ sudo apt-get nvidia-common</span><br><span class="line"></span><br><span class="line"><span class="comment"># find the right driver again</span></span><br><span class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">$ sudo apt update</span><br><span class="line">$ ubuntu-drivers devices</span><br><span class="line">$ sudo apt-get install nvidia-driver-460 <span class="comment"># the recommended one by ubuntu-drivers</span></span><br><span class="line">$ update-initramfs -u <span class="comment"># needed to do this so rebooting wouldn't lose configuration I think</span></span><br><span class="line"></span><br><span class="line">$ sudo reboot</span><br></pre></td></tr></table></figure><p>然后再重装NVIDIA-docker：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$curl</span> -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -</span><br><span class="line"><span class="variable">$curl</span> -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"><span class="variable">$sudo</span> apt-get update</span><br><span class="line"></span><br><span class="line"><span class="variable">$sudo</span> apt-get install nvidia-docker2</span><br><span class="line"><span class="variable">$sudo</span> pkill -SIGHUP dockerd</span><br><span class="line"><span class="variable">$docker</span> run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run --rm nvidia/cuda:10.1-devel nvidia-smi</span><br></pre></td></tr></table></figure><p>万幸CUDA, CuDNN都还有。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.cuda()</span><br><span class="line">tensor([[<span class="number">-0.4678</span>,  <span class="number">0.1525</span>]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure><p> 配置默认运行的是nvidia-docker 而不是 docker (<a href="https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json</a> 文件中配置如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span><br><span class="line">    &quot;runtimes&quot;: &#123;</span><br><span class="line">        &quot;nvidia&quot;: &#123;</span><br><span class="line">            &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;,</span><br><span class="line">            &quot;runtimeArgs&quot;: [],</span><br><span class="line">            &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;gemfield.mirror.aliyuncs.com&quot;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="pycharm里用docker"><a href="#pycharm里用docker" class="headerlink" title="pycharm里用docker"></a>pycharm里用docker</h2><p>python 位置：/home/shiyuuuu/anaconda3/bin/python</p><p><img src="/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/image-20210326170845579.png" alt="image-20210326170845579"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ubuntu-踩坑记录&quot;&gt;&lt;a href=&quot;#ubuntu-踩坑记录&quot; class=&quot;headerlink&quot; title=&quot;ubuntu 踩坑记录&quot;&gt;&lt;/a&gt;ubuntu 踩坑记录&lt;/h1&gt;&lt;h2 id=&quot;显卡驱动重装&quot;&gt;&lt;a href=&quot;#显卡驱动重装&quot; class=&quot;headerlink&quot; title=&quot;显卡驱动重装&quot;&gt;&lt;/a&gt;显卡驱动重装&lt;/h2&gt;&lt;p&gt;某次装好后，遇到bug：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="ubuntu" scheme="https://shiyuuuu.github.io/categories/ubuntu/"/>
    
      <category term="bug" scheme="https://shiyuuuu.github.io/categories/ubuntu/bug/"/>
    
    
      <category term="ubuntu" scheme="https://shiyuuuu.github.io/tags/ubuntu/"/>
    
      <category term="bug" scheme="https://shiyuuuu.github.io/tags/bug/"/>
    
      <category term="docker" scheme="https://shiyuuuu.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>1-两数之和</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T11:53:27.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-两数之和"><a href="#1-两数之和" class="headerlink" title="1. 两数之和"></a>1. 两数之和</h1><p><a href="https://leetcode-cn.com/problems/two-sum" target="_blank" rel="noopener">https://leetcode-cn.com/problems/two-sum</a></p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。<br><a id="more"></a><br>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。<br>你可以按任意顺序返回答案。</p><p>示例 1：</p><blockquote><p>输入：nums = [2,7,11,15], target = 9<br>输出：[0,1]<br>解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。</p></blockquote><p>示例 2：</p><blockquote><p>输入：nums = [3,2,4], target = 6<br>输出：[1,2]</p></blockquote><p>示例 3</p><blockquote><p>输入：nums = [3,3], target = 6<br>输出：[0,1]</p></blockquote><p>提示：</p><blockquote><p>2 &lt;= nums.length &lt;= 103<br>-109 &lt;= nums[i] &lt;= 109<br>-109 &lt;= target &lt;= 109<br>只会存在一个有效答案</p></blockquote><h2 id="暴力解答"><a href="#暴力解答" class="headerlink" title="暴力解答"></a>暴力解答</h2><p>我自己的解答：非常暴力检索，第一个:  <code>i</code> 从0到n，第二个: <code>j</code> 从i+1 到n（或者倒序来）。这样复杂度是O(n^2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(nums)):</span><br><span class="line">                <span class="keyword">if</span> nums[i]+nums[j]==target:</span><br><span class="line">                    <span class="keyword">return</span> [i,j]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="comment"># nums_sorted=sorted(nums)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>,i,<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums[i]+nums[j]==target:</span><br><span class="line">                    c=sorted([i,j])</span><br><span class="line">                    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>哈希表博文：<a href="/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/" title="哈希表">哈希表</a></p><h3 id="思路及算法"><a href="#思路及算法" class="headerlink" title="思路及算法"></a>思路及算法</h3><p>注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。</p><p>使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 <strong><em>O(N)</em></strong> 降低到 <strong><em>O(1)</em></strong>。</p><p>这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。</p><p>先建立一个空字典，查找target-num是不是hashtable的键值，如果是，直接return，如果不是，把这个num-i对以键值对的形式添加入字典。哈希表查找元素的复杂度为<strong><em>O(1)</em></strong></p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        hashtable = dict()</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span> target - num <span class="keyword">in</span> hashtable:</span><br><span class="line">                <span class="keyword">return</span> [hashtable[target - num], i]</span><br><span class="line">            hashtable[nums[i]] = i</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">s=Solution()</span><br><span class="line"><span class="comment"># nums=[2,7,11,15]</span></span><br><span class="line"><span class="comment"># target=9</span></span><br><span class="line"><span class="comment"># nums = [3,2,4]</span></span><br><span class="line"><span class="comment"># target = 6</span></span><br><span class="line">nums = [<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">target = <span class="number">6</span></span><br><span class="line">a=s.twoSum(nums,target)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p>时间复杂度：O(N)，其中 N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。</p><p>空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-两数之和&quot;&gt;&lt;a href=&quot;#1-两数之和&quot; class=&quot;headerlink&quot; title=&quot;1. 两数之和&quot;&gt;&lt;/a&gt;1. 两数之和&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/two-sum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/problems/two-sum&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;p&gt;给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://shiyuuuu.github.io/categories/LeetCode/"/>
    
      <category term="algorithm" scheme="https://shiyuuuu.github.io/categories/algorithm/"/>
    
      <category term="easy" scheme="https://shiyuuuu.github.io/categories/LeetCode/easy/"/>
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
      <category term="leetcode" scheme="https://shiyuuuu.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>哈希表</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T11:51:55.495Z</updated>
    
    <content type="html"><![CDATA[<h1 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h1><p>Hash Table，也叫散列表。<a href="/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/" title="力扣-两数之和">力扣-两数之和</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;哈希表&quot;&gt;&lt;a href=&quot;#哈希表&quot; class=&quot;headerlink&quot; title=&quot;哈希表&quot;&gt;&lt;/a&gt;哈希表&lt;/h1&gt;&lt;p&gt;Hash Table，也叫散列表。&lt;a href=&quot;/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8
      
    
    </summary>
    
    
      <category term="algorithm" scheme="https://shiyuuuu.github.io/categories/algorithm/"/>
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
</feed>
