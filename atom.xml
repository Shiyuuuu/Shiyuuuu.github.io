<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shiyu&#39;s Blog</title>
  
  <subtitle>Learn to live.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://shiyuuuu.github.io/"/>
  <updated>2021-03-29T12:49:59.620Z</updated>
  <id>https://shiyuuuu.github.io/</id>
  
  <author>
    <name>Shiyu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>阅读论文-GLEAN Generative Latent Bank for Large-Factor Image Super-Resolution</title>
    <link href="https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/"/>
    <id>https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-03-29T12:49:59.620Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329170458177.png" alt></p><a id="more"></a><p><a href="https://ckkelvinchan.github.io/projects/GLEAN/" target="_blank" rel="noopener">project</a>  code <a href="https://arxiv.org/pdf/2012.00739.pdf" target="_blank" rel="noopener">paper</a></p><p>任务是： 大尺度超分辨率（8$\times$ 到 64$\times$），most details and textures are lost during downsampling.</p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><ul><li>已有的SR方法：<ul><li>solely rely on $L_2$ loss: 视觉质量不好 (over-smoothing artifacts)。</li><li>adverssrial loss [ESRGAN]: Generator既要捕获图像characteristics又要保真（maintaining the fidelity to the GT）, 限制了其近似自然图像的能力，产生artifacts</li><li>GAN inversion methods [PULSE]：反转pre-trained GAN的生成过程：把image mapping回latent space；再由latent space中optimal vector重建图像。<strong>只靠低维隐向量不足以指导重建的过程</strong>，使得产生的结果low fidelity. 需要image-specific, iterative的优化.</li></ul></li><li>利用pre-trained GAN作为latent bank， 充分利用pre-trained GAN中封装的丰富且多样的先验。换用不同的bank可以不同类的图像：cat，building，human face，car. 利用字典学习的方式，测试阶段，只需要一次前传即可得到恢复后的图像。</li><li>GLEAN 的整体结构：encoder-bank-decoder</li></ul><h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><h3 id="large-factor-SR"><a href="#large-factor-SR" class="headerlink" title="large-factor SR"></a>large-factor SR</h3><ol><li>fully probabilistic pixel recursive network for upsampling extremely coarse images with resolution 8×8. (Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.)</li><li>RFB-ESRGAN：adopts multi-scale receptive fields blocks for 16× SR. (Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, and Yandong Guo. Perceptual extreme super resolution network with receptive field block. In CVPRW, 2020.)</li><li>VarSR: 8× SR by matching the latent distributions of LR and HR images to recover the missing details. (Sangeek Hyun and Jae-Pil Heo. VarSR: Variational super-resolution network for very low resolution images. In ECCV, 2020.)</li><li>perform 16× reference-based SR on paintings with a non-local matching module and a wavelet texture loss. (Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, and Yun Fu. Texture hallucination for large-scale painting super-resolution. In ECCV, 2020.)</li></ol><h3 id="GAN-inversion"><a href="#GAN-inversion" class="headerlink" title="GAN inversion"></a>GAN inversion</h3><ol><li><p>David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al. Semantic photo manipulation with a generative image prior. TOG, 2020.</p></li><li><p>Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code GAN prior. In CVPR, 2020.</p></li><li><p>Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. <strong>PULSE</strong>: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020.</p><p> 通过pixel-wise约束，迭代优化styleGAN的隐变量。</p></li><li><p>Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In ECCV, 2020.</p><p> finetune generator和latent code来缩小训练集和测试集分布的gap.</p></li></ol><p>降质图像$x$，latent space：$\mathcal{Z}$：</p><script type="math/tex; mode=display">z^*=argmin_{z \in \mathcal{Z}}\mathcal{L}(G(z), x)</script><p><strong><em>缺点：</em></strong></p><p>低维的隐向量不能保持图像的spatial information.</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>passing both the latent vectors and multi-resolution convolutional features from the encoder.</p><p>multi-resolution cues need to be passed from the bank to the decoder.</p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329194816287.png" alt></p><ul><li>整个结构为encoder-bank-decoder</li><li>encoder：$E_0$为RRDB-Net，$E_i, i \in\{1,…,N\}$代表堆叠一个stride=2的conv和一个stride=1的conv. 最后由FC层得到$C$, $C$表示隐向量，提供high-level信息。为了更好的指导结构重建，将多分辨率的特征和隐向量都送入bank.</li><li>Generative latent bank: 用pre-trained的Generator, styleGAN，提供纹理和细节生成的先验。<ul><li>对generator的每个block输入不同的隐向量$C_i$, $i \in \{0,…,k-1\}$</li><li>$\{g_i\}$代表每个block输出的feature, 它是由$C_i, g_{i-1}, f_{N-i}$由augmented style block得到。</li><li>不直接输出结果，而是将特征$\{g_i\}$输入到decoder</li><li>优势：像reference-based SR，HR reference image作为显式图像字典。性能很受 输入和reference相似度的影响。GLEAN用GAN-based的字典，不依赖于任何具体的图像，它获取的是图像的分布。而且没有global matching和reference images selection， 计算简便。</li></ul></li><li>decoder：progressive地聚合来自encoder和latent bank的特征。每个conv后跟着pixel-shuffle层。由于有encoder和decoder之间的skip-connection，encoder捕获的信息可以被强化，bank专注于纹理和细节的生成。</li><li>训练：$\mathcal{l_2}$ loss, perceptual loss, adversarial loss. 训练时fix住latent bank，实验发现，finetune latent bank没有性能提升，而且可能使latent bank偏向训练集的分布。</li></ul><h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329171413782.png" alt></p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203432053.png" alt></p><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203534511.png" alt></p><h3 id="image-retouching"><a href="#image-retouching" class="headerlink" title="image retouching"></a>image retouching</h3><p><img src="/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329203631010.png" alt></p><h2 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h2><p>[ESRGAN]: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. ESRGAN: Enhanced super-resolution generative adversarial networks. In ECCVW, 2018. </p><p>[PULSE]: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020.</p><h3 id="reference-based-SR"><a href="#reference-based-SR" class="headerlink" title="reference-based SR"></a>reference-based SR</h3><ol><li>Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restoration via deep multi-scale component dictionaries. In ECCV, 2020.</li><li>Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020.</li><li>Xu Yan, Weibing Zhao, Kun Yuan, Ruimao Zhang, Zhen Li, and Shuguang Cui. Towards content-independent multi-reference super-resolution: Adaptive pattern matching and feature aggregation. In ECCV, 2020.</li><li>Yang Zhang, Ivor W Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, and Xin Yu. Copy and Paste GAN: Face hallucination from shaded thumbnails. In CVPR, 2020.</li><li>Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. In CVPR, 2019.</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/image-20210329170458177.png&quot; alt&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="super-resolution" scheme="https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"/>
    
    
      <category term="Super-resolution" scheme="https://shiyuuuu.github.io/tags/Super-resolution/"/>
    
      <category term="GAN" scheme="https://shiyuuuu.github.io/tags/GAN/"/>
    
      <category term="highly ill-posed" scheme="https://shiyuuuu.github.io/tags/highly-ill-posed/"/>
    
  </entry>
  
  <entry>
    <title>阅读论文-Zero-Reference deep curve estimation for low-light image enhancement</title>
    <link href="https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/"/>
    <id>https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-03-29T13:08:40.347Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210553083.png" alt><br>出处：CVPR2020<br><a id="more"></a></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper PDF</a></p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf" target="_blank" rel="noopener">supplemental materials</a></p><p>project: <a href="https://li-chongyi.github.io/Proj_Zero-DCE.html" target="_blank" rel="noopener">https://li-chongyi.github.io/Proj_Zero-DCE.html</a></p><p>code: <a href="https://github.com/Li-Chongyi/Zero-DCE" target="_blank" rel="noopener">https://github.com/Li-Chongyi/Zero-DCE</a></p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>图像编辑软件通过调节曲线来增强图像=&gt;image-specific 曲线估计：</p><p>根据给定图像估计pixel-wise的调整曲线。不需要任何成对或不成对的训练数据（不需要reference/GT) </p><p>图像增强=&gt;非线性曲线映射  而不是通过image-to-image mapping</p><p><strong>方法：</strong> non-reference loss functions</p><h2 id="contribution"><a href="#contribution" class="headerlink" title="contribution"></a>contribution</h2><ol><li>no reference: 避免了需要paired/unpaired数据的方法中的overfitting的问题</li><li>设计image-specific曲线：高次、pixel-wise</li><li>提升人脸识别的性能</li></ol><h2 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h2><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><p>调整图像的直方图分布调整，增大图像的动态范围。</p><p>global level: </p><p>[1]. Dinu Coltuc, Philippe Bolon, and Jean-Marc Chassery. Exact histogram specification. IEEE Transactions on Image Processing, 15(5):1143–1152, 2006.   </p><p>[2]. Haidi Ibrahim and Nicholas Sia Pik Kong. Brightness preserving dynamic histogram equalization for image contrast enhancement. IEEE Transactions on Consumer Electronics, 53(4):1752–1758, 2007.</p><p>local level:</p><p>[3].  Chulwoo Lee, Chul Lee, and Chang-Su Kim. Contrast enhancement based on layered difference representation of 2d histograms. IEEE Transactions on Image Processing, 22(12):5372–5384, 2013.</p><p>[4]. J Alex Stark. Adaptive image contrast enhancement using generalizations of histogram equalization. IEEE Transactions on Image Processing, 9(5):889–896, 2000.</p><p>Retinex theory (将图像分解为reflectance和illumination，其中reflectance分量在任何光照条件下保持一致，图像质量增强任务变为illumination estimation问题):</p><p>[5].  Edwin H Land. The retinex theory of color vision. Scientific American, 237(6):108–128, 1977.</p><p>自然的信息保存方法：</p><p>[6].  Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Processing, 22(9):3538–3548, 2013.</p><p><strong>weighted vatiation:</strong></p><p>[7]. <em>Xueyang Fu,</em> Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016.</p><p>coarse illumination map: 搜索RGB中最大intensity的pixel</p><p>[8]. Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, 2017.</p><p>考虑噪声：</p><p>[9]. Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, 2018</p><p>自动exposure校正方法：</p><p>通过全局优化方法估计图像的S形曲线。</p><p>[10]. Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV, 2012. </p><h3 id="Data-driven方法"><a href="#Data-driven方法" class="headerlink" title="Data-driven方法"></a>Data-driven方法</h3><h4 id="CNN-based"><a href="#CNN-based" class="headerlink" title="CNN-based"></a>CNN-based</h4><p>一般需要pair对的数据：资源密集型(resource-intensive)。一般这种成对的数据通过  自动光照退化、改变相机的设置  采集  </p><p>或者用image retouching合成</p><ol><li><p><strong>LOL数据集</strong>[Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018.] 通过改变曝光时间和ISO获取成对的low/normal光照的图像。</p></li><li><p><strong>MIT-adobe FiveK数据集</strong>[Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo ´ Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR, 2011.]包括5000raw图，每一张raw图由专家生成5个retouched图像。</p></li></ol><p>[11]提出了一种估计illumination的方法：poor generalization capability</p><p>[11].  Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019.</p><h4 id="GAN-based"><a href="#GAN-based" class="headerlink" title="GAN-based"></a>GAN-based</h4><p>EnlightenGAN[Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. In CVPR, 2019.]</p><p>用unpair的low/normal光照的数据。需要仔细挑选unpaired的训练数据。</p><h2 id="methods"><a href="#methods" class="headerlink" title="methods"></a>methods</h2><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/b0821ec0907d0582c30b9aecd76e5b2.png" alt></p><p>通过non-reference loss function实现不需要paired/unpaired的数据。通过迭代训练得到结果。</p><h3 id="Light-enhancement-curve"><a href="#Light-enhancement-curve" class="headerlink" title="Light-enhancement curve"></a>Light-enhancement curve</h3><p>自适应的曲线参数只由输入图像决定。曲线要是单调的来保持周围像素的区别。曲线要处处可微保证可以梯度反传。</p><script type="math/tex; mode=display">LE(I(x);\alpha)=I(x)+\alpha I(x)(1-I(x))</script><p>其中$\alpha \in [-1,1]$, 将LE曲线分别作用于R\G\B通道，而不仅仅作用于illumination通道。调节3个通道可以更好的保持固有的色彩，减少过饱和。</p><p>上式可以多次迭代，写为：</p><script type="math/tex; mode=display">LE_n (x)=LE_{n-1}(x)+\alpha_nLE_{n-1}(x)(1-LE_{n-1}(x))</script><p>其中n为迭代次数，他们设为8. 多次迭代有更强的调节能力。</p><p>但是如果$\alpha$在每一个点是固定值，只能是全局调节。全局调节容易带来over-/under- enhance local区域。为了解决这个问题，他们将$\alpha$设置为pixel-wise的参数。上式变为：</p><script type="math/tex; mode=display">LE_n (x)=LE_{n-1}(x)+A_n(x)LE_{n-1}(x)(1-LE_{n-1}(x))</script><p>其中A是一个与原图等大小的parameter map。</p><p>假设，<strong>一个局部区域的像素点有相同的intensity.</strong> 所以输出图像里的相邻像素也有单调的关系。</p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/502493d9ba2415b1b60e47d715c939c.png" alt></p><p>DCE-Net（具体结构见补充材料）</p><ul><li>输入：低光照图像</li><li>输出：pixel-wise的高阶curve参数图。8次迭代共24个parameter maps</li><li>网络细节：不包括down-sampling和BN，它们会丢失相邻像素的关系。最后一层conv接Tanh</li></ul><h3 id="1-spatial-consistency-loss"><a href="#1-spatial-consistency-loss" class="headerlink" title="1. spatial consistency loss"></a>1. spatial consistency loss</h3><p>使增强后的图像保持时域的连贯性(coherence)</p><script type="math/tex; mode=display">L_{spa}=\frac{1}{K}\sum_{i=1}^K\sum_{j \in \Omega(i)}(|(Y_i-Y_j|-|I_i-I_j|)^2</script><p>$K$为局部区域数，$\Omega(i)$为4个相邻区域（上下左右）。Y和I是局部区域的平均强度值(intensity)。局部区域的大小设为4*4. 相当于对K个4*4的局部区域，要求enhance前后图的局部区域与其邻域的强度差，差不多。</p><h3 id="2-exposure-control-loss"><a href="#2-exposure-control-loss" class="headerlink" title="2. exposure control loss"></a>2. exposure control loss</h3><p>控制曝光的程度。测量局部区域的平均强度与well-exposedness的级别E之间的距离。E=0.6、在[0.4,0.7]之间差不多。</p><script type="math/tex; mode=display">L_{exp}=\frac{1}{M}\sum_{k=1}^M|Y_k-E|</script><p>M代表M个16*16的没有overlap的局部区域，Y是enhanced图像中局部的平均强度值。</p><h3 id="3-color-constancy-loss"><a href="#3-color-constancy-loss" class="headerlink" title="3. color constancy loss"></a>3. color constancy loss</h3><p>校正色彩偏差。</p><script type="math/tex; mode=display">L_{loc}=\sum_{\forall(p,q)\in \varepsilon}(J^p-J^q)^2,\varepsilon=\{(R,G),(R,B),(G,B)\}</script><p>其中，$J^p$代表enhanced图像p channel的平均强度。</p><h3 id="4-illumination-smoothness-loss"><a href="#4-illumination-smoothness-loss" class="headerlink" title="4. illumination smoothness loss"></a>4. illumination smoothness loss</h3><p>保持相邻pixel的单调性。对每个curve参数图A有：</p><script type="math/tex; mode=display">L_{tv_\mathcal{A}}=\frac{1}{N}\sum_{n=1}^N\sum_{c \in \xi}(|\nabla_x\mathcal{A}_n^c|+|\nabla_y\mathcal{A}_n^c|)^2,\xi=\{R,G,B\}</script><p>N为迭代次数。求梯度的操作有点像TV loss。</p><h3 id="total-loss"><a href="#total-loss" class="headerlink" title="total loss"></a>total loss</h3><script type="math/tex; mode=display">L_{total}=L_{spa}+L_{exp}+W_{col}L_{col}+W_{tv_A}L_{tv_A}</script><p>$W_{col}=0.5,W_{tv\mathcal{A}}=20$.</p><h3 id="ablation"><a href="#ablation" class="headerlink" title="ablation"></a>ablation</h3><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/d2a9106b97a950ec909bedb23c9f87f.png" alt></p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/a0389d8f096a01b816a148cc38e3030.png" alt></p><p>训练数据：用multi-exposure的训练数据更好。</p><h2 id="benchmark-results"><a href="#benchmark-results" class="headerlink" title="benchmark results"></a>benchmark results</h2><h3 id="no-reference-结果（US-PI）"><a href="#no-reference-结果（US-PI）" class="headerlink" title="no reference 结果（US/PI）"></a>no reference 结果（US/PI）</h3><p>视觉评判标准：</p><p>1) whether the results contain over-/under-exposed artifacts or over-/under enhanced regions; </p><p>2) whether the results introduce color deviation; </p><p>3) whether the results have unnatural texture and obvious noise.  </p><p>指标：</p><ul><li><p>user study (US) score</p></li><li><p>non-reference perceptual index (PI)</p><p>[Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.]</p><p>[Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:1–16, 2017.]</p></li></ul><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/1608020620(1" alt>.png)</p><h3 id="full-reference-结果（PSNR-SSIM"><a href="#full-reference-结果（PSNR-SSIM" class="headerlink" title="full reference 结果（PSNR/SSIM)"></a>full reference 结果（PSNR/SSIM)</h3><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210658313.png" alt></p><h3 id="face-detection"><a href="#face-detection" class="headerlink" title="face detection"></a>face detection</h3><p>用SOTA的face detector(DSFD: <a href="https://github.com/Ir1d/DARKFACE_eval_tools" target="_blank" rel="noopener">https://github.com/Ir1d/DARKFACE_eval_tools</a> )</p><p>将不同enhance方法得到的图送入DSFD，得到P-R curve。</p><p><img src="/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210832265.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/image-20210329210553083.png&quot; alt&gt;&lt;br&gt;出处：CVPR2020&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Paper reading" scheme="https://shiyuuuu.github.io/categories/Paper-reading/"/>
    
      <category term="enhancement" scheme="https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"/>
    
    
      <category term="enhancement" scheme="https://shiyuuuu.github.io/tags/enhancement/"/>
    
      <category term="loss functions" scheme="https://shiyuuuu.github.io/tags/loss-functions/"/>
    
      <category term="zero-reference" scheme="https://shiyuuuu.github.io/tags/zero-reference/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 踩坑记录</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T10:00:08.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ubuntu-踩坑记录"><a href="#ubuntu-踩坑记录" class="headerlink" title="ubuntu 踩坑记录"></a>ubuntu 踩坑记录</h1><h2 id="显卡驱动重装"><a href="#显卡驱动重装" class="headerlink" title="显卡驱动重装"></a>显卡驱动重装</h2><p>某次装好后，遇到bug：</p><blockquote><p>Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown</p></blockquote><a id="more"></a><p>docker 里nvidia-smi不能用了，直接在docker外nvidia-smi也报错：</p><blockquote><p>NVIDIA-SMI couldn’t find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH.</p></blockquote><p>估计是什么时候update弄成的。</p><p>解决方法：重装显卡驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BTW this is all in console mode (for me, alt+ctrl+F2)</span></span><br><span class="line"><span class="comment"># login + password as usual</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># removing ALL nvidia software</span></span><br><span class="line">$ sudo apt-get purge nvidia* </span><br><span class="line"></span><br><span class="line"><span class="comment"># Checking what's left:</span></span><br><span class="line">$ dpkg -l | grep nvidia</span><br><span class="line"><span class="comment"># Then I deleted the ones that showed up (mostly libnvidia-* but also xserver-xorg-video-nvidia-xxx`)</span></span><br><span class="line">$ sudo apt-get purge libnvidia* xserver-xorg-video-nvidia-440 </span><br><span class="line">$ sudo apt autoremove <span class="comment"># clean it up</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># now reinstall everything including nvidia-common</span></span><br><span class="line">$ sudo apt-get nvidia-common</span><br><span class="line"></span><br><span class="line"><span class="comment"># find the right driver again</span></span><br><span class="line">$ sudo add-apt-repository ppa:graphics-drivers/ppa</span><br><span class="line">$ sudo apt update</span><br><span class="line">$ ubuntu-drivers devices</span><br><span class="line">$ sudo apt-get install nvidia-driver-460 <span class="comment"># the recommended one by ubuntu-drivers</span></span><br><span class="line">$ update-initramfs -u <span class="comment"># needed to do this so rebooting wouldn't lose configuration I think</span></span><br><span class="line"></span><br><span class="line">$ sudo reboot</span><br></pre></td></tr></table></figure><p>然后再重装NVIDIA-docker：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$curl</span> -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -</span><br><span class="line"><span class="variable">$curl</span> -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line"><span class="variable">$sudo</span> apt-get update</span><br><span class="line"></span><br><span class="line"><span class="variable">$sudo</span> apt-get install nvidia-docker2</span><br><span class="line"><span class="variable">$sudo</span> pkill -SIGHUP dockerd</span><br><span class="line"><span class="variable">$docker</span> run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>测试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-docker run --rm nvidia/cuda:10.1-devel nvidia-smi</span><br></pre></td></tr></table></figure><p>万幸CUDA, CuDNN都还有。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cuda.is_available()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.randn(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.cuda()</span><br><span class="line">tensor([[<span class="number">-0.4678</span>,  <span class="number">0.1525</span>]], device=<span class="string">'cuda:0'</span>)</span><br></pre></td></tr></table></figure><p> 配置默认运行的是nvidia-docker 而不是 docker (<a href="https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json</a> 文件中配置如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;default-runtime&quot;: &quot;nvidia&quot;,</span><br><span class="line">    &quot;runtimes&quot;: &#123;</span><br><span class="line">        &quot;nvidia&quot;: &#123;</span><br><span class="line">            &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;,</span><br><span class="line">            &quot;runtimeArgs&quot;: [],</span><br><span class="line">            &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;gemfield.mirror.aliyuncs.com&quot;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="pycharm里用docker"><a href="#pycharm里用docker" class="headerlink" title="pycharm里用docker"></a>pycharm里用docker</h2><p>python 位置：/home/shiyuuuu/anaconda3/bin/python</p><p><img src="/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/image-20210326170845579.png" alt="image-20210326170845579"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ubuntu-踩坑记录&quot;&gt;&lt;a href=&quot;#ubuntu-踩坑记录&quot; class=&quot;headerlink&quot; title=&quot;ubuntu 踩坑记录&quot;&gt;&lt;/a&gt;ubuntu 踩坑记录&lt;/h1&gt;&lt;h2 id=&quot;显卡驱动重装&quot;&gt;&lt;a href=&quot;#显卡驱动重装&quot; class=&quot;headerlink&quot; title=&quot;显卡驱动重装&quot;&gt;&lt;/a&gt;显卡驱动重装&lt;/h2&gt;&lt;p&gt;某次装好后，遇到bug：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="ubuntu" scheme="https://shiyuuuu.github.io/categories/ubuntu/"/>
    
      <category term="bug" scheme="https://shiyuuuu.github.io/categories/ubuntu/bug/"/>
    
    
      <category term="ubuntu" scheme="https://shiyuuuu.github.io/tags/ubuntu/"/>
    
      <category term="bug" scheme="https://shiyuuuu.github.io/tags/bug/"/>
    
      <category term="docker" scheme="https://shiyuuuu.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>1-两数之和</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T11:53:27.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-两数之和"><a href="#1-两数之和" class="headerlink" title="1. 两数之和"></a>1. 两数之和</h1><p><a href="https://leetcode-cn.com/problems/two-sum" target="_blank" rel="noopener">https://leetcode-cn.com/problems/two-sum</a></p><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。<br><a id="more"></a><br>你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。<br>你可以按任意顺序返回答案。</p><p>示例 1：</p><blockquote><p>输入：nums = [2,7,11,15], target = 9<br>输出：[0,1]<br>解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。</p></blockquote><p>示例 2：</p><blockquote><p>输入：nums = [3,2,4], target = 6<br>输出：[1,2]</p></blockquote><p>示例 3</p><blockquote><p>输入：nums = [3,3], target = 6<br>输出：[0,1]</p></blockquote><p>提示：</p><blockquote><p>2 &lt;= nums.length &lt;= 103<br>-109 &lt;= nums[i] &lt;= 109<br>-109 &lt;= target &lt;= 109<br>只会存在一个有效答案</p></blockquote><h2 id="暴力解答"><a href="#暴力解答" class="headerlink" title="暴力解答"></a>暴力解答</h2><p>我自己的解答：非常暴力检索，第一个:  <code>i</code> 从0到n，第二个: <code>j</code> 从i+1 到n（或者倒序来）。这样复杂度是O(n^2)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,len(nums)):</span><br><span class="line">                <span class="keyword">if</span> nums[i]+nums[j]==target:</span><br><span class="line">                    <span class="keyword">return</span> [i,j]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        <span class="comment"># nums_sorted=sorted(nums)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>,i,<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> nums[i]+nums[j]==target:</span><br><span class="line">                    c=sorted([i,j])</span><br><span class="line">                    <span class="keyword">return</span> c</span><br></pre></td></tr></table></figure><h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>哈希表博文：<a href="/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/" title="哈希表">哈希表</a></p><h3 id="思路及算法"><a href="#思路及算法" class="headerlink" title="思路及算法"></a>思路及算法</h3><p>注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。</p><p>使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 <strong><em>O(N)</em></strong> 降低到 <strong><em>O(1)</em></strong>。</p><p>这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。</p><p>先建立一个空字典，查找target-num是不是hashtable的键值，如果是，直接return，如果不是，把这个num-i对以键值对的形式添加入字典。哈希表查找元素的复杂度为<strong><em>O(1)</em></strong></p><p>代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> List</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        hashtable = dict()</span><br><span class="line">        <span class="keyword">for</span> i, num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span> target - num <span class="keyword">in</span> hashtable:</span><br><span class="line">                <span class="keyword">return</span> [hashtable[target - num], i]</span><br><span class="line">            hashtable[nums[i]] = i</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line"></span><br><span class="line">s=Solution()</span><br><span class="line"><span class="comment"># nums=[2,7,11,15]</span></span><br><span class="line"><span class="comment"># target=9</span></span><br><span class="line"><span class="comment"># nums = [3,2,4]</span></span><br><span class="line"><span class="comment"># target = 6</span></span><br><span class="line">nums = [<span class="number">3</span>,<span class="number">3</span>]</span><br><span class="line">target = <span class="number">6</span></span><br><span class="line">a=s.twoSum(nums,target)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><p>时间复杂度：O(N)，其中 N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。</p><p>空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1-两数之和&quot;&gt;&lt;a href=&quot;#1-两数之和&quot; class=&quot;headerlink&quot; title=&quot;1. 两数之和&quot;&gt;&lt;/a&gt;1. 两数之和&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://leetcode-cn.com/problems/two-sum&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://leetcode-cn.com/problems/two-sum&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;headerlink&quot; title=&quot;题目&quot;&gt;&lt;/a&gt;题目&lt;/h2&gt;&lt;p&gt;给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="LeetCode" scheme="https://shiyuuuu.github.io/categories/LeetCode/"/>
    
      <category term="algorithm" scheme="https://shiyuuuu.github.io/categories/algorithm/"/>
    
      <category term="easy" scheme="https://shiyuuuu.github.io/categories/LeetCode/easy/"/>
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
      <category term="leetcode" scheme="https://shiyuuuu.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>哈希表</title>
    <link href="https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <id>https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/</id>
    <published>2021-03-25T16:00:00.000Z</published>
    <updated>2021-03-26T11:51:55.495Z</updated>
    
    <content type="html"><![CDATA[<h1 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h1><p>Hash Table，也叫散列表。<a href="/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/" title="力扣-两数之和">力扣-两数之和</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;哈希表&quot;&gt;&lt;a href=&quot;#哈希表&quot; class=&quot;headerlink&quot; title=&quot;哈希表&quot;&gt;&lt;/a&gt;哈希表&lt;/h1&gt;&lt;p&gt;Hash Table，也叫散列表。&lt;a href=&quot;/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8
      
    
    </summary>
    
    
      <category term="algorithm" scheme="https://shiyuuuu.github.io/categories/algorithm/"/>
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
    
      <category term="哈希表" scheme="https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
</feed>
