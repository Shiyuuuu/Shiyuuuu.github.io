{"meta":{"title":"Shiyu's Blog","subtitle":"Learn to live.","description":null,"author":"Shiyu","url":"https://shiyuuuu.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2021-03-26T09:37:09.711Z","updated":"2020-06-16T14:38:42.000Z","comments":true,"path":"404.html","permalink":"https://shiyuuuu.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2021-03-26T09:37:09.712Z","updated":"2020-06-16T14:25:06.000Z","comments":true,"path":"about/index.html","permalink":"https://shiyuuuu.github.io/about/index.html","excerpt":"","text":"一枚CS小学生。"},{"title":"所有分类","date":"2021-03-26T09:37:09.714Z","updated":"2020-06-16T14:35:18.000Z","comments":true,"path":"categories/index.html","permalink":"https://shiyuuuu.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-03-26T09:37:09.716Z","updated":"2020-06-16T14:40:28.000Z","comments":true,"path":"tags/index.html","permalink":"https://shiyuuuu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"阅读论文 DualSR- Zero-Shot Dual Learning for Real-World Super-Resolution","slug":"DualSR: Zero-Shot Dual Learning for Real-World Super-Resolution","date":"2021-04-13T16:00:00.000Z","updated":"2021-04-13T03:39:39.482Z","comments":true,"path":"2021/04/14/DualSR: Zero-Shot Dual Learning for Real-World Super-Resolution/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/14/DualSR:%20Zero-Shot%20Dual%20Learning%20for%20Real-World%20Super-Resolution/","excerpt":"WACV 2021","text":"WACV 2021 paper code motivation real world SR的降质过程复杂，而且每张图的降质可能都不同：每个相机的PSF都可能不同，同一相机不同的光照条件、场景深度、抖动。 contribution 提出DualSR，只在测试图像的patches上训练（用基于kernelGAN的kernel estimation 估计输入图像patch的分布），是image-specific的 ZSSR+CycleGAN+DualGAN masked interpolation loss related worksmedical image Kensuke Umehara, Junko Ota, and Takayuki Ishida. Application of super-resolution convolutional neural network for enhancing image resolution in chest ct. Journal of digital imaging, 31(4):441–450, 2018. Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan, Mengzhou Li, Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, et al. Ct super-resolution gan constrained by the identical, residual, and cycle learning ensemble (gan-circle). IEEE Transactions on Medical Imaging, 39(1):188–203, 2019. 有paired images blind SR I_{LR}=(I_{HR}*k)\\downarrow_s+n估image-specific downsampler的： [2]. Sefi Bell-Kligler, Assaf Shocher, and Michal Irani. Blind super-resolution kernel estimation using an internal-gan. In Advances in Neural Information Processing Systems, pages 284–293, 2019.(kernelGAN， 只用LR测试图像作为训练集来估计image-specific的blur kernel) [3]. Adrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a gan to learn how to do image degradation first. In Proceedings of the European conference on computer vision (ECCV), pages 185–200, 2018. [11]. Jinjin Gu, Hannan Lu, Wangmeng Zuo, and Chao Dong. Blind super-resolution with iterative kernel correction. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1604–1613, 2019.（IKC, 用了STF(spatial feature transform层来解决多个blurkernel） [26]. Assaf Shocher, Nadav Cohen, and Michal Irani. Zero-shot super-resolution using deep internal learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3118–3126, 2018. （ZSSR） [6]. Victor Cornillere, Abdelaziz Djelouah, Wang Yifan, Olga Sorkine-Hornung, and Christopher Schroers. Blind image super resolution with spatially variant degradations. ACM Transactions on Graphics (proceedings of ACM SIGGRAPH ASIA), 38(6), 2019（通过分析生成的HR中的artifacts来估计退化过程。训练一个kernel discrimination来预测错误核估计的误差，再通过最小化判别器的error，recover正确的核） [16]. Shady Abu Hussein, Tom Tirer, and Raja Giryes. Correction filter for single image super-resolution: Robustifying off-the shelf deep super-resolvers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428–1437, 2020. （其校正滤波器的闭合形式推导(closed-form derivation of their correction filter)，以对降级的LR图像进行变换，使其与双三次降采样结果相匹配。在non-isotropic的退化上结果没有report） non-blindSRMD[36] 用dimensionality stretching使网络输入degradation的参数（blur kernel，noise level） USRNet [35]分开解决数据子问题和先验子问题，用MAP methodarchitecture $G_{DN}$通过下采样输入的LR图像，使之与他自己（LR图像）在patch level上尽可能相似。 patch的internal分布可以由internalGAN在输入图像的patch上训练得到。 \\mathcal{L}_{total}=\\mathcal{L}_{GAN}+\\lambda_{cycle}\\mathcal{L}_{cycle}+\\lambda_{interp}\\mathcal{L}_{interp}其中$\\mathcal{L}_{GAN}$加入了正则项： \\mathcal{L}_{GAN}=\\mathbb{E}_y[D_{DN}(G_{DN}(y))-1]^2+\\mathcal{R}其中$\\mathcal{R}$是kernelGAN中估计的kernel的explicit先验。 masked interpolation loss保持色彩组成，消除输出图像的blurry。 upsampler的训练没有直接的监督，输出图在sharp edge的地方有ringing effects. 特别是在低频区域。引入一个loss 最小化bicubic上采样和输出的结果之差。（bicubic interpolation能正确上采样低频区域，但不能恢复高频细节。）所以如果对整张图加这个loss，效果也是不好的，如上图b。所以需要masked loss。 由sobel算子得到频率mask。 f_{mask}=1-Sobel(Bicubic(x))这个mask在低频部分值大，高频部分值小。 masked interpolation loss为： \\mathcal{L}_{interp}=\\mathbb{E}_x||[G_{UP}(x)-Bicubic(x)]*f_{mask}||_1experimentsloss权重的影响 synthesized images DIV2KRK：生成LR：11*11的anisotropic的高斯核，每张图的kernel都有不同的形状和sigma。再下采样，再非均匀的乘性噪声。 urban100：生成LR的方法同上。 NTIRE2017：SISR track2的数据集，用DIV2K的验证集作为HR。降质未知。 比较的方法： 第一类：在bicubic下采样上训练的模型。如EDSR+, SAN+, ZSSR(用bicubic下采样) 第二类：盲方法- ZSSR/USRNet (kernel由kernelGAN估计出) 第三类：有GT 的kernel。（除了在NTIRE2017数据集上没有GT的kernel） 在NTIRE数据集上的结果（2nd比1st类方法好）是符合常理的。kernelGAN+ZSSR在NTIRE数据集上效果不好是因为降质过程较复杂，不能由kernelGAN精确估计出来。而加上masked interpolation loss 后，效果有所提升。 blindSR是假设blur kernel是传统滤波器和anisotropic 高斯核的卷积。 但是kernelGAN+USRNet比不过bicubic类的方法，就不是很能理解。 real images用的数据集：RealSR数据集（NTIRE2019中所用），由DSLR相机获得。用28mm焦距得到的图像作为LR，50mm焦距得到的图像作为HR。长焦得到的图像有finer的细节。由于很难完全对齐，所以还是没有成pair的图像。（畅师兄的camera lens SR） kernelGAN或者blindSR都不能准确估计出退化模型。 ablation study 不足没有跟cycleGAN，cycle-in-cycle这类方法比较。 future worklarger scale factors harder use-cases with more extreme degradation settings applications in medical imaging or electron microscopy.","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"zero-shot","slug":"zero-shot","permalink":"https://shiyuuuu.github.io/tags/zero-shot/"},{"name":"self-supervised","slug":"self-supervised","permalink":"https://shiyuuuu.github.io/tags/self-supervised/"}]},{"title":"CVPR 2021 待读论文列表","slug":"CVPR21_toRead","date":"2021-04-12T16:00:00.000Z","updated":"2021-04-12T14:59:12.007Z","comments":true,"path":"2021/04/13/CVPR21_toRead/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/13/CVPR21_toRead/","excerpt":"Contrastive Learning for Compact Single Image Dehazing","text":"Contrastive Learning for Compact Single Image Dehazing","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"CVPR21","slug":"Paper-reading/CVPR21","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/CVPR21/"}],"tags":[{"name":"To Do","slug":"To-Do","permalink":"https://shiyuuuu.github.io/tags/To-Do/"}]},{"title":"阅读论文-Unsupervised Domain-Specific Deblurring via Disentangled Representations","slug":"阅读论文_Unsupervised Domain-Specific Deblurring via Disentangled Representations","date":"2021-04-11T16:00:00.000Z","updated":"2021-04-12T07:29:46.474Z","comments":true,"path":"2021/04/12/阅读论文_Unsupervised Domain-Specific Deblurring via Disentangled Representations/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/12/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Unsupervised%20Domain-Specific%20Deblurring%20via%20Disentangled%20Representations/","excerpt":"出处：CVPR2019","text":"出处：CVPR2019 paper code motivation 之前的方法大多需要成对的训练数据，在实际中采集成对数据较困难。 现有的无监督方法比如cycleGAN这种，往往编码了色彩、纹理等模糊之外的信息。 contribution 提出了一直特定域的无监督特征解耦的去模糊方法。通过将模糊图像中的内容和模糊特征解开，以将模糊信息准确地编码到去模糊框架中。 他们在人脸图像和文本图像去模糊上达到了与最好的监督学习方法comparable的效果。 method 目的是实现图像的去模糊，为什么要添加模糊编码器？作者的思路：既然清晰的图像是不含模糊信息的，可以认为，清晰图像的内容编码器提取到了清晰图像的内容信息，如果清晰的图像通过结合模糊编码器模糊特征去生成出模糊图像，是不是可以说，模糊编码器是在对清晰图像做模糊化处理，这个的前提就是 模糊编码器确实提取到了图像的模糊特征。所以说由清晰图像生成模糊图像也保证了模糊编码器是对图像的模糊信息进行编码的作用。清晰图像到模糊图像是为了优化模糊编码和模糊图像的内容编码的作用。 如何去保证这个模糊编码器是真的提取到模糊图像的模糊特征了呢？又怎么保证模糊图像的内容编码器真的提取到图像的内容信息？ 让清晰图像内容编码器从清晰图像S 提取到的特征和模糊编码器从模糊图像b提取的模糊特征一同经过Gb生成模糊图像bs，并让bs是模糊化后的s，而不包含b的任何内容，也就是模糊编码器Eb不编码模糊图像的内容信息。 通过添加一个 KL 散度损失来规范模糊特征的分布，使其接近正态分布 p(z)∼N(0,1)。这个思路和 VAE 中的限制数据编码的潜在空间的分布思路是相近的，这里将模糊编码器的编码向量限制住，旨在控制模糊编码器仅对图像的模糊信息进行编码。 loss function experiments内容编码器由三个卷积层和四个残差块组成。模糊编码器包含四个卷积层和一个全连接层。对于生成器，该架构与内容编码器对称，具有四个残差块，后面是三个反卷积层。对于判别器， 采用多尺度结构，其中每个尺度的特征图经过五个卷积层，然后被送到 sigmoid 输出。 ablation study results","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"deblurring","slug":"Paper-reading/deblurring","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/deblurring/"}],"tags":[{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"},{"name":"deblurring","slug":"deblurring","permalink":"https://shiyuuuu.github.io/tags/deblurring/"},{"name":"unsupervised","slug":"unsupervised","permalink":"https://shiyuuuu.github.io/tags/unsupervised/"}]},{"title":"画出好看的图","slug":"画图","date":"2021-04-11T16:00:00.000Z","updated":"2021-04-12T08:17:19.694Z","comments":true,"path":"2021/04/12/画图/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/12/%E7%94%BB%E5%9B%BE/","excerpt":"","text":"吊打 Pyecharts，这个新 Python 绘图库竟然这么漂亮！ 链接: https://pan.baidu.com/s/1ccJBCiB_O_Xo-ci0UwjcDg 密码: ah9a 听说一分钟就可以制作时间轴？","categories":[{"name":"工具","slug":"工具","permalink":"https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/"},{"name":"画图","slug":"工具/画图","permalink":"https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/%E7%94%BB%E5%9B%BE/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://shiyuuuu.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"画图","slug":"画图","permalink":"https://shiyuuuu.github.io/tags/%E7%94%BB%E5%9B%BE/"}]},{"title":"阅读论文-Toward Convolutional Blind Denoising of Real Photographs","slug":"阅读论文_Toward Convolutional Blind Denoising of Real Photographs","date":"2021-04-09T16:00:00.000Z","updated":"2021-04-12T03:21:05.828Z","comments":true,"path":"2021/04/10/阅读论文_Toward Convolutional Blind Denoising of Real Photographs/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/10/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87_Toward%20Convolutional%20Blind%20Denoising%20of%20Real%20Photographs/","excerpt":"出处：CVPR2019","text":"出处：CVPR2019 paper code motivation尽管之前一些基于CNN的算法在图像的加性高斯白噪声去除上取得了很好的效果，但这些方法往往是在过拟合AWGN，而在真实噪声去除上效果并不好。这篇文章提出了一种更加真实的噪声模型，既考虑了信号依赖噪声也考虑了相机内的ISP。另一方面为了进一步提供交互方法，方便纠正去噪结果，他们设计了一个噪声估计的子网络，它利用了非对称学习的策略避免噪声水平的低估。他们的方法在3个real-world noisy image的数据集上取得了state-of-the-art的结果。 contribution 同时考虑heteroscedastic Gaussian noise和in-camera processing pipeline 非对称学习的噪声估计子网络 三个real-world noisy image dataset上性能SOTA 噪声模型在真实成像系统中，图像噪声有多个来源，并且受到相机内ISP 的影响，包括去马赛克，gamma校正和压缩。泊松高斯分布被认为是比AWGN更适合模拟真实噪声分布的模型，包含了平稳噪声部分和信号依赖部分。实际上，由传感产生的噪声可以被建模成泊松，其余的平稳噪声可以被建模为高斯，所以泊松高斯模型是一个合理的噪声模型。泊松高斯模型可以近似为异方差高斯。L 为raw图像的数值（辐照度），它的噪声可以建模为两部分：信号依赖噪声部分和平稳噪声部分。前一项通常和图像亮度有关，一般亮度越暗，噪声水平越大（比如夜景）文章还进一步考虑了相机内ISP 流程，这里的y表示合成的噪声图像，DM 表示去马赛克，f（）表示相机响应函数CRF进一步考虑压缩效应，JPEG压缩后的合成噪声图像为3式所示对于RAW图像，可以使用第一个公式合成噪声；对于未压缩图像，可以使用第二个公式合成图像；对于压缩图像，使用第三个公式合成图像。有了这个噪声模型就可以生成合成的noisy images methodnetwork architecture网络包含两个子网络，一个是噪声估计子网络，一个是非盲去噪子网络。噪声估计子网络将噪声观测图像y转换为估计的噪声水平图σ^(y)。然后，非盲去噪子网络将y和σ^(y)作为输入得到学习到的残差项，再加上y 最终的去噪结果x^。噪声估计子网络使用五层全卷积网络，卷积核为3×3×323×3×32，并且不进行pooling和batch normalization。非盲去噪子网络使用16层的U-Net结构另外，虽然在DnCNN中提到，batch normalization成功应用于高斯去噪中，但是对于真实图像的噪声去除并没有多大帮助，这可能是由于真实世界的噪声分别与高斯分布相差较大。除此之外，噪声估计子网络允许用户在估计的噪声水平图σ^(y) 输入到非盲去噪子网络之前对应进行调整。(红框） Asymmetric Loss作者观察到非盲去噪方法（如BM3D、FFDNet等）对噪声估计的误差具有非对称敏感性。在之前的非盲去噪方法中，如果输入的噪声和真实的噪声匹配，可以较好的去噪，如果输入噪声水平（估计））高于真实噪声水平（高估）去噪结果仍能保持较好的效果，虽然也平滑了部分低对比度的纹理。但当输入噪声标准差低于真实值时（低估），去噪结果包含可察觉的噪声。正是因为这个特性，BM3D可以通过设置相对较高的输入噪声标准差得到满意的真实图像去噪效果。因此，非盲去噪方法对低估误差比较敏感，而对高估的误差比较鲁棒。本文就提出了这种非对称loss 用来避免噪声水平图的低估。给定像素i的估计噪声水平σ^(yi)和真实噪声水平σ(yi)，当估计值小于真实值时，应该对其MSE引入更多的惩罚。0&lt;alpha&lt;0.5，低估时，前一项大于0.5，高估时，前一项小于0.5。另外引入TV loss约束噪声水平图的平滑性，最后是一个重建误差，L2 loss，总的loss是这三项加权相加。 experiments如果只用合成数据，真实图像的噪声并不能由本文的噪声模型完美的刻画，而如果只用真实噪声图像，在获取clean image的时候，需要上百张noisy images 做平均，这个成本很高，而且由这样做平均得到的图像往往过于平滑。为了提高网络的泛化能力，交替使用一批合成图像和一批真实图像进行训练。 results on real datasets NC12没有GT，DND包括50对真实噪声图像和对应的干净图像（由low-ISO image后处理得到），Nam包括11个静止场景，每个场景的无噪图像由500张JPEG带噪图像平均得到。在这三个数据集上都取得了最佳的效果。 ablation study验证不同噪声模型的效果：第一种是最普通的高斯噪声，第二种是本文里用的异方差高斯，第三种是普通高斯加ISP ，第四种是异方差高斯加ISP ，也就是本文所用的模型。 验证训练时是否既用合成数据集又用真实数据集的效果是否最好，有两个对照组：只用合成噪声图像训练，或者只用真实噪声图像训练。 验证非对称loss，alpha=0.5时，在低估和高估情况下，加的惩罚一样多，alpha&lt;0.5时，给低估加更多惩罚。Alpha=0.3时，效果最好 对于给定的噪声估计图，用户可以交互式的修改σ(y)，来修正去噪效果，具体是在σ(y)前乘一个系数，第一幅图当这个系数等于0.7时，效果最好，第二幅图是系数为1.3时效果最好。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"denoising","slug":"Paper-reading/denoising","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/denoising/"}],"tags":[{"name":"denoising","slug":"denoising","permalink":"https://shiyuuuu.github.io/tags/denoising/"},{"name":"blind","slug":"blind","permalink":"https://shiyuuuu.github.io/tags/blind/"}]},{"title":"阅读论文-Unpaired Image Super-Resolution using Pseudo-Supervision","slug":"Unpaired Image Super-Resolution using Pseudo-Supervision","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T08:26:08.506Z","comments":true,"path":"2021/04/09/Unpaired Image Super-Resolution using Pseudo-Supervision/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/","excerpt":"出处：CVPR2020","text":"出处：CVPR2020 paper supplemental motivation unpaired super-resolution，当aligned 的HR-LR training set is unavailable. deviation between the generated LR distribution and the true LR distribution causes train-test discrepancy contribution（author: propose a new training method that overcomes the shortcomings of the existing GAN based unpaired super-resolution methods: generated LR distribution and the true LR distribution causes train-test discrepancy） bridge the gap between the well-studied existing SR methods and the real-world SR problem without paired datasets. Because our correction network is trained on not only the generated LR images but also the true LR images through the bi-directional structure (因为我们的校正网络不仅通过生成的LR图像进行训练，而且还通过双向结构对真实的LR图像进行训练) : minimize the train-test discrepancy any existing SR networks and pixel-wise loss function can be integrated because the SR network is separated to be able to learn in a paired manner. (SR网络可以pair对的学习) 包含unpaired kernel/noise correction network和pseudo-paired SR network unpaired kernel/noise correction network: 去除噪声、调整输入图像的kernel，从输入的HR图像生成pseudo-clean的LR图像 pseudo paired SR network: 学习pseudo-clean的LR image到输入的HR image的映射 SR网络独立于效验网络(correction network) related works paired SR： VDSR, EDSR, RCAN，LapSRN，DBPN blind SR [39,12,57] 由任意的kernel降质得到的LR，学习由这样的LR到HR的映射，但当真实图像不是以假设的degradation降质的，（degradation估计不准），就会让真实图像SR的任务很差。 ZSSR， IKC， 关于blind SR的研究很少涉及blur kernels以外的综合降质问题（比如noise，compression artifact)。 GAN based methods [51,4,56, 32] 可以直接学习LR 到HR的映射，不需要degradation的假设。 可以大致分为两类：一类是直接从LR image出发，在生成的HR image和真实的HR image之间加discriminator. 这种方式的确定是无法用pixel wise的loss，因为real HR是未知的。 另一类是，在HR到LR的过程加GAN，生成的LR image和真实的LR image之间加discriminator，使生成的LR尽可能逼近真实的LR image。然后生成的LR与原来的HR之间用pixel-wise的loss训练一个LR2HR的网络，也就是U。与cycleGAN的区别：HR端没有discriminator。缺点：生成的LR分布与真实的LR分布存在偏差，导致在training set和test set性能差别大。（当test set的图像分布在training set中完全没有） ICCV 2017: DualGAN: Unsupervised Dual Learning for Image-to-Image Translation ICCV 2017: CycleGAN ECCV 2018：To learn image super-resolution, use a gan to learn how to do image degradation first ICCVW 2019：Unsupervised learning for real-world super-resolution 以上两个第一次训练HR2LR的网络，并用degraded的输出训练LR2HR的网络。 CVPRW 2018：Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks 提出cycle-in-cycle network，但他们的degradation网络是确定的，并且SR网络与bi-cycle网络合在一起。选择loss function的时候有局限性 input LR $x$， GT HR $z$， $z$ bicubic下采样得到$y$ (clean LR) 先看里面的LR2clean LR，$x\\sim x’ $ (半个cycleGAN) 生成得到的clean LR 进SR网络，与GT送入判别器$D_2$，通过$G_3$再回到real LR space，$x \\sim x’’$ 【pixel-wise loss不能用在HR space】 arxiv 2018：Unsupervised degradation learning for single image super-resolution 利用了双向的结构，他们也在选择loss function的时候有局限性 以上4种，ECCV 2018人脸的和arxiv 2018的这篇基本基于cycleGAN的结构。 这篇文章和以上这些文章最主要的区别是，解决了在训练数据集和测试数据集分布不一致的问题。也就是pseudo-clean LR 和 real LR 通过硬件和数据对齐的操作建立 real SR的数据集： ICCV 2019：Toward real-world single image super-resolution: A new benchmark and a new model CVPR 2019：Camera lens super-resolution CVPR 2019：Zoom to learn, learn to zoom method解决其他GAN-based unpaired SR的缺点：separating the entire network into an unpaired kernel/noise correction network and a pseudo-pairedSR network correction network：是一个cycleGAN， 完成的是unpaired的real LR和 clean LR之间的translation。clean LR由HR经过predetermined operation得到。 SR network：成对的学习pseudo-clean LR到HR mapping 在训练阶段，correction network也先由clean LR到true LR再回到clean LR生成pseudo-clean的LR 图像。SR network成对的学习pseudo-clean LR image到HR的mapping。 学习印射$F_{XY}:(X)LR-&gt;(Y)HR$, 定义 clean LR：$Y_\\downarrow$是由$Y$经过一个指定的下采样操作得到的：$Y\\rightarrow Y_\\downarrow$是bicubic 下采样和gaussian blur的组合得到的。本文将$F_{XY}$拆分为两个mapping$G_{XY_\\downarrow}$和$U_{Y_\\downarrow Y}$的组合。 domain transfer in LR，其中学习mapping$G_{XY_\\downarrow}$是通过上图蓝框中cycleGAN的结构 mapping from LR to HR，只看绿线部分，由HR domain出发，先经过bicubic+Gaussian的下采样得到clean LR $Y_\\downarrow$, 再把它依次过cycleGAN的两个generator得到pseudo-clean LR $\\mathring{y_{\\downarrow}}$ , pseudo-clean LR到HR的mapping为$U_{Y_\\downarrow Y}$. 而$\\mathring{y_{\\downarrow}}$和y是成pair的，所以经过$U_{Y_\\downarrow Y}$上采样得到的$U_{Y_\\downarrow Y}(\\mathring{y_{\\downarrow}})$与y 之间可以用任意的pixel-wise的loss。 HR discriminator, 希望减小训练和测试的偏差，尽管$\\mathring{y_{\\downarrow}}$用来训练SR网络，但是实际应用的时候，输入的LR image是$G_{XY_\\downarrow}(x)$。所以pseudo clean LR和由real LR生成的clean LR的超分辨率后的差异尽可能小，所以最后还在这二者过$U_{Y_\\downarrow Y}$的输出上加判别器$D_{X_\\uparrow}$ test phase, 黑色实线部分，由real LR image先印射到 clean LR image $G_{XY_\\downarrow}(x)$，（cycleGAN训练到比较理想情况的时候, clean LR 与由real LR生成的clean LR 以及pseudo clean LR都很接近） loss function对于两个生成器和3个判别器，总共的loss： \\mathcal{L}_{trans}=\\mathcal{L}_{adv}(G_{XY_\\downarrow}, D_{Y_\\downarrow}, X, Y_\\downarrow)+\\mathcal{L}_{adv}(G_{Y_\\downarrow X}, D_{X}, Y_\\downarrow, X)\\\\ +\\gamma\\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)\\\\ +\\lambda_{cyc}\\mathcal{L}_{cyc}(G_{Y_\\downarrow X},G_{XY_\\downarrow})+\\lambda_{idt}\\mathcal{L}_{idt}(G_{XY_\\downarrow})+\\lambda_{geo}\\mathcal{L}_{geo}(G_{XY_\\downarrow})其中，$\\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)$是HR discriminator的loss: \\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)\\\\ =\\mathbb{E}_{x\\sim P_x}[logD_{X_\\uparrow}(U_{Y_\\downarrow Y}\\circ G_{XY_\\downarrow}(x))]\\\\ +\\mathbb{E}_{y_\\downarrow \\sim P_{Y_\\downarrow}}[log(1-D_{X_\\uparrow}(U_{Y_\\downarrow Y}(\\mathring{y_\\downarrow})))]cycle consistency loss 被放松到只有单向的： \\mathcal{L}_{cyc}=||G_{XY_\\downarrow}\\circ G_{Y_\\downarrow X}(y_\\downarrow)-y_\\downarrow||_1这使$G_{Y_\\downarrow X}$可以一对多，满足不同的噪声和LR图像的分布。 identity loss在cycleGAN里用来保持图像的色彩，本文中也用了identity loss来避免色彩偏差: \\mathcal{L}_{idt}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(y_\\downarrow)-y_\\downarrow||_1geometric ensemble loss [CVPR 2019: Geometry consistent generative adversarial networks for one-sided unsupervised domain mapping] 减少可能的translation来保持场景的几何形状。本文中的geometric ensemble loss用来保证输入图像翻转、旋转不改变结果。 \\mathcal{L}_{geo}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(x)-\\sum_{i=1}^{8}T_i^{-1}(G_{XY_\\downarrow}(T_i(x)))/8||_1共带有8中翻转旋转模式。 而SR网络$U_{Y_\\downarrow Y}$是与生成器、判别器无关的网络，只是用来放大图像的局部特征来作为HR端判别器的输入。用下式来更新SR网络: \\mathcal{L}_{rec}=||U_{Y_\\downarrow Y(\\mathring y_{\\downarrow})}-y||_1这里的$\\mathcal{L}_{rec}$可以由任意的pixel wise的loss代替。（perceptual loss, texture loss, adversarial loss) network architecture最上面那路的$G_{XY_\\downarrow}$和 $U_{Y_\\downarrow Y}$用RCAN的网络 而$G_{Y_\\downarrow X}$的网络结构：resBlock+fusion layers+BN+Leaky ReLU 判别器：patchGAN，LR的判别器的stride=1,5层卷积, HR的判别器前面几层stride=2. Experimentssynthetic distortionsDIV2K realistic-wild dataset (800 训练图像) simulate ： 4倍下采样、运动模糊、pixel shifting、加性噪声 每张图只有一种degradation，但是图像与图像之间的degradation不同，对于每张训练图像，合成4张降质图像。 训练：800 HR+3200 LR（unpair） 测试：100张validation set 超参：$\\lambda_{cyc}=1, \\lambda_{idt}=1,\\gamma=0.1$, 4倍SR intermediate images compare with blind methods blind denoising: NC [The noise clinic: a blind image denoising algorithm], RL-restore [ CVPR 2018: Crafting a toolchain for image restoration by deep reinforcement learning] 【blind denoising还有CVPR 2019: Toward Convolutional Blind Denoising of Real Photographs (Kai Zhang)】 blind deblurring: SRN-Deblur[CVPR 2018: Scale-recurrent network for deep image deblurring], DeblurGAN-v2 [Arxiv2019：Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better] SR: DBPN(non-blind), ZSSR, IKC (blind)（zssr: CVPR2018, IKC: cvpr 2019) ZSSR+KernelGAN 这些方法是用的各自论文里提及的数据集训练，没有在这里的数据集上训练。 compare with NTIRE 2018 baselines 这些baseline是pair-trained （upper bounds) 这篇论文的方法PSNR比不过 baseline方法，但是SSIM与baseline方法相当. 因为PSNR高估了整体的亮度和色彩的细微差别，这些差别不会显著影响perceptual quality。 ablation study 第2行：去除HR的判别器 第3行：SR network是在$y_\\downarrow$上训练的，而不是$\\mathring{y_\\downarrow}$上，这就相当于一个real LR和Gaussian+bicubic LR之间translation的网络(cycle GAN)加一个SR网络。 第4行：SR network是在$G_{Y_\\downarrow X}(y_\\downarrow)$ 上训练的，而不是$\\mathring{y_\\downarrow}$上. 相当于图1 的b 第5行：在第4行基础上，用RCAN官方的模型做validation。 perception-oriented training 在HR的$\\mathcal{L}_{rec}$里加上perceptual loss, content loss (ESRGAN里面的)，relativistic adversarial loss. 加上这些loss 后视觉效果比L1 loss好 realistic distortion 1follow unsupervised 人脸SR [ECCV 2018: To learn image super-resolution, use a gan to learn how to do image degradation ﬁrst] HR face images: Celeb-A, AFLW, LS3D-W, VGGFace2 (64*64) LR face images: 50000张Widerface 包含多种degradation（留出3000作为测试）(16*16) \\mathcal{L}_{\\bar{idt}}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(x)-x||_1他们实验发现identity loss加在x上比加在$y_\\downarrow$ 上好。 超参：$\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=2,\\lambda_{geo}=1,\\gamma=0.1$ 训练2倍SR：32*32=&gt;64*64 视觉指标FID比较：高亮的为基于GAN的unpaired的方法。 one-to-many degradation realistic distortion 2aerial image dataset DOTA GSD (ground sample distances), 62 LR images GSD在[55cm, 65cm]之间， HR image的GSD为30cm 超参：$\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=10,\\lambda_{geo}=100,\\gamma=0.1$ , 2倍SR 在这种数据集里，物体的像素点很少，所以对identity loss和geometric loss用了更大的权重。在训练初期，逐步提高geometric loss的权重。 只提供视觉上的比较，因为没有GT。 先用RL-restore（强化学习blind去噪修复）在input的LR image上去噪。但是他的输出over-smoothed. 即使再用SOTA的blind SR方法ZSSR超分辨，artifacts也不能被完全移除。 geometric loss的作用： AIM 2019 real world SR challenge没有HR-LR pair,测试时有官方的脚步来计算PSNR/SSIM。 $\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=5,\\lambda_{geo}=1,\\gamma=0.1$ ​ LPIPS: 视觉指标，越低越好。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"pseudo-supervision","slug":"pseudo-supervision","permalink":"https://shiyuuuu.github.io/tags/pseudo-supervision/"},{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"}]},{"title":"阅读论文-Dual super-resolution learning for semantic segmentation","slug":"Dual super-resolution learning for semantic segmentation","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T08:49:26.193Z","comments":true,"path":"2021/04/09/Dual super-resolution learning for semantic segmentation/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/","excerpt":"出处：CVPR2020 (oral)","text":"出处：CVPR2020 (oral) paper code unofficial-code motivation在不增加计算开销的前提下提高语义分割的性能，而语义分割依赖于HR feature representations，如果直接输入HR images会产生很大的计算开销，所以做SSSR。而只依赖于SSSR的decoder难以恢复original details，所以他们用了SISR来guide SSSR，如何guide呢？用feature affinity module，核心是feature相似矩阵的距离。 related works现有的语义分割方法能取得好的性能依赖于HR的深度特征表示：large computation budgets 现有的语义分割方法保持HR representations: 用空洞卷积代替 strided 卷积，比如DeepLabs 结合HR的pathway，比如Unet 这些方法的输入通常是original HR image，当限制输入图像的大小的时候，他们的性能有明显的下降。（现有语义分割方法：FCN, DeepLabs, PSPNet, 空洞卷积，pyramid pooling module, attention， context encoding） 现有的轻量级的语音分割：通过factorization加速卷积，ESPNets（split-merge，reduce-expand加速卷积计算），采用一些有效的分类网络（mobileNet、shuffleNet），知识蒸馏帮助训练对抗网络。但他们的性能比SOTA差很多。 本文提出一种two-stream的framework (dual super-resolution learning, DSRL) 在不产生额外计算开销的情况下提高semantic 分割的准确率: 对于LR的输入保持HR representations。具体的，SISR得到的HR features用来guide spatial维度的相关性学习。DSRL可以在相同的resolution下，显著提高准确率。 现有的SISR方法： pre-upsampling SR：先通过bicubic上采样得到HR图像，再用网络refine HR图像（计算开销大：网络在HR上做的） post-upsampling SR：在网络的最后面用可学习的上采样层 progressive SR：逐渐提高分辨率，可以handle multi-scale的SR（deep laplacian pyramid networks） iterative up-and-down SR：通过iterative上\\下采样的层得到中间图像，结合中间图像重建最终图像(deep back-projection networks) multi tasks： mask R-cnn（检测+实例分割） RCNN（姿态估计+动作识别） cross tasks： （希望把语义分割作为主要任务，SISR作为附加任务） proposed methodReview of encoder-decoder framework用来提取特征的encoder的scaling step是2，OS通常是8或者16（the ratio of input image spatial resolution to the Encoder output resolution），把最后两层strided conv换成空洞卷积。在decoder端，用一个bilinear 上采样层恢复分辨率。 现有的方法只能将feature上采样至与input image同样的大小，可能比original image小。（分割网络的输入往往是对原图做了下采样）。这样可能损失了一部分有用的label信息，另一方面，也难以只依赖decoder恢复original details。 DSRLcontribution： 在不额外增加计算开销的前提下，通过保持HR representations，提高性能 泛化性：可以扩展到需要HR representation的任务中，比如人体姿态估计 实验：在语音分割和人体姿态估计任务上都获得了良好的性能，相同计算开销，提高2% 包含三个部分： semantic segmentation super-resolution (SSSR) single image super-resolution (SISR) feature affinity (FA) 其中SISR与SSSR共享特征提取器， SSSR：加额外的上采样层（一些deconv）来得到最终的prediction mask，比如输入的是5121024，输出1024\\2048. 他们的方法可以利用全部的label SISR：只依赖decoder不足够恢复HR semantic特征表示，因为decoder的上采样结构不是简单的bilinear上采样，就是简单的sub-net，这并不会引入额外的信息，因为输入是LR的。 而SISR可以有效恢复图像细节，SSSR和SISR的feature如下图所示，SISR的feature包含更多的物体更多的复杂结构，尽管这些结构不能直接揭示物体属于哪一类，但他们可以根据像素与像素、区域与区域直接的相关性group起来，而这些像素区域的关联揭示了语义信息。 所以，SISR得到的HR feature用来guide SSSR的HR feature的学习。SISR根据original image的GT来优化。 FA: feature affinity SISR得到的structure information如何guide SSSR？用 feature affinity learning，FA来学习SSSR和SISR feature上相似矩阵的距离，相似矩阵刻画的是像素直接pairwise的关系。理论上，应该计算所有像素对的affinity，为了节省计算开销，他们subsamples得到1/8的像素对。为了减少由SISR和SSSR不一致引起的训练的不稳定性，他们还附加了一个feature transform模块。 S_{ij}=(\\frac{F_i}{||F_i||_p})^{T}(\\frac{F_j}{||F_j||_p})最后的FA loss为： L_{fa}=\\frac{1}{W^2H^2}\\sum_{i=1}^{W'H'}\\sum_{j=1}^{W'H'}||S_{ij}^{seg}-S_{ij}^{sr}||_q$p=2, q=1$，总的loss： L=L_{ce}+w_1L_{mse}+w_2L_{fa},\\\\其中， L_{ce}=\\frac{1}{N}\\sum_{i=1}^{N}-y_ilog(p_i),\\\\ L_{mse}=\\frac{1}{N}\\sum_{i=1}^{N}||SISR(X_i)-Y_i||^2$L_{ce}$ 为cross entropy loss, $w_1=0.1, w_2=1.0$ experimentssemantic segmentation：CitySpace数据集，将一张图分为19类。10242048；CamVid数据集，11类，960\\720。 metric：mIoU（mean Intersection over Union） segmentation architecture: ESPNetv2, DeepLabv3+(ablation study), PSPNet, BiseNet, DABNet (lightweight) effect of components输入：256512(resize, 1024\\2048=&gt;256*512) 输出：+SR的输出是5121024（2倍），不加SR的输出：256\\512 +SSSR+SISR+FA &gt; +SSSR+SISR &gt; SSSR &gt; 不加 原图下采样作为baseline和自己方法的输入，输出分辨率不同 【输出分辨率不同怎么比的？】 把原图的label分别下采样到256*512和512*1024比 output统统上采样到原图的分辨率（1024*2048）和label比。（保持同分辨率下比较，并且不对原始的label降质） 第2 种比较相当于比的是bicubic+seg和SR+seg effect of various input resolutionsinput resolution：256*512, 320*640, 384*768, 448*896, 512*1024 (CitySpace) 对于每一种input resolution，用他们的framework都比不用性能好，且随着input resolution逐渐增大，性能增值越来越小。 human pose estimationmetric：object keypoint similarity (OKS) architecture: HRNet-w32, 用offline person detection的结果预测关键点 输入human detection box (缩放到固定大小：256192， 162\\128, 128*96) 输出heatmap（64*48） resultsvisualization of segmentation features 对于structured objection提升尤其明显，比如人，车","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"segmentation","slug":"segmentation","permalink":"https://shiyuuuu.github.io/tags/segmentation/"},{"name":"LV+HV","slug":"LV-HV","permalink":"https://shiyuuuu.github.io/tags/LV-HV/"}]},{"title":"阅读论文-Learning to Have an Ear for Face Super-Resolution","slug":"Learning to Have an Ear for Face Super-Resolution","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T09:29:10.045Z","comments":true,"path":"2021/04/09/Learning to Have an Ear for Face Super-Resolution/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/","excerpt":"出处：CVPR2020 (oral)","text":"出处：CVPR2020 (oral) paper code project task: 用audio和LR图像做16倍的人脸超分辨率，输入的LR图像非常小：8*8 pixels, 这些图像的很多重要细节都被丢失了。如果LR的人脸图像是从视频中提取的，那么我们也可以得到这个人的音频信息，而audio中带有一些脸部的特征：性别和年龄。结合听觉和视觉，他们提出了一个：先从单独的音轨构建脸部特征的latent 表示，再从LR图像简历脸部特征潜在的表示。然后再fusion这两个表示。 motivation limited information ambiguous mapping =&gt; incorporate alternative source of information: audio audio carries information about age and gender, audio tracks are available in videos, audio and visual signals both capture some shared attributes of a person. contribution the first attempt to use audio for image restoration use both audio and a LR image to perform extreme face super-resolution ($16\\times$) do not use human annotation and thus can be easily trained with video datasets 建立了图像和音频的分解表示，因此它可以混合来自不同视频的LR图像和音频，并生成语义上有意义的真实面孔。（our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations. ） related worksspeech2Faceaudio to image: speech2Face [CVPR 2019: Speech2face: Learning the face behind a voice] styleGAN给定512维随机向量， styleGAN可以从这个向量重构维一张从未见过的人脸照片。 如果训练一个model，是从图片到512维latent encoding, 这个latent encoding可以通过styleGAN还原为原图。（图=&gt;512维latent code=&gt;原图 ） 也就是说，可以用这512维向量生成原图。 如果对这512维的空间稍微做点改变？这个超高维的空间对应人脸的不同属性（肤色、年龄、性别） 如何知道年龄对应的维度是哪些？ 1、作者一开始会训练一个分类器，分类器的训练样本、label来自于CelebA，40维label中就有一个维度是年龄（作者贴出来的代码里的wp.npy文件我猜就是CELEBA的label文件） 2、分类器收敛后，用个随机噪声z作为stylegan输入，生成图片x，再把这个x送入分类器，得到分类结果y，用个线性变换（或非线性也行）建立起z与y的关系。。。。然后你就可以通过控制y的变换方向来得到z，再生成想要的x了 总之，styleGAN可以从latent code生成逼真的人脸，更重要的是latent code对应人脸不同属性，可以通过改变latent code得到不同的人脸。（比如得到XX小时候的照片）。 Naive end-to-end training带有两个编码网络的多模态网络，再把encoder的输出concate再一起送入decoding网络得到HR图像。但这种多模态网络以传统的训练方法很难训练好，因为：不同模态的收敛速度不同。 实验发现，这样直接一起训练会忽略audio信号。audio信号需要更长的处理更强的网络来拟合其latent space。 method 包含这样几个部分：LR encoder $E_l$, audio encoder $E_a$，fusion network $F$, face generator$G$ 分开训练LR image encoder和audio encoder，这样他们的解耦精度就相等。fusion: audio作用在LR image固定的中间representation中，这样audio中的人脸属性可以解耦出来。 为什么styleGAN？styleGAN可以通过操控latent code的某些维度，改变生成人脸的某些属性。 Inverting the Generator 首先训练一个从高斯隐空间$z \\sim \\mathcal{N}(0,I_d)$（d维）开始的输出高分辨率图像的generator $G(z)$ 【styleGAN: 人脸生成，可以控制所生成图像的高层级属性：头发等，以高斯分布作为输入，输出高质量的samples和隐空间】，再，用自编码器的限制，通过固定$G(z)$得到generator(fixed) 反转后的encoder ($E_h$). 再将这两部分$G(z)$和$E_h$fine-tuning。这个encoder的映射是HR image到generator输入的隐空间，这个$E_h$可以得到图像$x_i$对应的representations ($z_i$)可以当做LR,audio，fusion network的encoder的目标，generator的输出是input HR的近似。【这个模型可以作为生成HR人脸图像的先验，并且中间的representations应该可以由audio编辑】 给定数据集$\\mathcal{D}=\\{(x_i^h,x_i^l,a_i)|i=1,…,n \\}$, \\mathcal{L}_{pre-train}=\\sum_{i=1}^{n}|G(z_i)-x_i^h|_1+\\lambda_f\\mathcal{l}_{feat}(G(z_i),x_i^h)其中$z_i=E_h(x_i^h)$ ，$l_{feat}$ 是perceptual loss (VGG feature) 他们实验发现只回归一个$z_i$不足以很好的恢复$x_i^h$，所以像styleGAN一样，把$z_i$非线性变换得到$w_i$，所以他们生成$k$个不同的$z_{ij},j=1,…,k$. 将非线性变换得到的$w_{ij}$分别插入到generator的不同层。 再fine-tuning： min_{E_h,G}\\mathcal{L}_{pre-train}+\\lambda_t|G_{init}-G|_2^2其中$G_{init}$是styleGAN训练后$G$的权重。训练过程中，总的loss最小后，将$\\lambda_t$减小为原来1/2，pre-training和减小正则化：让encoder和decoder逐渐收敛，不至于过早失去G的latent representation的结构 总结，(1). 从标准高斯分布学习G(z)（styleGAN），获得HR 图像的分布 (2). 固定$G(z)$, 用公式1用autoencoder的方式训练reference encoder$E_h$, (3). fine-tuning $G(z)$和$E_h$，$E_h$可以得到HR图像的latent representations. Pre-training Low-Res and Audio encoders 给定HR-LR pair，pre-train一个LR encoder，将输入的LR映射到与HR相同的的reference encoder（前面训练的）输出的latent representations 如果直接训练fusion network，使fusion model$F(x_i^l,a_i)$映射到$z_i=E_h(x_i^h)$，会使网络完全忽略音频信号$a_i$。所以他们先分开训练encoder$E_l$和$E_a$， 让他们尽可能从这两种模态多提取信息，再fusion他们。 为了防止过拟合，pre-train $E_l$和$E_a$时，只用一半数量的训练数据（记为$\\mathcal{D}_{pre}$），在fusion的训练阶段用整个训练数据。 训练$E_l$的目标函数： min_{E_l}\\sum_{x_i^l,x_i^h \\in \\mathcal{D_{pre}}}|E_l(x_i^l)-z_i|_1+\\lambda|D \\circ G(E_l(x_i^l))-x_i^l|_1其中$D \\circ x$是$x$的16倍下采样。$\\lambda=40$. 前面一项很好理解：希望$E_l$在LR图像$x_i^l$上得到的latent representation与HR图像上的latent representation($z_i=E_h(x_i^h)$)相似。第二项：LR图像$x_i^l$经过encoder(得到HR图像的latent representation)再经过decoder$G$（得到HR图像$x_i^h$）再16倍下采样得到LR 图像$x_i^l$ （因为$x_i^l$与$x_i^h$成pair） 而对于音频encoder：如果将$E_a(a_i)$回归到$z_i$必然有overfitting，因为一些$z_i$中有的属性，$a_i$中没有，比如脸部的姿态(朝左朝右？)。为了消除$z_i$中与$a_i$无关的属性，将$E_a(a_i)$的目标定义为： \\bar{z_i}=\\frac{1}{2}(E_h(x_i^h)+E_h(\\hat{x}_i^h))这里的 $\\hat{x}_i^h$是$x_i^h$的水平翻转版本（将原HR图像水平翻转）。 训练$E_a$的目标函数： min_{E_a}\\sum_{a_i,x_i^h \\in \\mathcal{D}_{pre}}|E_a(a_i)-\\bar{z_i}|_1【这里没懂】由于styleGAN的分层的结构，水平翻转的图片的latent code求平均，就消除了音频无法传递的信息。比如图里，输入朝左的面部图像和它水平翻转后的图像（这个水平翻转图像的面部朝向是朝右的），把encoder提取到的latent code求平均，再经过decoder，就得到了朝向正面的图像（消除了音频无法传递的面部朝向信息）。 小结： audio encoder, fusion network: 固定LR image encoder， 提升他的latent representation 。为了加快audio encoder的训练速度，将HR reference encoder的输出和其水平镜像的平均值作为latent representation 对audio encoder预训练。而这个平均消除了音频无法传递的信息，比如视点。 fusing audio and low-resolution encodings现在希望聚合pre-train得到的encoder$E_l$和$E_a$提取的信息。由于$E_l$已经是$E_h$的近似，那么希望引入的音频能补出residual：$\\Delta z_i=z_i-z_i^l$, 所以fusion network $F$应满足： z_i^f=E_l(x_i^l)+F(E_l(x_i^l),E_a(a_i))因为$E_a$更难训练，所以继续把$E_a$和$F$一起优化。所以，fusion network的优化目标是: min_{E_a,F}=\\sum_{a_i,x_i^h,x_i^l \\in \\mathcal{D}}|z_i^f-z_i|_1+\\lambda|D \\circ G(z_i^f)-x_i^l|_1 (a). matching inputs; (b) LR image与audio来自不同video 把LR image(8*8)输入到$E_l$得到latent representation, fusion network 融合$E_a$编码后的音轨和encoded LR image, 这一部分与前面的latent representation相加得到的新的latent representation 与预先训练得到的HR image的latent representation很相似，再通过decoder G输出HR图像 共有3个mappings： audio to HR (speech2face是用预训练的人脸识别网络作为额外监督，而我们的方法是完全无监督的) LR to HR LR+audio to HR Experimentsdataset： VoxCeleb2 Dataset, 包含145K 人说话的video 2M frames at 128*128 pixels. 将每个speaker的一半的数据放入$\\mathcal{D_{pre}}$ test set: 同人的不同video. audio to image简单的比较了一下他们的audio-only model ($E_a+G$) 和speech2Face 第一行是speech2face的结果，第二行是他们的audio2image的结果 ($E_a+G$) 性别分类：96%~97%准确率（没有在性别分类上与speech2face比，因为speech2face训练时用了分类器的监督） classification as a performance measure预训练好的 身份分类器、性别分类器、年龄分类器 closed set：training set和test set用同一个人的不同video open set：training set和 test set不同人 ablations (a), (b): 训练$E_h$后时候和$G$一起fine-tune 【(b): HR 图像的表现，upper bound】 (c), (d)：without fusion F。只要LR 或者只要音频 ​ c与d相比，Audio更能提供性别信息。$\\mathcal{C}_g$ (e)~(h): (f)(g)(h)相比：不加audio&lt;固定audio encoder&lt;fine-tuning ​ (e)与(h)相比：一个全连接层&lt;三个全连接层 ​ (g)与(h)相比：在训练fusion网络时也fine-tuning $E_a$ 对结果有些许提升。 gender和age在open set上也能预测比较准确。 comparisons to other SR methods LapSR(CVPR 2017) , W-SRNet (ICCV 2017 人脸SR) 在这个数据集上重新训练。 mixing给定LR，与不同的audio混合 failure cases","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"audio+image","slug":"audio-image","permalink":"https://shiyuuuu.github.io/tags/audio-image/"}]},{"title":"阅读论文-Unpaired Image Super-Resolution using Pseudo-Supervision","slug":"阅读论文SAN","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-10T12:43:46.869Z","comments":true,"path":"2021/04/09/阅读论文SAN/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87SAN/","excerpt":"出处：CVPR2019 (oral)","text":"出处：CVPR2019 (oral) paper supp video code bicubic SR 的 SOTA motivation现有的SISR方法专注于更宽更深的结构设计，而忽略了挖掘中间层特征之间的相关性。本文受启发于ICCV 2017一篇做识别分类的文章（利用了二阶统计信息），提出了一个深度二阶注意力网络SAN 以获得更好的特征表达和特征相关性学习。特别地，提出了一个二阶通道注意力机制SOCA来进行相关性学习。同时，提出了一个non-locally增强残差组NLRG来捕获长距离空间内容信息。性能方面取得了SISR最好的结果。 contribution 设计了一个利用了二阶统计信息的网络：second-order attention network (SAN) 特别的，SAN网络包含second-order channel attention (SOCA)模块 然后还结合了non-locally enhanced residual group (NLRG)来捕获长距离空间内容信息。 Method RL-NL模块non-local模块是用来在high-level任务中捕获整幅图像的长范围依赖的。但是，全局non-local操作可能会受限于：1)全局non-local操作需要大量的计算量; 2)对于low-level的任务来说，在一定的区域范围中进行non-local操作是更有效的所以在SAN里，将图像划为k*k个region, 在每个region中进行non-local 操作，RL-NL充分利用LR feature的结构关联 LSRAG模块每一个同源残差组结构（SSRG）包括G 个局部模块LSRAG加上一个同源残差连接结构SSC，简单的LSRAG堆叠不能取得更好的性能，于是加入SSC（share-source skip connections），使LR的低频信息通过，且利用训练。所谓同源残差连接，就是把LR的特征加到每个group的输入x中，这种连接不仅可以帮助深度CNN的训练，同时还可以传递LR图像中丰富的低频信息给high-level的层。Wssc是一个可学习参数，一开始被设置为0。对于每个group来说，都会收到SSC传递过来的F0 每个LSRAG包括M个简单的residual blocks 和local source skip connection，最后还包含一个二阶channel attention 模块（second-order channel attention module）在LSRAG模块最后有一个二阶channel attention 模块，这是本文的重点 关于channel attentionRCAN中： 如上图所示，输入是一个 H×W×C 的特征，我们先进行一个空间的全局平均池化得到一个 1×1×C 的通道描述。接着，再经过一个下采样层和一个上采样层得到每一个通道的权重系数，将权重系数和原来的特征相乘即可得到缩放后的新特征，整个过程实际上就是对不同通道的特征重新进行了加权分配。其中，下采样和上采样层都利用 1×1 的卷积来实现，下采样层的通道数减少 r 倍，f 是激活函数 sigmoid 文本的二阶channel attention和RCAN的channel attention的区别在于将 全局平均池化 替代为全局协方差池化。 文本的二阶channel attention和RCAN的channel attention的区别在于将 全局平均池化 替代为全局协方差池化。 协方差归一化（就是所谓的二阶）协方差可以用来描述变量之间的相关性，所以对于HxWxC的特征，reshape为WH（C个维度），可以用协方差矩阵描述C个通道之间的相关性。$I$和1分别是sxs的单位矩阵和全1矩阵。因为这个协方差矩阵是个实对称矩阵，实对称矩阵必可由正交矩阵对角化，可对协方差矩阵作特征值分解，U为正交矩阵，Λ为对角阵，对角元素为特征值，按非增顺序排列。再对协方差做正则化，因为协方差的正则化对表示特征非常重要。Alpha=0.5 在具体实现上，在SSRG模块里包含20个LSRAG，和一前一后共两个region-level non-local模块，RL-NL中将图像分为2*2的region，在每个LSRAG里用了10个residual blocks.上采样模块用的是sub-pixel上采样。 Resultsablation study 与其他SR方法对比 这个是视觉上的比较，对于丰富复杂的纹理，SAN能更好的恢复","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"pseudo-supervision","slug":"pseudo-supervision","permalink":"https://shiyuuuu.github.io/tags/pseudo-supervision/"},{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"}]},{"title":"阅读论文-GLEAN Generative Latent Bank for Large-Factor Image Super-Resolution","slug":"阅读论文GLEAN","date":"2021-03-28T16:00:00.000Z","updated":"2021-04-09T07:54:01.237Z","comments":true,"path":"2021/03/29/阅读论文GLEAN/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/","excerpt":"出处：CVPR2021 (oral)","text":"出处：CVPR2021 (oral) project code paper 任务是： 大尺度超分辨率（8$\\times$ 到 64$\\times$），most details and textures are lost during downsampling. motivation 已有的SR方法： solely rely on $L_2$ loss: 视觉质量不好 (over-smoothing artifacts)。 adverssrial loss [ESRGAN]: Generator既要捕获图像characteristics又要保真（maintaining the fidelity to the GT）, 限制了其近似自然图像的能力，产生artifacts GAN inversion methods [PULSE]：反转pre-trained GAN的生成过程：把image mapping回latent space；再由latent space中optimal vector重建图像。只靠低维隐向量不足以指导重建的过程，使得产生的结果low fidelity. 需要image-specific, iterative的优化. 利用pre-trained GAN作为latent bank， 充分利用pre-trained GAN中封装的丰富且多样的先验。换用不同的bank可以不同类的图像：cat，building，human face，car. 利用字典学习的方式，测试阶段，只需要一次前传即可得到恢复后的图像。 GLEAN 的整体结构：encoder-bank-decoder related worklarge-factor SR fully probabilistic pixel recursive network for upsampling extremely coarse images with resolution 8×8. (Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.) RFB-ESRGAN：adopts multi-scale receptive fields blocks for 16× SR. (Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, and Yandong Guo. Perceptual extreme super resolution network with receptive field block. In CVPRW, 2020.) VarSR: 8× SR by matching the latent distributions of LR and HR images to recover the missing details. (Sangeek Hyun and Jae-Pil Heo. VarSR: Variational super-resolution network for very low resolution images. In ECCV, 2020.) perform 16× reference-based SR on paintings with a non-local matching module and a wavelet texture loss. (Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, and Yun Fu. Texture hallucination for large-scale painting super-resolution. In ECCV, 2020.) GAN inversion David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al. Semantic photo manipulation with a generative image prior. TOG, 2020. Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code GAN prior. In CVPR, 2020. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. 通过pixel-wise约束，迭代优化styleGAN的隐变量。 Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In ECCV, 2020. finetune generator和latent code来缩小训练集和测试集分布的gap. 降质图像$x$，latent space：$\\mathcal{Z}$： z^*=argmin_{z \\in \\mathcal{Z}}\\mathcal{L}(G(z), x)缺点： 低维的隐向量不能保持图像的spatial information. 方法passing both the latent vectors and multi-resolution convolutional features from the encoder. multi-resolution cues need to be passed from the bank to the decoder. 整个结构为encoder-bank-decoder encoder：$E_0$为RRDB-Net，$E_i, i \\in\\{1,…,N\\}$代表堆叠一个stride=2的conv和一个stride=1的conv. 最后由FC层得到$C$, $C$表示隐向量，提供high-level信息。为了更好的指导结构重建，将多分辨率的特征和隐向量都送入bank. Generative latent bank: 用pre-trained的Generator, styleGAN，提供纹理和细节生成的先验。 对generator的每个block输入不同的隐向量$C_i$, $i \\in \\{0,…,k-1\\}$ $\\{g_i\\}$代表每个block输出的feature, 它是由$C_i, g_{i-1}, f_{N-i}$由augmented style block得到。 不直接输出结果，而是将特征$\\{g_i\\}$输入到decoder 优势：像reference-based SR，HR reference image作为显式图像字典。性能很受 输入和reference相似度的影响。GLEAN用GAN-based的字典，不依赖于任何具体的图像，它获取的是图像的分布。而且没有global matching和reference images selection， 计算简便。 decoder：progressive地聚合来自encoder和latent bank的特征。每个conv后跟着pixel-shuffle层。由于有encoder和decoder之间的skip-connection，encoder捕获的信息可以被强化，bank专注于纹理和细节的生成。 训练：$\\mathcal{l_2}$ loss, perceptual loss, adversarial loss. 训练时fix住latent bank，实验发现，finetune latent bank没有性能提升，而且可能使latent bank偏向训练集的分布。 主要结果 image retouching 参考文献：[ESRGAN]: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. ESRGAN: Enhanced super-resolution generative adversarial networks. In ECCVW, 2018. [PULSE]: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. reference-based SR Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restoration via deep multi-scale component dictionaries. In ECCV, 2020. Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020. Xu Yan, Weibing Zhao, Kun Yuan, Ruimao Zhang, Zhen Li, and Shuguang Cui. Towards content-independent multi-reference super-resolution: Adaptive pattern matching and feature aggregation. In ECCV, 2020. Yang Zhang, Ivor W Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, and Xin Yu. Copy and Paste GAN: Face hallucination from shaded thumbnails. In CVPR, 2020. Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. In CVPR, 2019.","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"}]},{"title":"阅读论文-Zero-Reference deep curve estimation for low-light image enhancement","slug":"Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement","date":"2021-03-28T16:00:00.000Z","updated":"2021-03-29T13:08:40.347Z","comments":true,"path":"2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","excerpt":"出处：CVPR2020","text":"出处：CVPR2020 paper PDF supplemental materials project: https://li-chongyi.github.io/Proj_Zero-DCE.html code: https://github.com/Li-Chongyi/Zero-DCE motivation图像编辑软件通过调节曲线来增强图像=&gt;image-specific 曲线估计： 根据给定图像估计pixel-wise的调整曲线。不需要任何成对或不成对的训练数据（不需要reference/GT) 图像增强=&gt;非线性曲线映射 而不是通过image-to-image mapping 方法： non-reference loss functions contribution no reference: 避免了需要paired/unpaired数据的方法中的overfitting的问题 设计image-specific曲线：高次、pixel-wise 提升人脸识别的性能 related work传统方法调整图像的直方图分布调整，增大图像的动态范围。 global level: [1]. Dinu Coltuc, Philippe Bolon, and Jean-Marc Chassery. Exact histogram specification. IEEE Transactions on Image Processing, 15(5):1143–1152, 2006. [2]. Haidi Ibrahim and Nicholas Sia Pik Kong. Brightness preserving dynamic histogram equalization for image contrast enhancement. IEEE Transactions on Consumer Electronics, 53(4):1752–1758, 2007. local level: [3]. Chulwoo Lee, Chul Lee, and Chang-Su Kim. Contrast enhancement based on layered difference representation of 2d histograms. IEEE Transactions on Image Processing, 22(12):5372–5384, 2013. [4]. J Alex Stark. Adaptive image contrast enhancement using generalizations of histogram equalization. IEEE Transactions on Image Processing, 9(5):889–896, 2000. Retinex theory (将图像分解为reflectance和illumination，其中reflectance分量在任何光照条件下保持一致，图像质量增强任务变为illumination estimation问题): [5]. Edwin H Land. The retinex theory of color vision. Scientific American, 237(6):108–128, 1977. 自然的信息保存方法： [6]. Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Processing, 22(9):3538–3548, 2013. weighted vatiation: [7]. Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016. coarse illumination map: 搜索RGB中最大intensity的pixel [8]. Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, 2017. 考虑噪声： [9]. Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, 2018 自动exposure校正方法： 通过全局优化方法估计图像的S形曲线。 [10]. Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV, 2012. Data-driven方法CNN-based一般需要pair对的数据：资源密集型(resource-intensive)。一般这种成对的数据通过 自动光照退化、改变相机的设置 采集 或者用image retouching合成 LOL数据集[Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018.] 通过改变曝光时间和ISO获取成对的low/normal光照的图像。 MIT-adobe FiveK数据集[Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo ´ Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR, 2011.]包括5000raw图，每一张raw图由专家生成5个retouched图像。 [11]提出了一种估计illumination的方法：poor generalization capability [11]. Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019. GAN-basedEnlightenGAN[Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. In CVPR, 2019.] 用unpair的low/normal光照的数据。需要仔细挑选unpaired的训练数据。 methods 通过non-reference loss function实现不需要paired/unpaired的数据。通过迭代训练得到结果。 Light-enhancement curve自适应的曲线参数只由输入图像决定。曲线要是单调的来保持周围像素的区别。曲线要处处可微保证可以梯度反传。 LE(I(x);\\alpha)=I(x)+\\alpha I(x)(1-I(x))其中$\\alpha \\in [-1,1]$, 将LE曲线分别作用于R\\G\\B通道，而不仅仅作用于illumination通道。调节3个通道可以更好的保持固有的色彩，减少过饱和。 上式可以多次迭代，写为： LE_n (x)=LE_{n-1}(x)+\\alpha_nLE_{n-1}(x)(1-LE_{n-1}(x))其中n为迭代次数，他们设为8. 多次迭代有更强的调节能力。 但是如果$\\alpha$在每一个点是固定值，只能是全局调节。全局调节容易带来over-/under- enhance local区域。为了解决这个问题，他们将$\\alpha$设置为pixel-wise的参数。上式变为： LE_n (x)=LE_{n-1}(x)+A_n(x)LE_{n-1}(x)(1-LE_{n-1}(x))其中A是一个与原图等大小的parameter map。 假设，一个局部区域的像素点有相同的intensity. 所以输出图像里的相邻像素也有单调的关系。 DCE-Net（具体结构见补充材料） 输入：低光照图像 输出：pixel-wise的高阶curve参数图。8次迭代共24个parameter maps 网络细节：不包括down-sampling和BN，它们会丢失相邻像素的关系。最后一层conv接Tanh 1. spatial consistency loss使增强后的图像保持时域的连贯性(coherence) L_{spa}=\\frac{1}{K}\\sum_{i=1}^K\\sum_{j \\in \\Omega(i)}(|(Y_i-Y_j|-|I_i-I_j|)^2$K$为局部区域数，$\\Omega(i)$为4个相邻区域（上下左右）。Y和I是局部区域的平均强度值(intensity)。局部区域的大小设为4*4. 相当于对K个4*4的局部区域，要求enhance前后图的局部区域与其邻域的强度差，差不多。 2. exposure control loss控制曝光的程度。测量局部区域的平均强度与well-exposedness的级别E之间的距离。E=0.6、在[0.4,0.7]之间差不多。 L_{exp}=\\frac{1}{M}\\sum_{k=1}^M|Y_k-E|M代表M个16*16的没有overlap的局部区域，Y是enhanced图像中局部的平均强度值。 3. color constancy loss校正色彩偏差。 L_{loc}=\\sum_{\\forall(p,q)\\in \\varepsilon}(J^p-J^q)^2,\\varepsilon=\\{(R,G),(R,B),(G,B)\\}其中，$J^p$代表enhanced图像p channel的平均强度。 4. illumination smoothness loss保持相邻pixel的单调性。对每个curve参数图A有： L_{tv_\\mathcal{A}}=\\frac{1}{N}\\sum_{n=1}^N\\sum_{c \\in \\xi}(|\\nabla_x\\mathcal{A}_n^c|+|\\nabla_y\\mathcal{A}_n^c|)^2,\\xi=\\{R,G,B\\}N为迭代次数。求梯度的操作有点像TV loss。 total loss L_{total}=L_{spa}+L_{exp}+W_{col}L_{col}+W_{tv_A}L_{tv_A}$W_{col}=0.5,W_{tv\\mathcal{A}}=20$. ablation 训练数据：用multi-exposure的训练数据更好。 benchmark resultsno reference 结果（US/PI）视觉评判标准： 1) whether the results contain over-/under-exposed artifacts or over-/under enhanced regions; 2) whether the results introduce color deviation; 3) whether the results have unnatural texture and obvious noise. 指标： user study (US) score non-reference perceptual index (PI) [Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.] [Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:1–16, 2017.] .png) full reference 结果（PSNR/SSIM) face detection用SOTA的face detector(DSFD: https://github.com/Ir1d/DARKFACE_eval_tools ) 将不同enhance方法得到的图送入DSFD，得到P-R curve。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"}],"tags":[{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"}]},{"title":"ubuntu 踩坑记录","slug":"ubuntu踩坑记录","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T10:00:08.290Z","comments":true,"path":"2021/03/26/ubuntu踩坑记录/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","excerpt":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown","text":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown docker 里nvidia-smi不能用了，直接在docker外nvidia-smi也报错： NVIDIA-SMI couldn’t find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH. 估计是什么时候update弄成的。 解决方法：重装显卡驱动 1234567891011121314151617181920212223# BTW this is all in console mode (for me, alt+ctrl+F2)# login + password as usual# removing ALL nvidia software$ sudo apt-get purge nvidia* # Checking what's left:$ dpkg -l | grep nvidia# Then I deleted the ones that showed up (mostly libnvidia-* but also xserver-xorg-video-nvidia-xxx`)$ sudo apt-get purge libnvidia* xserver-xorg-video-nvidia-440 $ sudo apt autoremove # clean it up# now reinstall everything including nvidia-common$ sudo apt-get nvidia-common# find the right driver again$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt update$ ubuntu-drivers devices$ sudo apt-get install nvidia-driver-460 # the recommended one by ubuntu-drivers$ update-initramfs -u # needed to do this so rebooting wouldn't lose configuration I think$ sudo reboot 然后再重装NVIDIA-docker： 1234567$curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$sudo apt-get update$sudo apt-get install nvidia-docker2$sudo pkill -SIGHUP dockerd$docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 测试： 1sudo nvidia-docker run --rm nvidia/cuda:10.1-devel nvidia-smi 万幸CUDA, CuDNN都还有。 123456&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True&gt;&gt;&gt; a=torch.randn(1,2)&gt;&gt;&gt; a.cuda()tensor([[-0.4678, 0.1525]], device='cuda:0') 配置默认运行的是nvidia-docker 而不是 docker (https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json 文件中配置如下内容： 12345678910&#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [], &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;gemfield.mirror.aliyuncs.com&quot;] &#125; &#125;&#125; pycharm里用dockerpython 位置：/home/shiyuuuu/anaconda3/bin/python","categories":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"}]},{"title":"1-两数之和","slug":"1_两数之和","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:53:27.565Z","comments":true,"path":"2021/03/26/1_两数之和/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/","excerpt":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。","text":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6输出：[1,2] 示例 3 输入：nums = [3,3], target = 6输出：[0,1] 提示： 2 &lt;= nums.length &lt;= 103-109 &lt;= nums[i] &lt;= 109-109 &lt;= target &lt;= 109只会存在一个有效答案 暴力解答我自己的解答：非常暴力检索，第一个: i 从0到n，第二个: j 从i+1 到n（或者倒序来）。这样复杂度是O(n^2) 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): for j in range(i+1,len(nums)): if nums[i]+nums[j]==target: return [i,j] 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: # nums_sorted=sorted(nums) for i in range(len(nums)): for j in range(len(nums)-1,i,-1): if nums[i]+nums[j]==target: c=sorted([i,j]) return c 哈希表哈希表博文：哈希表 思路及算法注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。 使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。 这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 先建立一个空字典，查找target-num是不是hashtable的键值，如果是，直接return，如果不是，把这个num-i对以键值对的形式添加入字典。哈希表查找元素的复杂度为O(1) 代码 1234567891011121314151617181920from typing import Listclass Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i return []s=Solution()# nums=[2,7,11,15]# target=9# nums = [3,2,4]# target = 6nums = [3,3]target = 6a=s.twoSum(nums,target)print(a) 复杂度分析时间复杂度：O(N)，其中 N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。 空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]},{"title":"哈希表","slug":"哈希表","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:51:55.495Z","comments":true,"path":"2021/03/26/哈希表/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/","excerpt":"","text":"哈希表Hash Table，也叫散列表。力扣-两数之和","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]}],"categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"},{"name":"CVPR21","slug":"Paper-reading/CVPR21","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/CVPR21/"},{"name":"deblurring","slug":"Paper-reading/deblurring","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/deblurring/"},{"name":"工具","slug":"工具","permalink":"https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/"},{"name":"画图","slug":"工具/画图","permalink":"https://shiyuuuu.github.io/categories/%E5%B7%A5%E5%85%B7/%E7%94%BB%E5%9B%BE/"},{"name":"denoising","slug":"Paper-reading/denoising","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/denoising/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"zero-shot","slug":"zero-shot","permalink":"https://shiyuuuu.github.io/tags/zero-shot/"},{"name":"self-supervised","slug":"self-supervised","permalink":"https://shiyuuuu.github.io/tags/self-supervised/"},{"name":"To Do","slug":"To-Do","permalink":"https://shiyuuuu.github.io/tags/To-Do/"},{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"},{"name":"deblurring","slug":"deblurring","permalink":"https://shiyuuuu.github.io/tags/deblurring/"},{"name":"unsupervised","slug":"unsupervised","permalink":"https://shiyuuuu.github.io/tags/unsupervised/"},{"name":"工具","slug":"工具","permalink":"https://shiyuuuu.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"画图","slug":"画图","permalink":"https://shiyuuuu.github.io/tags/%E7%94%BB%E5%9B%BE/"},{"name":"denoising","slug":"denoising","permalink":"https://shiyuuuu.github.io/tags/denoising/"},{"name":"blind","slug":"blind","permalink":"https://shiyuuuu.github.io/tags/blind/"},{"name":"pseudo-supervision","slug":"pseudo-supervision","permalink":"https://shiyuuuu.github.io/tags/pseudo-supervision/"},{"name":"segmentation","slug":"segmentation","permalink":"https://shiyuuuu.github.io/tags/segmentation/"},{"name":"LV+HV","slug":"LV-HV","permalink":"https://shiyuuuu.github.io/tags/LV-HV/"},{"name":"audio+image","slug":"audio-image","permalink":"https://shiyuuuu.github.io/tags/audio-image/"},{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"},{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"},{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]}