{"meta":{"title":"Shiyu's Blog","subtitle":"Learn to live.","description":null,"author":"Shiyu","url":"https://shiyuuuu.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2021-03-26T09:37:09.711Z","updated":"2020-06-16T14:38:42.000Z","comments":true,"path":"404.html","permalink":"https://shiyuuuu.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2021-03-26T09:37:09.712Z","updated":"2020-06-16T14:25:06.000Z","comments":true,"path":"about/index.html","permalink":"https://shiyuuuu.github.io/about/index.html","excerpt":"","text":"一枚CS小学生。"},{"title":"所有分类","date":"2021-03-26T09:37:09.714Z","updated":"2020-06-16T14:35:18.000Z","comments":true,"path":"categories/index.html","permalink":"https://shiyuuuu.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-03-26T09:37:09.716Z","updated":"2020-06-16T14:40:28.000Z","comments":true,"path":"tags/index.html","permalink":"https://shiyuuuu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"阅读论文-Unpaired Image Super-Resolution using Pseudo-Supervision","slug":"Unpaired Image Super-Resolution using Pseudo-Supervision","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T08:26:08.506Z","comments":true,"path":"2021/04/09/Unpaired Image Super-Resolution using Pseudo-Supervision/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Unpaired%20Image%20Super-Resolution%20using%20Pseudo-Supervision/","excerpt":"出处：CVPR2020","text":"出处：CVPR2020 paper supplemental motivation unpaired super-resolution，当aligned 的HR-LR training set is unavailable. deviation between the generated LR distribution and the true LR distribution causes train-test discrepancy contribution（author: propose a new training method that overcomes the shortcomings of the existing GAN based unpaired super-resolution methods: generated LR distribution and the true LR distribution causes train-test discrepancy） bridge the gap between the well-studied existing SR methods and the real-world SR problem without paired datasets. Because our correction network is trained on not only the generated LR images but also the true LR images through the bi-directional structure (因为我们的校正网络不仅通过生成的LR图像进行训练，而且还通过双向结构对真实的LR图像进行训练) : minimize the train-test discrepancy any existing SR networks and pixel-wise loss function can be integrated because the SR network is separated to be able to learn in a paired manner. (SR网络可以pair对的学习) 包含unpaired kernel/noise correction network和pseudo-paired SR network unpaired kernel/noise correction network: 去除噪声、调整输入图像的kernel，从输入的HR图像生成pseudo-clean的LR图像 pseudo paired SR network: 学习pseudo-clean的LR image到输入的HR image的映射 SR网络独立于效验网络(correction network) related works paired SR： VDSR, EDSR, RCAN，LapSRN，DBPN blind SR [39,12,57] 由任意的kernel降质得到的LR，学习由这样的LR到HR的映射，但当真实图像不是以假设的degradation降质的，（degradation估计不准），就会让真实图像SR的任务很差。 ZSSR， IKC， 关于blind SR的研究很少涉及blur kernels以外的综合降质问题（比如noise，compression artifact)。 GAN based methods [51,4,56, 32] 可以直接学习LR 到HR的映射，不需要degradation的假设。 可以大致分为两类：一类是直接从LR image出发，在生成的HR image和真实的HR image之间加discriminator. 这种方式的确定是无法用pixel wise的loss，因为real HR是未知的。 另一类是，在HR到LR的过程加GAN，生成的LR image和真实的LR image之间加discriminator，使生成的LR尽可能逼近真实的LR image。然后生成的LR与原来的HR之间用pixel-wise的loss训练一个LR2HR的网络，也就是U。与cycleGAN的区别：HR端没有discriminator。缺点：生成的LR分布与真实的LR分布存在偏差，导致在training set和test set性能差别大。（当test set的图像分布在training set中完全没有） ICCV 2017: DualGAN: Unsupervised Dual Learning for Image-to-Image Translation ICCV 2017: CycleGAN ECCV 2018：To learn image super-resolution, use a gan to learn how to do image degradation first ICCVW 2019：Unsupervised learning for real-world super-resolution 以上两个第一次训练HR2LR的网络，并用degraded的输出训练LR2HR的网络。 CVPRW 2018：Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks 提出cycle-in-cycle network，但他们的degradation网络是确定的，并且SR网络与bi-cycle网络合在一起。选择loss function的时候有局限性 input LR $x$， GT HR $z$， $z$ bicubic下采样得到$y$ (clean LR) 先看里面的LR2clean LR，$x\\sim x’ $ (半个cycleGAN) 生成得到的clean LR 进SR网络，与GT送入判别器$D_2$，通过$G_3$再回到real LR space，$x \\sim x’’$ 【pixel-wise loss不能用在HR space】 arxiv 2018：Unsupervised degradation learning for single image super-resolution 利用了双向的结构，他们也在选择loss function的时候有局限性 以上4种，ECCV 2018人脸的和arxiv 2018的这篇基本基于cycleGAN的结构。 这篇文章和以上这些文章最主要的区别是，解决了在训练数据集和测试数据集分布不一致的问题。也就是pseudo-clean LR 和 real LR 通过硬件和数据对齐的操作建立 real SR的数据集： ICCV 2019：Toward real-world single image super-resolution: A new benchmark and a new model CVPR 2019：Camera lens super-resolution CVPR 2019：Zoom to learn, learn to zoom method解决其他GAN-based unpaired SR的缺点：separating the entire network into an unpaired kernel/noise correction network and a pseudo-pairedSR network correction network：是一个cycleGAN， 完成的是unpaired的real LR和 clean LR之间的translation。clean LR由HR经过predetermined operation得到。 SR network：成对的学习pseudo-clean LR到HR mapping 在训练阶段，correction network也先由clean LR到true LR再回到clean LR生成pseudo-clean的LR 图像。SR network成对的学习pseudo-clean LR image到HR的mapping。 学习印射$F_{XY}:(X)LR-&gt;(Y)HR$, 定义 clean LR：$Y_\\downarrow$是由$Y$经过一个指定的下采样操作得到的：$Y\\rightarrow Y_\\downarrow$是bicubic 下采样和gaussian blur的组合得到的。本文将$F_{XY}$拆分为两个mapping$G_{XY_\\downarrow}$和$U_{Y_\\downarrow Y}$的组合。 domain transfer in LR，其中学习mapping$G_{XY_\\downarrow}$是通过上图蓝框中cycleGAN的结构 mapping from LR to HR，只看绿线部分，由HR domain出发，先经过bicubic+Gaussian的下采样得到clean LR $Y_\\downarrow$, 再把它依次过cycleGAN的两个generator得到pseudo-clean LR $\\mathring{y_{\\downarrow}}$ , pseudo-clean LR到HR的mapping为$U_{Y_\\downarrow Y}$. 而$\\mathring{y_{\\downarrow}}$和y是成pair的，所以经过$U_{Y_\\downarrow Y}$上采样得到的$U_{Y_\\downarrow Y}(\\mathring{y_{\\downarrow}})$与y 之间可以用任意的pixel-wise的loss。 HR discriminator, 希望减小训练和测试的偏差，尽管$\\mathring{y_{\\downarrow}}$用来训练SR网络，但是实际应用的时候，输入的LR image是$G_{XY_\\downarrow}(x)$。所以pseudo clean LR和由real LR生成的clean LR的超分辨率后的差异尽可能小，所以最后还在这二者过$U_{Y_\\downarrow Y}$的输出上加判别器$D_{X_\\uparrow}$ test phase, 黑色实线部分，由real LR image先印射到 clean LR image $G_{XY_\\downarrow}(x)$，（cycleGAN训练到比较理想情况的时候, clean LR 与由real LR生成的clean LR 以及pseudo clean LR都很接近） loss function对于两个生成器和3个判别器，总共的loss： \\mathcal{L}_{trans}=\\mathcal{L}_{adv}(G_{XY_\\downarrow}, D_{Y_\\downarrow}, X, Y_\\downarrow)+\\mathcal{L}_{adv}(G_{Y_\\downarrow X}, D_{X}, Y_\\downarrow, X)\\\\ +\\gamma\\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)\\\\ +\\lambda_{cyc}\\mathcal{L}_{cyc}(G_{Y_\\downarrow X},G_{XY_\\downarrow})+\\lambda_{idt}\\mathcal{L}_{idt}(G_{XY_\\downarrow})+\\lambda_{geo}\\mathcal{L}_{geo}(G_{XY_\\downarrow})其中，$\\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)$是HR discriminator的loss: \\mathcal{L}_{adv}((G_{XY_\\downarrow},G_{Y_\\downarrow X}),D_{X_\\uparrow}, Y_\\downarrow,X_\\uparrow)\\\\ =\\mathbb{E}_{x\\sim P_x}[logD_{X_\\uparrow}(U_{Y_\\downarrow Y}\\circ G_{XY_\\downarrow}(x))]\\\\ +\\mathbb{E}_{y_\\downarrow \\sim P_{Y_\\downarrow}}[log(1-D_{X_\\uparrow}(U_{Y_\\downarrow Y}(\\mathring{y_\\downarrow})))]cycle consistency loss 被放松到只有单向的： \\mathcal{L}_{cyc}=||G_{XY_\\downarrow}\\circ G_{Y_\\downarrow X}(y_\\downarrow)-y_\\downarrow||_1这使$G_{Y_\\downarrow X}$可以一对多，满足不同的噪声和LR图像的分布。 identity loss在cycleGAN里用来保持图像的色彩，本文中也用了identity loss来避免色彩偏差: \\mathcal{L}_{idt}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(y_\\downarrow)-y_\\downarrow||_1geometric ensemble loss [CVPR 2019: Geometry consistent generative adversarial networks for one-sided unsupervised domain mapping] 减少可能的translation来保持场景的几何形状。本文中的geometric ensemble loss用来保证输入图像翻转、旋转不改变结果。 \\mathcal{L}_{geo}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(x)-\\sum_{i=1}^{8}T_i^{-1}(G_{XY_\\downarrow}(T_i(x)))/8||_1共带有8中翻转旋转模式。 而SR网络$U_{Y_\\downarrow Y}$是与生成器、判别器无关的网络，只是用来放大图像的局部特征来作为HR端判别器的输入。用下式来更新SR网络: \\mathcal{L}_{rec}=||U_{Y_\\downarrow Y(\\mathring y_{\\downarrow})}-y||_1这里的$\\mathcal{L}_{rec}$可以由任意的pixel wise的loss代替。（perceptual loss, texture loss, adversarial loss) network architecture最上面那路的$G_{XY_\\downarrow}$和 $U_{Y_\\downarrow Y}$用RCAN的网络 而$G_{Y_\\downarrow X}$的网络结构：resBlock+fusion layers+BN+Leaky ReLU 判别器：patchGAN，LR的判别器的stride=1,5层卷积, HR的判别器前面几层stride=2. Experimentssynthetic distortionsDIV2K realistic-wild dataset (800 训练图像) simulate ： 4倍下采样、运动模糊、pixel shifting、加性噪声 每张图只有一种degradation，但是图像与图像之间的degradation不同，对于每张训练图像，合成4张降质图像。 训练：800 HR+3200 LR（unpair） 测试：100张validation set 超参：$\\lambda_{cyc}=1, \\lambda_{idt}=1,\\gamma=0.1$, 4倍SR intermediate images compare with blind methods blind denoising: NC [The noise clinic: a blind image denoising algorithm], RL-restore [ CVPR 2018: Crafting a toolchain for image restoration by deep reinforcement learning] 【blind denoising还有CVPR 2019: Toward Convolutional Blind Denoising of Real Photographs (Kai Zhang)】 blind deblurring: SRN-Deblur[CVPR 2018: Scale-recurrent network for deep image deblurring], DeblurGAN-v2 [Arxiv2019：Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better] SR: DBPN(non-blind), ZSSR, IKC (blind)（zssr: CVPR2018, IKC: cvpr 2019) ZSSR+KernelGAN 这些方法是用的各自论文里提及的数据集训练，没有在这里的数据集上训练。 compare with NTIRE 2018 baselines 这些baseline是pair-trained （upper bounds) 这篇论文的方法PSNR比不过 baseline方法，但是SSIM与baseline方法相当. 因为PSNR高估了整体的亮度和色彩的细微差别，这些差别不会显著影响perceptual quality。 ablation study 第2行：去除HR的判别器 第3行：SR network是在$y_\\downarrow$上训练的，而不是$\\mathring{y_\\downarrow}$上，这就相当于一个real LR和Gaussian+bicubic LR之间translation的网络(cycle GAN)加一个SR网络。 第4行：SR network是在$G_{Y_\\downarrow X}(y_\\downarrow)$ 上训练的，而不是$\\mathring{y_\\downarrow}$上. 相当于图1 的b 第5行：在第4行基础上，用RCAN官方的模型做validation。 perception-oriented training 在HR的$\\mathcal{L}_{rec}$里加上perceptual loss, content loss (ESRGAN里面的)，relativistic adversarial loss. 加上这些loss 后视觉效果比L1 loss好 realistic distortion 1follow unsupervised 人脸SR [ECCV 2018: To learn image super-resolution, use a gan to learn how to do image degradation ﬁrst] HR face images: Celeb-A, AFLW, LS3D-W, VGGFace2 (64*64) LR face images: 50000张Widerface 包含多种degradation（留出3000作为测试）(16*16) \\mathcal{L}_{\\bar{idt}}(G_{XY_\\downarrow})=||G_{XY_\\downarrow}(x)-x||_1他们实验发现identity loss加在x上比加在$y_\\downarrow$ 上好。 超参：$\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=2,\\lambda_{geo}=1,\\gamma=0.1$ 训练2倍SR：32*32=&gt;64*64 视觉指标FID比较：高亮的为基于GAN的unpaired的方法。 one-to-many degradation realistic distortion 2aerial image dataset DOTA GSD (ground sample distances), 62 LR images GSD在[55cm, 65cm]之间， HR image的GSD为30cm 超参：$\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=10,\\lambda_{geo}=100,\\gamma=0.1$ , 2倍SR 在这种数据集里，物体的像素点很少，所以对identity loss和geometric loss用了更大的权重。在训练初期，逐步提高geometric loss的权重。 只提供视觉上的比较，因为没有GT。 先用RL-restore（强化学习blind去噪修复）在input的LR image上去噪。但是他的输出over-smoothed. 即使再用SOTA的blind SR方法ZSSR超分辨，artifacts也不能被完全移除。 geometric loss的作用： AIM 2019 real world SR challenge没有HR-LR pair,测试时有官方的脚步来计算PSNR/SSIM。 $\\lambda_{cyc}=1,\\lambda_{\\bar{idt}}=5,\\lambda_{geo}=1,\\gamma=0.1$ ​ LPIPS: 视觉指标，越低越好。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"pseudo-supervision","slug":"pseudo-supervision","permalink":"https://shiyuuuu.github.io/tags/pseudo-supervision/"},{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"}]},{"title":"阅读论文-Dual super-resolution learning for semantic segmentation","slug":"Dual super-resolution learning for semantic segmentation","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T08:49:26.193Z","comments":true,"path":"2021/04/09/Dual super-resolution learning for semantic segmentation/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Dual%20super-resolution%20learning%20for%20semantic%20segmentation/","excerpt":"出处：CVPR2020 (oral)","text":"出处：CVPR2020 (oral) paper code unofficial-code motivation在不增加计算开销的前提下提高语义分割的性能，而语义分割依赖于HR feature representations，如果直接输入HR images会产生很大的计算开销，所以做SSSR。而只依赖于SSSR的decoder难以恢复original details，所以他们用了SISR来guide SSSR，如何guide呢？用feature affinity module，核心是feature相似矩阵的距离。 related works现有的语义分割方法能取得好的性能依赖于HR的深度特征表示：large computation budgets 现有的语义分割方法保持HR representations: 用空洞卷积代替 strided 卷积，比如DeepLabs 结合HR的pathway，比如Unet 这些方法的输入通常是original HR image，当限制输入图像的大小的时候，他们的性能有明显的下降。（现有语义分割方法：FCN, DeepLabs, PSPNet, 空洞卷积，pyramid pooling module, attention， context encoding） 现有的轻量级的语音分割：通过factorization加速卷积，ESPNets（split-merge，reduce-expand加速卷积计算），采用一些有效的分类网络（mobileNet、shuffleNet），知识蒸馏帮助训练对抗网络。但他们的性能比SOTA差很多。 本文提出一种two-stream的framework (dual super-resolution learning, DSRL) 在不产生额外计算开销的情况下提高semantic 分割的准确率: 对于LR的输入保持HR representations。具体的，SISR得到的HR features用来guide spatial维度的相关性学习。DSRL可以在相同的resolution下，显著提高准确率。 现有的SISR方法： pre-upsampling SR：先通过bicubic上采样得到HR图像，再用网络refine HR图像（计算开销大：网络在HR上做的） post-upsampling SR：在网络的最后面用可学习的上采样层 progressive SR：逐渐提高分辨率，可以handle multi-scale的SR（deep laplacian pyramid networks） iterative up-and-down SR：通过iterative上\\下采样的层得到中间图像，结合中间图像重建最终图像(deep back-projection networks) multi tasks： mask R-cnn（检测+实例分割） RCNN（姿态估计+动作识别） cross tasks： （希望把语义分割作为主要任务，SISR作为附加任务） proposed methodReview of encoder-decoder framework用来提取特征的encoder的scaling step是2，OS通常是8或者16（the ratio of input image spatial resolution to the Encoder output resolution），把最后两层strided conv换成空洞卷积。在decoder端，用一个bilinear 上采样层恢复分辨率。 现有的方法只能将feature上采样至与input image同样的大小，可能比original image小。（分割网络的输入往往是对原图做了下采样）。这样可能损失了一部分有用的label信息，另一方面，也难以只依赖decoder恢复original details。 DSRLcontribution： 在不额外增加计算开销的前提下，通过保持HR representations，提高性能 泛化性：可以扩展到需要HR representation的任务中，比如人体姿态估计 实验：在语音分割和人体姿态估计任务上都获得了良好的性能，相同计算开销，提高2% 包含三个部分： semantic segmentation super-resolution (SSSR) single image super-resolution (SISR) feature affinity (FA) 其中SISR与SSSR共享特征提取器， SSSR：加额外的上采样层（一些deconv）来得到最终的prediction mask，比如输入的是5121024，输出1024\\2048. 他们的方法可以利用全部的label SISR：只依赖decoder不足够恢复HR semantic特征表示，因为decoder的上采样结构不是简单的bilinear上采样，就是简单的sub-net，这并不会引入额外的信息，因为输入是LR的。 而SISR可以有效恢复图像细节，SSSR和SISR的feature如下图所示，SISR的feature包含更多的物体更多的复杂结构，尽管这些结构不能直接揭示物体属于哪一类，但他们可以根据像素与像素、区域与区域直接的相关性group起来，而这些像素区域的关联揭示了语义信息。 所以，SISR得到的HR feature用来guide SSSR的HR feature的学习。SISR根据original image的GT来优化。 FA: feature affinity SISR得到的structure information如何guide SSSR？用 feature affinity learning，FA来学习SSSR和SISR feature上相似矩阵的距离，相似矩阵刻画的是像素直接pairwise的关系。理论上，应该计算所有像素对的affinity，为了节省计算开销，他们subsamples得到1/8的像素对。为了减少由SISR和SSSR不一致引起的训练的不稳定性，他们还附加了一个feature transform模块。 S_{ij}=(\\frac{F_i}{||F_i||_p})^{T}(\\frac{F_j}{||F_j||_p})最后的FA loss为： L_{fa}=\\frac{1}{W^2H^2}\\sum_{i=1}^{W'H'}\\sum_{j=1}^{W'H'}||S_{ij}^{seg}-S_{ij}^{sr}||_q$p=2, q=1$，总的loss： L=L_{ce}+w_1L_{mse}+w_2L_{fa},\\\\其中， L_{ce}=\\frac{1}{N}\\sum_{i=1}^{N}-y_ilog(p_i),\\\\ L_{mse}=\\frac{1}{N}\\sum_{i=1}^{N}||SISR(X_i)-Y_i||^2$L_{ce}$ 为cross entropy loss, $w_1=0.1, w_2=1.0$ experimentssemantic segmentation：CitySpace数据集，将一张图分为19类。10242048；CamVid数据集，11类，960\\720。 metric：mIoU（mean Intersection over Union） segmentation architecture: ESPNetv2, DeepLabv3+(ablation study), PSPNet, BiseNet, DABNet (lightweight) effect of components输入：256512(resize, 1024\\2048=&gt;256*512) 输出：+SR的输出是5121024（2倍），不加SR的输出：256\\512 +SSSR+SISR+FA &gt; +SSSR+SISR &gt; SSSR &gt; 不加 原图下采样作为baseline和自己方法的输入，输出分辨率不同 【输出分辨率不同怎么比的？】 把原图的label分别下采样到256*512和512*1024比 output统统上采样到原图的分辨率（1024*2048）和label比。（保持同分辨率下比较，并且不对原始的label降质） 第2 种比较相当于比的是bicubic+seg和SR+seg effect of various input resolutionsinput resolution：256*512, 320*640, 384*768, 448*896, 512*1024 (CitySpace) 对于每一种input resolution，用他们的framework都比不用性能好，且随着input resolution逐渐增大，性能增值越来越小。 human pose estimationmetric：object keypoint similarity (OKS) architecture: HRNet-w32, 用offline person detection的结果预测关键点 输入human detection box (缩放到固定大小：256192， 162\\128, 128*96) 输出heatmap（64*48） resultsvisualization of segmentation features 对于structured objection提升尤其明显，比如人，车","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"segmentation","slug":"segmentation","permalink":"https://shiyuuuu.github.io/tags/segmentation/"},{"name":"LV+HV","slug":"LV-HV","permalink":"https://shiyuuuu.github.io/tags/LV-HV/"}]},{"title":"阅读论文-Learning to Have an Ear for Face Super-Resolution","slug":"Learning to Have an Ear for Face Super-Resolution","date":"2021-04-08T16:00:00.000Z","updated":"2021-04-09T09:29:10.045Z","comments":true,"path":"2021/04/09/Learning to Have an Ear for Face Super-Resolution/","link":"","permalink":"https://shiyuuuu.github.io/2021/04/09/Learning%20to%20Have%20an%20Ear%20for%20Face%20Super-Resolution/","excerpt":"出处：CVPR2020 (oral)","text":"出处：CVPR2020 (oral) paper code project task: 用audio和LR图像做16倍的人脸超分辨率，输入的LR图像非常小：8*8 pixels, 这些图像的很多重要细节都被丢失了。如果LR的人脸图像是从视频中提取的，那么我们也可以得到这个人的音频信息，而audio中带有一些脸部的特征：性别和年龄。结合听觉和视觉，他们提出了一个：先从单独的音轨构建脸部特征的latent 表示，再从LR图像简历脸部特征潜在的表示。然后再fusion这两个表示。 motivation limited information ambiguous mapping =&gt; incorporate alternative source of information: audio audio carries information about age and gender, audio tracks are available in videos, audio and visual signals both capture some shared attributes of a person. contribution the first attempt to use audio for image restoration use both audio and a LR image to perform extreme face super-resolution ($16\\times$) do not use human annotation and thus can be easily trained with video datasets 建立了图像和音频的分解表示，因此它可以混合来自不同视频的LR图像和音频，并生成语义上有意义的真实面孔。（our model builds a factorized representation of images and audio as it allows one to mix low-resolution images and audio from different videos and to generate realistic faces with semantically meaningful combinations. ） related worksspeech2Faceaudio to image: speech2Face [CVPR 2019: Speech2face: Learning the face behind a voice] styleGAN给定512维随机向量， styleGAN可以从这个向量重构维一张从未见过的人脸照片。 如果训练一个model，是从图片到512维latent encoding, 这个latent encoding可以通过styleGAN还原为原图。（图=&gt;512维latent code=&gt;原图 ） 也就是说，可以用这512维向量生成原图。 如果对这512维的空间稍微做点改变？这个超高维的空间对应人脸的不同属性（肤色、年龄、性别） 如何知道年龄对应的维度是哪些？ 1、作者一开始会训练一个分类器，分类器的训练样本、label来自于CelebA，40维label中就有一个维度是年龄（作者贴出来的代码里的wp.npy文件我猜就是CELEBA的label文件） 2、分类器收敛后，用个随机噪声z作为stylegan输入，生成图片x，再把这个x送入分类器，得到分类结果y，用个线性变换（或非线性也行）建立起z与y的关系。。。。然后你就可以通过控制y的变换方向来得到z，再生成想要的x了 总之，styleGAN可以从latent code生成逼真的人脸，更重要的是latent code对应人脸不同属性，可以通过改变latent code得到不同的人脸。（比如得到XX小时候的照片）。 Naive end-to-end training带有两个编码网络的多模态网络，再把encoder的输出concate再一起送入decoding网络得到HR图像。但这种多模态网络以传统的训练方法很难训练好，因为：不同模态的收敛速度不同。 实验发现，这样直接一起训练会忽略audio信号。audio信号需要更长的处理更强的网络来拟合其latent space。 method 包含这样几个部分：LR encoder $E_l$, audio encoder $E_a$，fusion network $F$, face generator$G$ 分开训练LR image encoder和audio encoder，这样他们的解耦精度就相等。fusion: audio作用在LR image固定的中间representation中，这样audio中的人脸属性可以解耦出来。 为什么styleGAN？styleGAN可以通过操控latent code的某些维度，改变生成人脸的某些属性。 Inverting the Generator 首先训练一个从高斯隐空间$z \\sim \\mathcal{N}(0,I_d)$（d维）开始的输出高分辨率图像的generator $G(z)$ 【styleGAN: 人脸生成，可以控制所生成图像的高层级属性：头发等，以高斯分布作为输入，输出高质量的samples和隐空间】，再，用自编码器的限制，通过固定$G(z)$得到generator(fixed) 反转后的encoder ($E_h$). 再将这两部分$G(z)$和$E_h$fine-tuning。这个encoder的映射是HR image到generator输入的隐空间，这个$E_h$可以得到图像$x_i$对应的representations ($z_i$)可以当做LR,audio，fusion network的encoder的目标，generator的输出是input HR的近似。【这个模型可以作为生成HR人脸图像的先验，并且中间的representations应该可以由audio编辑】 给定数据集$\\mathcal{D}=\\{(x_i^h,x_i^l,a_i)|i=1,…,n \\}$, \\mathcal{L}_{pre-train}=\\sum_{i=1}^{n}|G(z_i)-x_i^h|_1+\\lambda_f\\mathcal{l}_{feat}(G(z_i),x_i^h)其中$z_i=E_h(x_i^h)$ ，$l_{feat}$ 是perceptual loss (VGG feature) 他们实验发现只回归一个$z_i$不足以很好的恢复$x_i^h$，所以像styleGAN一样，把$z_i$非线性变换得到$w_i$，所以他们生成$k$个不同的$z_{ij},j=1,…,k$. 将非线性变换得到的$w_{ij}$分别插入到generator的不同层。 再fine-tuning： min_{E_h,G}\\mathcal{L}_{pre-train}+\\lambda_t|G_{init}-G|_2^2其中$G_{init}$是styleGAN训练后$G$的权重。训练过程中，总的loss最小后，将$\\lambda_t$减小为原来1/2，pre-training和减小正则化：让encoder和decoder逐渐收敛，不至于过早失去G的latent representation的结构 总结，(1). 从标准高斯分布学习G(z)（styleGAN），获得HR 图像的分布 (2). 固定$G(z)$, 用公式1用autoencoder的方式训练reference encoder$E_h$, (3). fine-tuning $G(z)$和$E_h$，$E_h$可以得到HR图像的latent representations. Pre-training Low-Res and Audio encoders 给定HR-LR pair，pre-train一个LR encoder，将输入的LR映射到与HR相同的的reference encoder（前面训练的）输出的latent representations 如果直接训练fusion network，使fusion model$F(x_i^l,a_i)$映射到$z_i=E_h(x_i^h)$，会使网络完全忽略音频信号$a_i$。所以他们先分开训练encoder$E_l$和$E_a$， 让他们尽可能从这两种模态多提取信息，再fusion他们。 为了防止过拟合，pre-train $E_l$和$E_a$时，只用一半数量的训练数据（记为$\\mathcal{D}_{pre}$），在fusion的训练阶段用整个训练数据。 训练$E_l$的目标函数： min_{E_l}\\sum_{x_i^l,x_i^h \\in \\mathcal{D_{pre}}}|E_l(x_i^l)-z_i|_1+\\lambda|D \\circ G(E_l(x_i^l))-x_i^l|_1其中$D \\circ x$是$x$的16倍下采样。$\\lambda=40$. 前面一项很好理解：希望$E_l$在LR图像$x_i^l$上得到的latent representation与HR图像上的latent representation($z_i=E_h(x_i^h)$)相似。第二项：LR图像$x_i^l$经过encoder(得到HR图像的latent representation)再经过decoder$G$（得到HR图像$x_i^h$）再16倍下采样得到LR 图像$x_i^l$ （因为$x_i^l$与$x_i^h$成pair） 而对于音频encoder：如果将$E_a(a_i)$回归到$z_i$必然有overfitting，因为一些$z_i$中有的属性，$a_i$中没有，比如脸部的姿态(朝左朝右？)。为了消除$z_i$中与$a_i$无关的属性，将$E_a(a_i)$的目标定义为： \\bar{z_i}=\\frac{1}{2}(E_h(x_i^h)+E_h(\\hat{x}_i^h))这里的 $\\hat{x}_i^h$是$x_i^h$的水平翻转版本（将原HR图像水平翻转）。 训练$E_a$的目标函数： min_{E_a}\\sum_{a_i,x_i^h \\in \\mathcal{D}_{pre}}|E_a(a_i)-\\bar{z_i}|_1【这里没懂】由于styleGAN的分层的结构，水平翻转的图片的latent code求平均，就消除了音频无法传递的信息。比如图里，输入朝左的面部图像和它水平翻转后的图像（这个水平翻转图像的面部朝向是朝右的），把encoder提取到的latent code求平均，再经过decoder，就得到了朝向正面的图像（消除了音频无法传递的面部朝向信息）。 小结： audio encoder, fusion network: 固定LR image encoder， 提升他的latent representation 。为了加快audio encoder的训练速度，将HR reference encoder的输出和其水平镜像的平均值作为latent representation 对audio encoder预训练。而这个平均消除了音频无法传递的信息，比如视点。 fusing audio and low-resolution encodings现在希望聚合pre-train得到的encoder$E_l$和$E_a$提取的信息。由于$E_l$已经是$E_h$的近似，那么希望引入的音频能补出residual：$\\Delta z_i=z_i-z_i^l$, 所以fusion network $F$应满足： z_i^f=E_l(x_i^l)+F(E_l(x_i^l),E_a(a_i))因为$E_a$更难训练，所以继续把$E_a$和$F$一起优化。所以，fusion network的优化目标是: min_{E_a,F}=\\sum_{a_i,x_i^h,x_i^l \\in \\mathcal{D}}|z_i^f-z_i|_1+\\lambda|D \\circ G(z_i^f)-x_i^l|_1 (a). matching inputs; (b) LR image与audio来自不同video 把LR image(8*8)输入到$E_l$得到latent representation, fusion network 融合$E_a$编码后的音轨和encoded LR image, 这一部分与前面的latent representation相加得到的新的latent representation 与预先训练得到的HR image的latent representation很相似，再通过decoder G输出HR图像 共有3个mappings： audio to HR (speech2face是用预训练的人脸识别网络作为额外监督，而我们的方法是完全无监督的) LR to HR LR+audio to HR Experimentsdataset： VoxCeleb2 Dataset, 包含145K 人说话的video 2M frames at 128*128 pixels. 将每个speaker的一半的数据放入$\\mathcal{D_{pre}}$ test set: 同人的不同video. audio to image简单的比较了一下他们的audio-only model ($E_a+G$) 和speech2Face 第一行是speech2face的结果，第二行是他们的audio2image的结果 ($E_a+G$) 性别分类：96%~97%准确率（没有在性别分类上与speech2face比，因为speech2face训练时用了分类器的监督） classification as a performance measure预训练好的 身份分类器、性别分类器、年龄分类器 closed set：training set和test set用同一个人的不同video open set：training set和 test set不同人 ablations (a), (b): 训练$E_h$后时候和$G$一起fine-tune 【(b): HR 图像的表现，upper bound】 (c), (d)：without fusion F。只要LR 或者只要音频 ​ c与d相比，Audio更能提供性别信息。$\\mathcal{C}_g$ (e)~(h): (f)(g)(h)相比：不加audio&lt;固定audio encoder&lt;fine-tuning ​ (e)与(h)相比：一个全连接层&lt;三个全连接层 ​ (g)与(h)相比：在训练fusion网络时也fine-tuning $E_a$ 对结果有些许提升。 gender和age在open set上也能预测比较准确。 comparisons to other SR methods LapSR(CVPR 2017) , W-SRNet (ICCV 2017 人脸SR) 在这个数据集上重新训练。 mixing给定LR，与不同的audio混合 failure cases","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"audio+image","slug":"audio-image","permalink":"https://shiyuuuu.github.io/tags/audio-image/"}]},{"title":"阅读论文-GLEAN Generative Latent Bank for Large-Factor Image Super-Resolution","slug":"阅读论文GLEAN","date":"2021-03-28T16:00:00.000Z","updated":"2021-04-09T07:54:01.237Z","comments":true,"path":"2021/03/29/阅读论文GLEAN/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/","excerpt":"出处：CVPR2021 (oral)","text":"出处：CVPR2021 (oral) project code paper 任务是： 大尺度超分辨率（8$\\times$ 到 64$\\times$），most details and textures are lost during downsampling. motivation 已有的SR方法： solely rely on $L_2$ loss: 视觉质量不好 (over-smoothing artifacts)。 adverssrial loss [ESRGAN]: Generator既要捕获图像characteristics又要保真（maintaining the fidelity to the GT）, 限制了其近似自然图像的能力，产生artifacts GAN inversion methods [PULSE]：反转pre-trained GAN的生成过程：把image mapping回latent space；再由latent space中optimal vector重建图像。只靠低维隐向量不足以指导重建的过程，使得产生的结果low fidelity. 需要image-specific, iterative的优化. 利用pre-trained GAN作为latent bank， 充分利用pre-trained GAN中封装的丰富且多样的先验。换用不同的bank可以不同类的图像：cat，building，human face，car. 利用字典学习的方式，测试阶段，只需要一次前传即可得到恢复后的图像。 GLEAN 的整体结构：encoder-bank-decoder related worklarge-factor SR fully probabilistic pixel recursive network for upsampling extremely coarse images with resolution 8×8. (Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.) RFB-ESRGAN：adopts multi-scale receptive fields blocks for 16× SR. (Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, and Yandong Guo. Perceptual extreme super resolution network with receptive field block. In CVPRW, 2020.) VarSR: 8× SR by matching the latent distributions of LR and HR images to recover the missing details. (Sangeek Hyun and Jae-Pil Heo. VarSR: Variational super-resolution network for very low resolution images. In ECCV, 2020.) perform 16× reference-based SR on paintings with a non-local matching module and a wavelet texture loss. (Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, and Yun Fu. Texture hallucination for large-scale painting super-resolution. In ECCV, 2020.) GAN inversion David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al. Semantic photo manipulation with a generative image prior. TOG, 2020. Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code GAN prior. In CVPR, 2020. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. 通过pixel-wise约束，迭代优化styleGAN的隐变量。 Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In ECCV, 2020. finetune generator和latent code来缩小训练集和测试集分布的gap. 降质图像$x$，latent space：$\\mathcal{Z}$： z^*=argmin_{z \\in \\mathcal{Z}}\\mathcal{L}(G(z), x)缺点： 低维的隐向量不能保持图像的spatial information. 方法passing both the latent vectors and multi-resolution convolutional features from the encoder. multi-resolution cues need to be passed from the bank to the decoder. 整个结构为encoder-bank-decoder encoder：$E_0$为RRDB-Net，$E_i, i \\in\\{1,…,N\\}$代表堆叠一个stride=2的conv和一个stride=1的conv. 最后由FC层得到$C$, $C$表示隐向量，提供high-level信息。为了更好的指导结构重建，将多分辨率的特征和隐向量都送入bank. Generative latent bank: 用pre-trained的Generator, styleGAN，提供纹理和细节生成的先验。 对generator的每个block输入不同的隐向量$C_i$, $i \\in \\{0,…,k-1\\}$ $\\{g_i\\}$代表每个block输出的feature, 它是由$C_i, g_{i-1}, f_{N-i}$由augmented style block得到。 不直接输出结果，而是将特征$\\{g_i\\}$输入到decoder 优势：像reference-based SR，HR reference image作为显式图像字典。性能很受 输入和reference相似度的影响。GLEAN用GAN-based的字典，不依赖于任何具体的图像，它获取的是图像的分布。而且没有global matching和reference images selection， 计算简便。 decoder：progressive地聚合来自encoder和latent bank的特征。每个conv后跟着pixel-shuffle层。由于有encoder和decoder之间的skip-connection，encoder捕获的信息可以被强化，bank专注于纹理和细节的生成。 训练：$\\mathcal{l_2}$ loss, perceptual loss, adversarial loss. 训练时fix住latent bank，实验发现，finetune latent bank没有性能提升，而且可能使latent bank偏向训练集的分布。 主要结果 image retouching 参考文献：[ESRGAN]: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. ESRGAN: Enhanced super-resolution generative adversarial networks. In ECCVW, 2018. [PULSE]: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. reference-based SR Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restoration via deep multi-scale component dictionaries. In ECCV, 2020. Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020. Xu Yan, Weibing Zhao, Kun Yuan, Ruimao Zhang, Zhen Li, and Shuguang Cui. Towards content-independent multi-reference super-resolution: Adaptive pattern matching and feature aggregation. In ECCV, 2020. Yang Zhang, Ivor W Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, and Xin Yu. Copy and Paste GAN: Face hallucination from shaded thumbnails. In CVPR, 2020. Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. In CVPR, 2019.","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"}]},{"title":"阅读论文-Zero-Reference deep curve estimation for low-light image enhancement","slug":"Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement","date":"2021-03-28T16:00:00.000Z","updated":"2021-03-29T13:08:40.347Z","comments":true,"path":"2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","excerpt":"出处：CVPR2020","text":"出处：CVPR2020 paper PDF supplemental materials project: https://li-chongyi.github.io/Proj_Zero-DCE.html code: https://github.com/Li-Chongyi/Zero-DCE motivation图像编辑软件通过调节曲线来增强图像=&gt;image-specific 曲线估计： 根据给定图像估计pixel-wise的调整曲线。不需要任何成对或不成对的训练数据（不需要reference/GT) 图像增强=&gt;非线性曲线映射 而不是通过image-to-image mapping 方法： non-reference loss functions contribution no reference: 避免了需要paired/unpaired数据的方法中的overfitting的问题 设计image-specific曲线：高次、pixel-wise 提升人脸识别的性能 related work传统方法调整图像的直方图分布调整，增大图像的动态范围。 global level: [1]. Dinu Coltuc, Philippe Bolon, and Jean-Marc Chassery. Exact histogram specification. IEEE Transactions on Image Processing, 15(5):1143–1152, 2006. [2]. Haidi Ibrahim and Nicholas Sia Pik Kong. Brightness preserving dynamic histogram equalization for image contrast enhancement. IEEE Transactions on Consumer Electronics, 53(4):1752–1758, 2007. local level: [3]. Chulwoo Lee, Chul Lee, and Chang-Su Kim. Contrast enhancement based on layered difference representation of 2d histograms. IEEE Transactions on Image Processing, 22(12):5372–5384, 2013. [4]. J Alex Stark. Adaptive image contrast enhancement using generalizations of histogram equalization. IEEE Transactions on Image Processing, 9(5):889–896, 2000. Retinex theory (将图像分解为reflectance和illumination，其中reflectance分量在任何光照条件下保持一致，图像质量增强任务变为illumination estimation问题): [5]. Edwin H Land. The retinex theory of color vision. Scientific American, 237(6):108–128, 1977. 自然的信息保存方法： [6]. Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Processing, 22(9):3538–3548, 2013. weighted vatiation: [7]. Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016. coarse illumination map: 搜索RGB中最大intensity的pixel [8]. Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, 2017. 考虑噪声： [9]. Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, 2018 自动exposure校正方法： 通过全局优化方法估计图像的S形曲线。 [10]. Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV, 2012. Data-driven方法CNN-based一般需要pair对的数据：资源密集型(resource-intensive)。一般这种成对的数据通过 自动光照退化、改变相机的设置 采集 或者用image retouching合成 LOL数据集[Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018.] 通过改变曝光时间和ISO获取成对的low/normal光照的图像。 MIT-adobe FiveK数据集[Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo ´ Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR, 2011.]包括5000raw图，每一张raw图由专家生成5个retouched图像。 [11]提出了一种估计illumination的方法：poor generalization capability [11]. Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019. GAN-basedEnlightenGAN[Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. In CVPR, 2019.] 用unpair的low/normal光照的数据。需要仔细挑选unpaired的训练数据。 methods 通过non-reference loss function实现不需要paired/unpaired的数据。通过迭代训练得到结果。 Light-enhancement curve自适应的曲线参数只由输入图像决定。曲线要是单调的来保持周围像素的区别。曲线要处处可微保证可以梯度反传。 LE(I(x);\\alpha)=I(x)+\\alpha I(x)(1-I(x))其中$\\alpha \\in [-1,1]$, 将LE曲线分别作用于R\\G\\B通道，而不仅仅作用于illumination通道。调节3个通道可以更好的保持固有的色彩，减少过饱和。 上式可以多次迭代，写为： LE_n (x)=LE_{n-1}(x)+\\alpha_nLE_{n-1}(x)(1-LE_{n-1}(x))其中n为迭代次数，他们设为8. 多次迭代有更强的调节能力。 但是如果$\\alpha$在每一个点是固定值，只能是全局调节。全局调节容易带来over-/under- enhance local区域。为了解决这个问题，他们将$\\alpha$设置为pixel-wise的参数。上式变为： LE_n (x)=LE_{n-1}(x)+A_n(x)LE_{n-1}(x)(1-LE_{n-1}(x))其中A是一个与原图等大小的parameter map。 假设，一个局部区域的像素点有相同的intensity. 所以输出图像里的相邻像素也有单调的关系。 DCE-Net（具体结构见补充材料） 输入：低光照图像 输出：pixel-wise的高阶curve参数图。8次迭代共24个parameter maps 网络细节：不包括down-sampling和BN，它们会丢失相邻像素的关系。最后一层conv接Tanh 1. spatial consistency loss使增强后的图像保持时域的连贯性(coherence) L_{spa}=\\frac{1}{K}\\sum_{i=1}^K\\sum_{j \\in \\Omega(i)}(|(Y_i-Y_j|-|I_i-I_j|)^2$K$为局部区域数，$\\Omega(i)$为4个相邻区域（上下左右）。Y和I是局部区域的平均强度值(intensity)。局部区域的大小设为4*4. 相当于对K个4*4的局部区域，要求enhance前后图的局部区域与其邻域的强度差，差不多。 2. exposure control loss控制曝光的程度。测量局部区域的平均强度与well-exposedness的级别E之间的距离。E=0.6、在[0.4,0.7]之间差不多。 L_{exp}=\\frac{1}{M}\\sum_{k=1}^M|Y_k-E|M代表M个16*16的没有overlap的局部区域，Y是enhanced图像中局部的平均强度值。 3. color constancy loss校正色彩偏差。 L_{loc}=\\sum_{\\forall(p,q)\\in \\varepsilon}(J^p-J^q)^2,\\varepsilon=\\{(R,G),(R,B),(G,B)\\}其中，$J^p$代表enhanced图像p channel的平均强度。 4. illumination smoothness loss保持相邻pixel的单调性。对每个curve参数图A有： L_{tv_\\mathcal{A}}=\\frac{1}{N}\\sum_{n=1}^N\\sum_{c \\in \\xi}(|\\nabla_x\\mathcal{A}_n^c|+|\\nabla_y\\mathcal{A}_n^c|)^2,\\xi=\\{R,G,B\\}N为迭代次数。求梯度的操作有点像TV loss。 total loss L_{total}=L_{spa}+L_{exp}+W_{col}L_{col}+W_{tv_A}L_{tv_A}$W_{col}=0.5,W_{tv\\mathcal{A}}=20$. ablation 训练数据：用multi-exposure的训练数据更好。 benchmark resultsno reference 结果（US/PI）视觉评判标准： 1) whether the results contain over-/under-exposed artifacts or over-/under enhanced regions; 2) whether the results introduce color deviation; 3) whether the results have unnatural texture and obvious noise. 指标： user study (US) score non-reference perceptual index (PI) [Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.] [Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:1–16, 2017.] .png) full reference 结果（PSNR/SSIM) face detection用SOTA的face detector(DSFD: https://github.com/Ir1d/DARKFACE_eval_tools ) 将不同enhance方法得到的图送入DSFD，得到P-R curve。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"}],"tags":[{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"}]},{"title":"ubuntu 踩坑记录","slug":"ubuntu踩坑记录","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T10:00:08.290Z","comments":true,"path":"2021/03/26/ubuntu踩坑记录/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","excerpt":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown","text":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown docker 里nvidia-smi不能用了，直接在docker外nvidia-smi也报错： NVIDIA-SMI couldn’t find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH. 估计是什么时候update弄成的。 解决方法：重装显卡驱动 1234567891011121314151617181920212223# BTW this is all in console mode (for me, alt+ctrl+F2)# login + password as usual# removing ALL nvidia software$ sudo apt-get purge nvidia* # Checking what's left:$ dpkg -l | grep nvidia# Then I deleted the ones that showed up (mostly libnvidia-* but also xserver-xorg-video-nvidia-xxx`)$ sudo apt-get purge libnvidia* xserver-xorg-video-nvidia-440 $ sudo apt autoremove # clean it up# now reinstall everything including nvidia-common$ sudo apt-get nvidia-common# find the right driver again$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt update$ ubuntu-drivers devices$ sudo apt-get install nvidia-driver-460 # the recommended one by ubuntu-drivers$ update-initramfs -u # needed to do this so rebooting wouldn't lose configuration I think$ sudo reboot 然后再重装NVIDIA-docker： 1234567$curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$sudo apt-get update$sudo apt-get install nvidia-docker2$sudo pkill -SIGHUP dockerd$docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 测试： 1sudo nvidia-docker run --rm nvidia/cuda:10.1-devel nvidia-smi 万幸CUDA, CuDNN都还有。 123456&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True&gt;&gt;&gt; a=torch.randn(1,2)&gt;&gt;&gt; a.cuda()tensor([[-0.4678, 0.1525]], device='cuda:0') 配置默认运行的是nvidia-docker 而不是 docker (https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json 文件中配置如下内容： 12345678910&#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [], &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;gemfield.mirror.aliyuncs.com&quot;] &#125; &#125;&#125; pycharm里用dockerpython 位置：/home/shiyuuuu/anaconda3/bin/python","categories":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"}]},{"title":"1-两数之和","slug":"1_两数之和","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:53:27.565Z","comments":true,"path":"2021/03/26/1_两数之和/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/","excerpt":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。","text":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6输出：[1,2] 示例 3 输入：nums = [3,3], target = 6输出：[0,1] 提示： 2 &lt;= nums.length &lt;= 103-109 &lt;= nums[i] &lt;= 109-109 &lt;= target &lt;= 109只会存在一个有效答案 暴力解答我自己的解答：非常暴力检索，第一个: i 从0到n，第二个: j 从i+1 到n（或者倒序来）。这样复杂度是O(n^2) 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): for j in range(i+1,len(nums)): if nums[i]+nums[j]==target: return [i,j] 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: # nums_sorted=sorted(nums) for i in range(len(nums)): for j in range(len(nums)-1,i,-1): if nums[i]+nums[j]==target: c=sorted([i,j]) return c 哈希表哈希表博文：哈希表 思路及算法注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。 使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。 这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 先建立一个空字典，查找target-num是不是hashtable的键值，如果是，直接return，如果不是，把这个num-i对以键值对的形式添加入字典。哈希表查找元素的复杂度为O(1) 代码 1234567891011121314151617181920from typing import Listclass Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i return []s=Solution()# nums=[2,7,11,15]# target=9# nums = [3,2,4]# target = 6nums = [3,3]target = 6a=s.twoSum(nums,target)print(a) 复杂度分析时间复杂度：O(N)，其中 N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。 空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]},{"title":"哈希表","slug":"哈希表","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:51:55.495Z","comments":true,"path":"2021/03/26/哈希表/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/","excerpt":"","text":"哈希表Hash Table，也叫散列表。力扣-两数之和","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]}],"categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"super-resolution","slug":"super-resolution","permalink":"https://shiyuuuu.github.io/tags/super-resolution/"},{"name":"pseudo-supervision","slug":"pseudo-supervision","permalink":"https://shiyuuuu.github.io/tags/pseudo-supervision/"},{"name":"unpaired","slug":"unpaired","permalink":"https://shiyuuuu.github.io/tags/unpaired/"},{"name":"segmentation","slug":"segmentation","permalink":"https://shiyuuuu.github.io/tags/segmentation/"},{"name":"LV+HV","slug":"LV-HV","permalink":"https://shiyuuuu.github.io/tags/LV-HV/"},{"name":"audio+image","slug":"audio-image","permalink":"https://shiyuuuu.github.io/tags/audio-image/"},{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"},{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"},{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]}