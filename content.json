{"meta":{"title":"Shiyu's Blog","subtitle":"Learn to live.","description":null,"author":"Shiyu","url":"https://shiyuuuu.github.io","root":"/"},"pages":[{"title":"404 Not Found","date":"2021-03-26T09:37:09.711Z","updated":"2020-06-16T14:38:42.000Z","comments":true,"path":"404.html","permalink":"https://shiyuuuu.github.io/404.html","excerpt":"","text":"404 很抱歉，您访问的页面不存在 可能是输入地址有误或该地址已被删除"},{"title":"关于","date":"2021-03-26T09:37:09.712Z","updated":"2020-06-16T14:25:06.000Z","comments":true,"path":"about/index.html","permalink":"https://shiyuuuu.github.io/about/index.html","excerpt":"","text":"一枚CS小学生。"},{"title":"所有分类","date":"2021-03-26T09:37:09.714Z","updated":"2020-06-16T14:35:18.000Z","comments":true,"path":"categories/index.html","permalink":"https://shiyuuuu.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2021-03-26T09:37:09.716Z","updated":"2020-06-16T14:40:28.000Z","comments":true,"path":"tags/index.html","permalink":"https://shiyuuuu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"阅读论文-GLEAN Generative Latent Bank for Large-Factor Image Super-Resolution","slug":"阅读论文GLEAN","date":"2021-03-28T16:00:00.000Z","updated":"2021-03-29T12:49:59.620Z","comments":true,"path":"2021/03/29/阅读论文GLEAN/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87GLEAN/","excerpt":"","text":"project code paper 任务是： 大尺度超分辨率（8$\\times$ 到 64$\\times$），most details and textures are lost during downsampling. motivation 已有的SR方法： solely rely on $L_2$ loss: 视觉质量不好 (over-smoothing artifacts)。 adverssrial loss [ESRGAN]: Generator既要捕获图像characteristics又要保真（maintaining the fidelity to the GT）, 限制了其近似自然图像的能力，产生artifacts GAN inversion methods [PULSE]：反转pre-trained GAN的生成过程：把image mapping回latent space；再由latent space中optimal vector重建图像。只靠低维隐向量不足以指导重建的过程，使得产生的结果low fidelity. 需要image-specific, iterative的优化. 利用pre-trained GAN作为latent bank， 充分利用pre-trained GAN中封装的丰富且多样的先验。换用不同的bank可以不同类的图像：cat，building，human face，car. 利用字典学习的方式，测试阶段，只需要一次前传即可得到恢复后的图像。 GLEAN 的整体结构：encoder-bank-decoder related worklarge-factor SR fully probabilistic pixel recursive network for upsampling extremely coarse images with resolution 8×8. (Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In ICCV, 2017.) RFB-ESRGAN：adopts multi-scale receptive fields blocks for 16× SR. (Taizhang Shang, Qiuju Dai, Shengchen Zhu, Tong Yang, and Yandong Guo. Perceptual extreme super resolution network with receptive field block. In CVPRW, 2020.) VarSR: 8× SR by matching the latent distributions of LR and HR images to recover the missing details. (Sangeek Hyun and Jae-Pil Heo. VarSR: Variational super-resolution network for very low resolution images. In ECCV, 2020.) perform 16× reference-based SR on paintings with a non-local matching module and a wavelet texture loss. (Yulun Zhang, Zhifei Zhang, Stephen DiVerdi, Zhaowen Wang, Jose Echevarria, and Yun Fu. Texture hallucination for large-scale painting super-resolution. In ECCV, 2020.) GAN inversion David Bau, Hendrik Strobelt, William Peebles, Bolei Zhou, Jun-Yan Zhu, Antonio Torralba, et al. Semantic photo manipulation with a generative image prior. TOG, 2020. Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code GAN prior. In CVPR, 2020. Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. 通过pixel-wise约束，迭代优化styleGAN的隐变量。 Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, and Ping Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In ECCV, 2020. finetune generator和latent code来缩小训练集和测试集分布的gap. 降质图像$x$，latent space：$\\mathcal{Z}$： z^*=argmin_{z \\in \\mathcal{Z}}\\mathcal{L}(G(z), x)缺点： 低维的隐向量不能保持图像的spatial information. 方法passing both the latent vectors and multi-resolution convolutional features from the encoder. multi-resolution cues need to be passed from the bank to the decoder. 整个结构为encoder-bank-decoder encoder：$E_0$为RRDB-Net，$E_i, i \\in\\{1,…,N\\}$代表堆叠一个stride=2的conv和一个stride=1的conv. 最后由FC层得到$C$, $C$表示隐向量，提供high-level信息。为了更好的指导结构重建，将多分辨率的特征和隐向量都送入bank. Generative latent bank: 用pre-trained的Generator, styleGAN，提供纹理和细节生成的先验。 对generator的每个block输入不同的隐向量$C_i$, $i \\in \\{0,…,k-1\\}$ $\\{g_i\\}$代表每个block输出的feature, 它是由$C_i, g_{i-1}, f_{N-i}$由augmented style block得到。 不直接输出结果，而是将特征$\\{g_i\\}$输入到decoder 优势：像reference-based SR，HR reference image作为显式图像字典。性能很受 输入和reference相似度的影响。GLEAN用GAN-based的字典，不依赖于任何具体的图像，它获取的是图像的分布。而且没有global matching和reference images selection， 计算简便。 decoder：progressive地聚合来自encoder和latent bank的特征。每个conv后跟着pixel-shuffle层。由于有encoder和decoder之间的skip-connection，encoder捕获的信息可以被强化，bank专注于纹理和细节的生成。 训练：$\\mathcal{l_2}$ loss, perceptual loss, adversarial loss. 训练时fix住latent bank，实验发现，finetune latent bank没有性能提升，而且可能使latent bank偏向训练集的分布。 主要结果 image retouching 参考文献：[ESRGAN]: Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, and Xiaoou Tang. ESRGAN: Enhanced super-resolution generative adversarial networks. In ECCVW, 2018. [PULSE]: Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. PULSE: Self-supervised photo upsampling via latent space exploration of generative models. In CVPR, 2020. reference-based SR Xiaoming Li, Chaofeng Chen, Shangchen Zhou, Xianhui Lin, Wangmeng Zuo, and Lei Zhang. Blind face restoration via deep multi-scale component dictionaries. In ECCV, 2020. Xiaoming Li, Wenyu Li, Dongwei Ren, Hongzhi Zhang, Meng Wang, and Wangmeng Zuo. Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion. In CVPR, 2020. Xu Yan, Weibing Zhao, Kun Yuan, Ruimao Zhang, Zhen Li, and Shuguang Cui. Towards content-independent multi-reference super-resolution: Adaptive pattern matching and feature aggregation. In ECCV, 2020. Yang Zhang, Ivor W Tsang, Yawei Luo, Changhui Hu, Xiaobo Lu, and Xin Yu. Copy and Paste GAN: Face hallucination from shaded thumbnails. In CVPR, 2020. Zhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. Image super-resolution by neural texture transfer. In CVPR, 2019.","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"}],"tags":[{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"}]},{"title":"阅读论文-Zero-Reference deep curve estimation for low-light image enhancement","slug":"Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement","date":"2021-03-28T16:00:00.000Z","updated":"2021-03-29T13:08:40.347Z","comments":true,"path":"2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/29/Zero-DCE_CVPR2020_zero-reference-deep-curve-estimation-for-low-light-image-enhancement/","excerpt":"出处：CVPR2020","text":"出处：CVPR2020 paper PDF supplemental materials project: https://li-chongyi.github.io/Proj_Zero-DCE.html code: https://github.com/Li-Chongyi/Zero-DCE motivation图像编辑软件通过调节曲线来增强图像=&gt;image-specific 曲线估计： 根据给定图像估计pixel-wise的调整曲线。不需要任何成对或不成对的训练数据（不需要reference/GT) 图像增强=&gt;非线性曲线映射 而不是通过image-to-image mapping 方法： non-reference loss functions contribution no reference: 避免了需要paired/unpaired数据的方法中的overfitting的问题 设计image-specific曲线：高次、pixel-wise 提升人脸识别的性能 related work传统方法调整图像的直方图分布调整，增大图像的动态范围。 global level: [1]. Dinu Coltuc, Philippe Bolon, and Jean-Marc Chassery. Exact histogram specification. IEEE Transactions on Image Processing, 15(5):1143–1152, 2006. [2]. Haidi Ibrahim and Nicholas Sia Pik Kong. Brightness preserving dynamic histogram equalization for image contrast enhancement. IEEE Transactions on Consumer Electronics, 53(4):1752–1758, 2007. local level: [3]. Chulwoo Lee, Chul Lee, and Chang-Su Kim. Contrast enhancement based on layered difference representation of 2d histograms. IEEE Transactions on Image Processing, 22(12):5372–5384, 2013. [4]. J Alex Stark. Adaptive image contrast enhancement using generalizations of histogram equalization. IEEE Transactions on Image Processing, 9(5):889–896, 2000. Retinex theory (将图像分解为reflectance和illumination，其中reflectance分量在任何光照条件下保持一致，图像质量增强任务变为illumination estimation问题): [5]. Edwin H Land. The retinex theory of color vision. Scientific American, 237(6):108–128, 1977. 自然的信息保存方法： [6]. Shuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE Transactions on Image Processing, 22(9):3538–3548, 2013. weighted vatiation: [7]. Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016. coarse illumination map: 搜索RGB中最大intensity的pixel [8]. Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. IEEE Transactions on Image Processing, 26(2):982–993, 2017. 考虑噪声： [9]. Mading Li, Jiaying Liu, Wenhan Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. IEEE Transactions on Image Processing, 27(6):2828–2841, 2018 自动exposure校正方法： 通过全局优化方法估计图像的S形曲线。 [10]. Lu Yuan and Jian Sun. Automatic exposure correction of consumer photographs. In ECCV, 2012. Data-driven方法CNN-based一般需要pair对的数据：资源密集型(resource-intensive)。一般这种成对的数据通过 自动光照退化、改变相机的设置 采集 或者用image retouching合成 LOL数据集[Chen Wei, Wenjing Wang, Wenhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In BMVC, 2018.] 通过改变曝光时间和ISO获取成对的low/normal光照的图像。 MIT-adobe FiveK数据集[Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo ´ Durand. Learning photographic global tonal adjustment with a database of input/output image pairs. In CVPR, 2011.]包括5000raw图，每一张raw图由专家生成5个retouched图像。 [11]提出了一种估计illumination的方法：poor generalization capability [11]. Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019. GAN-basedEnlightenGAN[Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. In CVPR, 2019.] 用unpair的low/normal光照的数据。需要仔细挑选unpaired的训练数据。 methods 通过non-reference loss function实现不需要paired/unpaired的数据。通过迭代训练得到结果。 Light-enhancement curve自适应的曲线参数只由输入图像决定。曲线要是单调的来保持周围像素的区别。曲线要处处可微保证可以梯度反传。 LE(I(x);\\alpha)=I(x)+\\alpha I(x)(1-I(x))其中$\\alpha \\in [-1,1]$, 将LE曲线分别作用于R\\G\\B通道，而不仅仅作用于illumination通道。调节3个通道可以更好的保持固有的色彩，减少过饱和。 上式可以多次迭代，写为： LE_n (x)=LE_{n-1}(x)+\\alpha_nLE_{n-1}(x)(1-LE_{n-1}(x))其中n为迭代次数，他们设为8. 多次迭代有更强的调节能力。 但是如果$\\alpha$在每一个点是固定值，只能是全局调节。全局调节容易带来over-/under- enhance local区域。为了解决这个问题，他们将$\\alpha$设置为pixel-wise的参数。上式变为： LE_n (x)=LE_{n-1}(x)+A_n(x)LE_{n-1}(x)(1-LE_{n-1}(x))其中A是一个与原图等大小的parameter map。 假设，一个局部区域的像素点有相同的intensity. 所以输出图像里的相邻像素也有单调的关系。 DCE-Net（具体结构见补充材料） 输入：低光照图像 输出：pixel-wise的高阶curve参数图。8次迭代共24个parameter maps 网络细节：不包括down-sampling和BN，它们会丢失相邻像素的关系。最后一层conv接Tanh 1. spatial consistency loss使增强后的图像保持时域的连贯性(coherence) L_{spa}=\\frac{1}{K}\\sum_{i=1}^K\\sum_{j \\in \\Omega(i)}(|(Y_i-Y_j|-|I_i-I_j|)^2$K$为局部区域数，$\\Omega(i)$为4个相邻区域（上下左右）。Y和I是局部区域的平均强度值(intensity)。局部区域的大小设为4*4. 相当于对K个4*4的局部区域，要求enhance前后图的局部区域与其邻域的强度差，差不多。 2. exposure control loss控制曝光的程度。测量局部区域的平均强度与well-exposedness的级别E之间的距离。E=0.6、在[0.4,0.7]之间差不多。 L_{exp}=\\frac{1}{M}\\sum_{k=1}^M|Y_k-E|M代表M个16*16的没有overlap的局部区域，Y是enhanced图像中局部的平均强度值。 3. color constancy loss校正色彩偏差。 L_{loc}=\\sum_{\\forall(p,q)\\in \\varepsilon}(J^p-J^q)^2,\\varepsilon=\\{(R,G),(R,B),(G,B)\\}其中，$J^p$代表enhanced图像p channel的平均强度。 4. illumination smoothness loss保持相邻pixel的单调性。对每个curve参数图A有： L_{tv_\\mathcal{A}}=\\frac{1}{N}\\sum_{n=1}^N\\sum_{c \\in \\xi}(|\\nabla_x\\mathcal{A}_n^c|+|\\nabla_y\\mathcal{A}_n^c|)^2,\\xi=\\{R,G,B\\}N为迭代次数。求梯度的操作有点像TV loss。 total loss L_{total}=L_{spa}+L_{exp}+W_{col}L_{col}+W_{tv_A}L_{tv_A}$W_{col}=0.5,W_{tv\\mathcal{A}}=20$. ablation 训练数据：用multi-exposure的训练数据更好。 benchmark resultsno reference 结果（US/PI）视觉评判标准： 1) whether the results contain over-/under-exposed artifacts or over-/under enhanced regions; 2) whether the results introduce color deviation; 3) whether the results have unnatural texture and obvious noise. 指标： user study (US) score non-reference perceptual index (PI) [Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In CVPR, 2018.] [Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning a no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:1–16, 2017.] .png) full reference 结果（PSNR/SSIM) face detection用SOTA的face detector(DSFD: https://github.com/Ir1d/DARKFACE_eval_tools ) 将不同enhance方法得到的图送入DSFD，得到P-R curve。","categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"}],"tags":[{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"}]},{"title":"ubuntu 踩坑记录","slug":"ubuntu踩坑记录","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T10:00:08.290Z","comments":true,"path":"2021/03/26/ubuntu踩坑记录/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/ubuntu%E8%B8%A9%E5%9D%91%E8%AE%B0%E5%BD%95/","excerpt":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown","text":"ubuntu 踩坑记录显卡驱动重装某次装好后，遇到bug： Can’t run remote python interpreter: OCI runtime create failed: container_linux.go:367: starting container process caused: process_linux.go:495: container init caused: Running hook #1:: error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: initialization error: driver error: failed to process request: unknown docker 里nvidia-smi不能用了，直接在docker外nvidia-smi也报错： NVIDIA-SMI couldn’t find libnvidia-ml.so library in your system. Please make sure that the NVIDIA Display Driver is properly installed and present in your system. Please also try adding directory that contains libnvidia-ml.so to your system PATH. 估计是什么时候update弄成的。 解决方法：重装显卡驱动 1234567891011121314151617181920212223# BTW this is all in console mode (for me, alt+ctrl+F2)# login + password as usual# removing ALL nvidia software$ sudo apt-get purge nvidia* # Checking what's left:$ dpkg -l | grep nvidia# Then I deleted the ones that showed up (mostly libnvidia-* but also xserver-xorg-video-nvidia-xxx`)$ sudo apt-get purge libnvidia* xserver-xorg-video-nvidia-440 $ sudo apt autoremove # clean it up# now reinstall everything including nvidia-common$ sudo apt-get nvidia-common# find the right driver again$ sudo add-apt-repository ppa:graphics-drivers/ppa$ sudo apt update$ ubuntu-drivers devices$ sudo apt-get install nvidia-driver-460 # the recommended one by ubuntu-drivers$ update-initramfs -u # needed to do this so rebooting wouldn't lose configuration I think$ sudo reboot 然后再重装NVIDIA-docker： 1234567$curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -$curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list$sudo apt-get update$sudo apt-get install nvidia-docker2$sudo pkill -SIGHUP dockerd$docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi 测试： 1sudo nvidia-docker run --rm nvidia/cuda:10.1-devel nvidia-smi 万幸CUDA, CuDNN都还有。 123456&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.cuda.is_available()True&gt;&gt;&gt; a=torch.randn(1,2)&gt;&gt;&gt; a.cuda()tensor([[-0.4678, 0.1525]], device='cuda:0') 配置默认运行的是nvidia-docker 而不是 docker (https://zhuanlan.zhihu.com/p/37519492)，在/etc/docker/daemon.json 文件中配置如下内容： 12345678910&#123; &quot;default-runtime&quot;: &quot;nvidia&quot;, &quot;runtimes&quot;: &#123; &quot;nvidia&quot;: &#123; &quot;path&quot;: &quot;&#x2F;usr&#x2F;bin&#x2F;nvidia-container-runtime&quot;, &quot;runtimeArgs&quot;: [], &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;gemfield.mirror.aliyuncs.com&quot;] &#125; &#125;&#125; pycharm里用dockerpython 位置：/home/shiyuuuu/anaconda3/bin/python","categories":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"}],"tags":[{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"}]},{"title":"1-两数之和","slug":"1_两数之和","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:53:27.565Z","comments":true,"path":"2021/03/26/1_两数之和/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/1_%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C/","excerpt":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。","text":"1. 两数之和https://leetcode-cn.com/problems/two-sum 题目给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 的那 两个 整数，并返回它们的数组下标。你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9输出：[0,1]解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6输出：[1,2] 示例 3 输入：nums = [3,3], target = 6输出：[0,1] 提示： 2 &lt;= nums.length &lt;= 103-109 &lt;= nums[i] &lt;= 109-109 &lt;= target &lt;= 109只会存在一个有效答案 暴力解答我自己的解答：非常暴力检索，第一个: i 从0到n，第二个: j 从i+1 到n（或者倒序来）。这样复杂度是O(n^2) 123456class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: for i in range(len(nums)): for j in range(i+1,len(nums)): if nums[i]+nums[j]==target: return [i,j] 12345678class Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: # nums_sorted=sorted(nums) for i in range(len(nums)): for j in range(len(nums)-1,i,-1): if nums[i]+nums[j]==target: c=sorted([i,j]) return c 哈希表哈希表博文：哈希表 思路及算法注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。 使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。 这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 先建立一个空字典，查找target-num是不是hashtable的键值，如果是，直接return，如果不是，把这个num-i对以键值对的形式添加入字典。哈希表查找元素的复杂度为O(1) 代码 1234567891011121314151617181920from typing import Listclass Solution: def twoSum(self, nums: List[int], target: int) -&gt; List[int]: hashtable = dict() for i, num in enumerate(nums): if target - num in hashtable: return [hashtable[target - num], i] hashtable[nums[i]] = i return []s=Solution()# nums=[2,7,11,15]# target=9# nums = [3,2,4]# target = 6nums = [3,3]target = 6a=s.twoSum(nums,target)print(a) 复杂度分析时间复杂度：O(N)，其中 N是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。 空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。","categories":[{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]},{"title":"哈希表","slug":"哈希表","date":"2021-03-25T16:00:00.000Z","updated":"2021-03-26T11:51:55.495Z","comments":true,"path":"2021/03/26/哈希表/","link":"","permalink":"https://shiyuuuu.github.io/2021/03/26/%E5%93%88%E5%B8%8C%E8%A1%A8/","excerpt":"","text":"哈希表Hash Table，也叫散列表。力扣-两数之和","categories":[{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"}]}],"categories":[{"name":"Paper reading","slug":"Paper-reading","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/"},{"name":"super-resolution","slug":"Paper-reading/super-resolution","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/super-resolution/"},{"name":"enhancement","slug":"Paper-reading/enhancement","permalink":"https://shiyuuuu.github.io/categories/Paper-reading/enhancement/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/categories/ubuntu/"},{"name":"bug","slug":"ubuntu/bug","permalink":"https://shiyuuuu.github.io/categories/ubuntu/bug/"},{"name":"LeetCode","slug":"LeetCode","permalink":"https://shiyuuuu.github.io/categories/LeetCode/"},{"name":"algorithm","slug":"algorithm","permalink":"https://shiyuuuu.github.io/categories/algorithm/"},{"name":"easy","slug":"LeetCode/easy","permalink":"https://shiyuuuu.github.io/categories/LeetCode/easy/"},{"name":"哈希表","slug":"algorithm/哈希表","permalink":"https://shiyuuuu.github.io/categories/algorithm/%E5%93%88%E5%B8%8C%E8%A1%A8/"}],"tags":[{"name":"Super-resolution","slug":"Super-resolution","permalink":"https://shiyuuuu.github.io/tags/Super-resolution/"},{"name":"GAN","slug":"GAN","permalink":"https://shiyuuuu.github.io/tags/GAN/"},{"name":"highly ill-posed","slug":"highly-ill-posed","permalink":"https://shiyuuuu.github.io/tags/highly-ill-posed/"},{"name":"enhancement","slug":"enhancement","permalink":"https://shiyuuuu.github.io/tags/enhancement/"},{"name":"loss functions","slug":"loss-functions","permalink":"https://shiyuuuu.github.io/tags/loss-functions/"},{"name":"zero-reference","slug":"zero-reference","permalink":"https://shiyuuuu.github.io/tags/zero-reference/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://shiyuuuu.github.io/tags/ubuntu/"},{"name":"bug","slug":"bug","permalink":"https://shiyuuuu.github.io/tags/bug/"},{"name":"docker","slug":"docker","permalink":"https://shiyuuuu.github.io/tags/docker/"},{"name":"哈希表","slug":"哈希表","permalink":"https://shiyuuuu.github.io/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"leetcode","slug":"leetcode","permalink":"https://shiyuuuu.github.io/tags/leetcode/"}]}